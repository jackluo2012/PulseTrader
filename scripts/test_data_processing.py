#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†æµç¨‹ç»¼åˆæµ‹è¯•è„šæœ¬
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

import logging
from datetime import datetime, timedelta

import numpy as np
import pandas as pd

from src.data.processors.cleaner import DataCleaner
from src.data.processors.transformer import DataTransformer
from src.data.processors.validator import DataValidator


def create_test_data():
    """åˆ›å»ºåŒ…å«å„ç§æ•°æ®è´¨é‡é—®é¢˜çš„æµ‹è¯•æ•°æ®"""

    # ç”Ÿæˆ30å¤©çš„æµ‹è¯•æ•°æ®
    dates = pd.date_range("2024-01-01", periods=30, freq="D")

    # æ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®ï¼ŒåŒ…å«å„ç§é—®é¢˜
    np.random.seed(42)
    base_price = 100.0
    price_changes = np.random.normal(0, 0.03, 30)
    prices = [base_price]
    for change in price_changes[1:]:
        new_price = prices[-1] * (1 + change)
        prices.append(max(new_price, 1.0))  # ç¡®ä¿ä»·æ ¼ä¸ºæ­£

    test_data = pd.DataFrame(
        {
            "symbol": ["000001"] * 30 + ["000001"] + ["000002"] * 30,  # åŒ…å«é‡å¤
            "date": list(dates) + [dates[5]] + list(dates),  # åŒ…å«é‡å¤æ—¥æœŸ
            "open": prices + [None] + [p * 0.95 for p in prices],  # åŒ…å«ç¼ºå¤±å€¼
            "high": [p * 1.05 for p in prices]
            + [80.0]
            + [p * 1.08 for p in prices],  # åŒ…å«å¼‚å¸¸å€¼
            "low": [p * 0.95 for p in prices]
            + [120.0]
            + [p * 0.92 for p in prices],  # åŒ…å«å¼‚å¸¸å€¼
            "close": prices + [105.0] + prices,  # åŒ…å«å¼‚å¸¸å€¼
            "volume": np.random.randint(1000000, 5000000, 61),  # æ­£å¸¸æˆäº¤é‡
        }
    )

    # æ·»åŠ ä¸€äº›é¢å¤–çš„é—®é¢˜
    test_data.loc[10, "volume"] = -1000000  # è´Ÿæˆäº¤é‡
    test_data.loc[15, "open"] = None  # ç¼ºå¤±å¼€ç›˜ä»·
    test_data.loc[20, "close"] = 0.01  # å¼‚å¸¸ä½ä»·

    return test_data


def main():
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    print("ğŸš€ å¼€å§‹æµ‹è¯•æ•°æ®å¤„ç†æµç¨‹...\n")

    # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
    print("=== 1. åˆ›å»ºæµ‹è¯•æ•°æ® ===")
    test_data = create_test_data()
    print(f"åŸå§‹æµ‹è¯•æ•°æ®ï¼š{len(test_data)} è¡Œ x {len(test_data.columns)} åˆ—")
    print("æ•°æ®é¢„è§ˆï¼š")
    print(test_data.head(10))
    print()

    # 2. æ•°æ®éªŒè¯
    print("=== 2. æ•°æ®éªŒè¯ ===")
    validator = DataValidator()

    validation_result = validator.validate(
        test_data, "price_data", check_completeness=True
    )

    print(f"éªŒè¯ç»“æœï¼š{'é€šè¿‡' if validation_result['is_valid'] else 'å¤±è´¥'}")
    print(f"é”™è¯¯æ•°é‡ï¼š{validation_result['summary']['total_errors']}")
    print(f"è­¦å‘Šæ•°é‡ï¼š{validation_result['summary']['total_warnings']}")

    if validation_result["structure"]["errors"]:
        print("ç»“æ„é”™è¯¯ï¼š", validation_result["structure"]["errors"])

    if validation_result["content"]["errors"]:
        print("å†…å®¹é”™è¯¯ï¼š", validation_result["content"]["errors"])

    if validation_result["content"]["warnings"]:
        print("å†…å®¹è­¦å‘Šï¼š", validation_result["content"]["warnings"])

    print()

    # 3. æ•°æ®æ¸…æ´—
    print("=== 3. æ•°æ®æ¸…æ´— ===")
    cleaner = DataCleaner()

    cleaning_steps = [
        "remove_duplicates",
        "standardize_types",
        "handle_missing",
        "handle_outliers",
        "validate_consistency",
    ]

    cleaned_data, cleaning_report = cleaner.clean_data(
        test_data, "price_data", cleaning_steps
    )

    print(f"æ¸…æ´—å‰ï¼š{test_data.shape}")
    print(f"æ¸…æ´—åï¼š{cleaned_data.shape}")
    print(f"åˆ é™¤è¡Œæ•°ï¼š{cleaning_report['total_removed_rows']}")

    # æ˜¾ç¤ºå„æ­¥éª¤çš„æ¸…æ´—ç»“æœ
    for step_info in cleaning_report["steps_performed"]:
        if "removed_count" in step_info:
            print(f"- {step_info['operation']}: åˆ é™¤ {step_info['removed_count']} è¡Œ")
        elif "handled_missing" in step_info:
            print(
                f"- {step_info['operation']}: å¤„ç† {step_info['handled_missing']} ä¸ªç¼ºå¤±å€¼"
            )
        elif "total_corrections" in step_info:
            print(
                f"- {step_info['operation']}: ä¿®æ­£ {step_info['total_corrections']} ä¸ªå¼‚å¸¸"
            )

    print()

    # 4. éªŒè¯æ¸…æ´—åçš„æ•°æ®
    print("=== 4. æ¸…æ´—åæ•°æ®éªŒè¯ ===")
    post_cleaning_validation = validator.validate(cleaned_data, "price_data")

    print(f"æ¸…æ´—åéªŒè¯ï¼š{'é€šè¿‡' if post_cleaning_validation['is_valid'] else 'å¤±è´¥'}")
    print(f"å‰©ä½™é”™è¯¯ï¼š{post_cleaning_validation['summary']['total_errors']}")
    print(f"å‰©ä½™è­¦å‘Šï¼š{post_cleaning_validation['summary']['total_warnings']}")

    if post_cleaning_validation["content"]["quality_metrics"]:
        quality_metrics = post_cleaning_validation["content"]["quality_metrics"]
        print("è´¨é‡æŒ‡æ ‡ï¼š")
        print(f"  ç¼ºå¤±å€¼æ¯”ä¾‹ï¼š{sum(quality_metrics['missing_ratio'].values()):.4f}")
        print(f"  é‡å¤è¡Œæ¯”ä¾‹ï¼š{quality_metrics['duplicate_ratio']:.4f}")

    print()

    # 5. æ•°æ®è½¬æ¢
    print("=== 5. æ•°æ®è½¬æ¢ ===")
    transformer = DataTransformer()

    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
    if len(cleaned_data) >= 20:
        transformations = ["normalize", "indicators", "features"]
        transformed_data, transformation_report = transformer.transform_data(
            cleaned_data, "price_data", transformations
        )

        print(f"è½¬æ¢å‰ï¼š{cleaned_data.shape}")
        print(f"è½¬æ¢åï¼š{transformed_data.shape}")

        # ç»Ÿè®¡æ–°å¢çš„ç‰¹å¾
        original_columns = set(cleaned_data.columns)
        new_columns = set(transformed_data.columns) - original_columns
        print(f"æ–°å¢ç‰¹å¾æ•°é‡ï¼š{len(new_columns)}")
        print("éƒ¨åˆ†æ–°å¢ç‰¹å¾ï¼š", list(new_columns)[:10])

        print()

    # 6. æœ€ç»ˆæ•°æ®è´¨é‡æŠ¥å‘Š
    print("=== 6. æœ€ç»ˆæ•°æ®è´¨é‡æŠ¥å‘Š ===")

    final_data = transformed_data if "transformed_data" in locals() else cleaned_data
    final_validation = validator.validate(final_data, "price_data")

    print("ğŸ“Š æ•°æ®å¤„ç†æ€»ç»“ï¼š")
    print(f"  åŸå§‹æ•°æ®é‡ï¼š{len(test_data)} è¡Œ")
    print(f"  æœ€ç»ˆæ•°æ®é‡ï¼š{len(final_data)} è¡Œ")
    print(f"  æ•°æ®ä¿ç•™ç‡ï¼š{len(final_data)/len(test_data):.2%}")
    print(f"  æœ€ç»ˆéªŒè¯çŠ¶æ€ï¼š{'âœ… é€šè¿‡' if final_validation['is_valid'] else 'âŒ å¤±è´¥'}")
    print(f"  æœ€ç»ˆç‰¹å¾æ•°ï¼š{len(final_data.columns)}")

    if final_validation["content"]["quality_metrics"]:
        metrics = final_validation["content"]["quality_metrics"]
        print(f"  æ•°æ®å®Œæ•´æ€§ï¼š{(1-sum(metrics['missing_ratio'].values())):.2%}")
        print(
            f"  æ•°æ®ä¸€è‡´æ€§ï¼š{'âœ…' if not final_validation['content']['errors'] else 'âŒ'}"
        )

    print("\nğŸ‰ æ•°æ®å¤„ç†æµç¨‹æµ‹è¯•å®Œæˆï¼")

    # 7. ä¿å­˜å¤„ç†åçš„æ ·æœ¬æ•°æ®ï¼ˆå¯é€‰ï¼‰
    output_path = "data/processed/cleaned_sample_data.csv"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    try:
        final_data.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"\nğŸ’¾ å¤„ç†åæ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_path}")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜æ•°æ®å¤±è´¥ï¼š{e}")


if __name__ == "__main__":
    main()
