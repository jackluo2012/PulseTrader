# 2.2 æ•°æ®æ¸…æ´—ä¸éªŒè¯

## ğŸ“– æœ¬èŠ‚æ¦‚è¿°

æœ¬èŠ‚å°†å»ºç«‹å®Œæ•´çš„æ•°æ®è´¨é‡ç®¡æ§ä½“ç³»ï¼ŒåŒ…æ‹¬æ•°æ®éªŒè¯è§„åˆ™ã€å¼‚å¸¸æ•°æ®æ£€æµ‹ã€æ•°æ®æ¸…æ´—æµç¨‹å’Œæ•°æ®æ ‡å‡†åŒ–å¤„ç†ã€‚é«˜è´¨é‡çš„æ•°æ®æ˜¯é‡åŒ–äº¤æ˜“ç³»ç»ŸæˆåŠŸçš„åŸºç¡€ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- âœ… è®¾è®¡æ•°æ®è´¨é‡æ£€æŸ¥è§„åˆ™
- âœ… æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸æ•°æ®
- âœ… å®ç°æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
- âœ… å¤„ç†ç¼ºå¤±å€¼å’Œé‡å¤æ•°æ®
- âœ… å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§ä½“ç³»

## â±ï¸ é¢„è®¡æ—¶é—´ï¼š35-45åˆ†é’Ÿ

---

## ğŸ” æ•°æ®è´¨é‡æ¡†æ¶

### 1. åˆ›å»ºæ•°æ®éªŒè¯å™¨

åˆ›å»º `src/data/processors/validator.py`ï¼š

```python
"""
æ•°æ®éªŒè¯å™¨
æä¾›å„ç§æ•°æ®è´¨é‡æ£€æŸ¥å’ŒéªŒè¯åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import logging
import re

logger = logging.getLogger(__name__)


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.validation_rules = self._load_validation_rules()
        self.validation_results = {}

    def _load_validation_rules(self) -> Dict[str, Dict]:
        """åŠ è½½éªŒè¯è§„åˆ™é…ç½®"""
        return {
            'price_data': {
                'required_columns': ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume'],
                'numeric_columns': ['open', 'high', 'low', 'close', 'volume'],
                'positive_columns': ['open', 'high', 'low', 'close', 'volume'],
                'price_relations': {
                    'high_ge_low': lambda df: (df['high'] >= df['low']).all(),
                    'high_ge_close': lambda df: (df['high'] >= df['close']).all(),
                    'low_le_close': lambda df: (df['low'] <= df['close']).all(),
                    'high_ge_open': lambda df: (df['high'] >= df['open']).all(),
                    'low_le_open': lambda df: (df['low'] <= df['open']).all()
                },
                'reasonable_ranges': {
                    'price': (0.01, 10000),    # è‚¡ä»·åˆç†èŒƒå›´
                    'volume': (0, 1e12),        # æˆäº¤é‡åˆç†èŒƒå›´
                    'change_percent': (-0.21, 0.21)  # æ¶¨è·Œå¹…é™åˆ¶ï¼ˆè€ƒè™‘STè‚¡ï¼‰
                }
            },
            'stock_info': {
                'required_columns': ['symbol', 'stock_name'],
                'string_columns': ['symbol', 'stock_name'],
                'symbol_pattern': r'^\d{6}$',  # 6ä½æ•°å­—è‚¡ç¥¨ä»£ç 
                'max_string_length': {
                    'symbol': 6,
                    'stock_name': 20
                }
            }
        }

    def validate_structure(self, data: pd.DataFrame, data_type: str) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®ç»“æ„

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            data_type: æ•°æ®ç±»å‹

        Returns:
            éªŒè¯ç»“æœ
        """
        results = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'info': {}
        }

        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºDataFrame
            if not isinstance(data, pd.DataFrame):
                results['is_valid'] = False
                results['errors'].append(f"æ•°æ®ç±»å‹é”™è¯¯ï¼šæœŸæœ›DataFrameï¼Œå®é™…{type(data)}")
                return results

            # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
            if data.empty:
                results['is_valid'] = False
                results['errors'].append("æ•°æ®ä¸ºç©º")
                return results

            # è·å–éªŒè¯è§„åˆ™
            rules = self.validation_rules.get(data_type, {})

            # æ£€æŸ¥å¿…éœ€åˆ—
            if 'required_columns' in rules:
                missing_columns = set(rules['required_columns']) - set(data.columns)
                if missing_columns:
                    results['is_valid'] = False
                    results['errors'].append(f"ç¼ºå°‘å¿…éœ€åˆ—ï¼š{missing_columns}")

            # æ£€æŸ¥é¢å¤–åˆ—
            extra_columns = set(data.columns) - set(rules.get('required_columns', []))
            if extra_columns:
                results['warnings'].append(f"å‘ç°é¢å¤–åˆ—ï¼š{extra_columns}")

            # åŸºæœ¬ä¿¡æ¯
            results['info'] = {
                'rows': len(data),
                'columns': len(data.columns),
                'column_names': list(data.columns),
                'data_types': data.dtypes.to_dict(),
                'memory_usage': data.memory_usage(deep=True).sum()
            }

        except Exception as e:
            results['is_valid'] = False
            results['errors'].append(f"ç»“æ„éªŒè¯å¼‚å¸¸ï¼š{e}")

        return results

    def validate_content(self, data: pd.DataFrame, data_type: str) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®å†…å®¹

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            data_type: æ•°æ®ç±»å‹

        Returns:
            éªŒè¯ç»“æœ
        """
        results = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'quality_metrics': {}
        }

        try:
            rules = self.validation_rules.get(data_type, {})

            # æ£€æŸ¥æ•°æ®ç±»å‹
            if 'numeric_columns' in rules:
                for col in rules['numeric_columns']:
                    if col in data.columns:
                        non_numeric = data[col].apply(lambda x: not pd.api.types.is_numeric_dtype(type(x)))
                        if non_numeric.any():
                            results['warnings'].append(f"åˆ— '{col}' åŒ…å«éæ•°å€¼æ•°æ®ï¼š{non_numeric.sum()} ä¸ª")

            # æ£€æŸ¥æ­£æ•°çº¦æŸ
            if 'positive_columns' in rules:
                for col in rules['positive_columns']:
                    if col in data.columns:
                        negative_count = (data[col] <= 0).sum()
                        if negative_count > 0:
                            results['errors'].append(f"åˆ— '{col}' åŒ…å«éæ­£æ•°ï¼š{negative_count} ä¸ª")

            # æ£€æŸ¥ä»·æ ¼å…³ç³»
            if 'price_relations' in rules:
                for relation_name, relation_func in rules['price_relations'].items():
                    try:
                        if not relation_func(data):
                            results['errors'].append(f"ä»·æ ¼å…³ç³»éªŒè¯å¤±è´¥ï¼š{relation_name}")
                    except Exception as e:
                        results['warnings'].append(f"ä»·æ ¼å…³ç³»æ£€æŸ¥å¼‚å¸¸ {relation_name}ï¼š{e}")

            # æ£€æŸ¥åˆç†èŒƒå›´
            if 'reasonable_ranges' in rules:
                for range_name, (min_val, max_val) in rules['reasonable_ranges'].items():
                    if range_name == 'price':
                        price_columns = ['open', 'high', 'low', 'close']
                        for col in price_columns:
                            if col in data.columns:
                                out_of_range = ((data[col] < min_val) | (data[col] > max_val)).sum()
                                if out_of_range > 0:
                                    results['warnings'].append(f"åˆ— '{col}' æœ‰ {out_of_range} ä¸ªå€¼è¶…å‡ºåˆç†èŒƒå›´ [{min_val}, {max_val}]")
                    elif range_name == 'change_percent' and 'change_percent' in data.columns:
                        out_of_range = ((data[col] < min_val) | (data[col] > max_val)).sum()
                        if out_of_range > 0:
                            results['warnings'].append(f"æ¶¨è·Œå¹…æœ‰ {out_of_range} ä¸ªå€¼è¶…å‡ºåˆç†èŒƒå›´")

            # æ£€æŸ¥è‚¡ç¥¨ä»£ç æ ¼å¼
            if data_type == 'stock_info' and 'symbol_pattern' in rules:
                if 'symbol' in data.columns:
                    pattern = re.compile(rules['symbol_pattern'])
                    invalid_symbols = data[~data['symbol'].astype(str).str.match(pattern)]
                    if not invalid_symbols.empty:
                        results['errors'].append(f"å‘ç° {len(invalid_symbols)} ä¸ªæ— æ•ˆè‚¡ç¥¨ä»£ç æ ¼å¼")

            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            results['quality_metrics'] = self._calculate_quality_metrics(data)

        except Exception as e:
            results['is_valid'] = False
            results['errors'].append(f"å†…å®¹éªŒè¯å¼‚å¸¸ï¼š{e}")

        return results

    def _calculate_quality_metrics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡"""
        metrics = {}

        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing_stats = data.isnull().sum()
        metrics['missing_values'] = missing_stats.to_dict()
        metrics['missing_ratio'] = (missing_stats / len(data)).to_dict()

        # é‡å¤å€¼ç»Ÿè®¡
        metrics['duplicate_rows'] = data.duplicated().sum()
        metrics['duplicate_ratio'] = metrics['duplicate_rows'] / len(data)

        # æ•°å€¼åˆ—ç»Ÿè®¡
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            metrics['numeric_summary'] = data[numeric_columns].describe().to_dict()

        # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆIQRæ–¹æ³•ï¼‰
        outliers = {}
        for col in numeric_columns:
            if col in data.columns:
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                outlier_count = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()
                outliers[col] = outlier_count

        metrics['outliers'] = outliers

        return metrics

    def validate_completeness(self, data: pd.DataFrame, date_column: str = 'date') -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆä¸»è¦æ˜¯æ—¶é—´åºåˆ—å®Œæ•´æ€§ï¼‰

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            date_column: æ—¥æœŸåˆ—å

        Returns:
            å®Œæ•´æ€§éªŒè¯ç»“æœ
        """
        results = {
            'is_complete': True,
            'missing_dates': [],
            'gaps': [],
            'completeness_ratio': 1.0
        }

        try:
            if date_column not in data.columns:
                results['is_complete'] = False
                results['missing_dates'].append(f"ç¼ºå°‘æ—¥æœŸåˆ—ï¼š{date_column}")
                return results

            # è½¬æ¢æ—¥æœŸåˆ—
            dates = pd.to_datetime(data[date_column]).sort_values()

            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            date_range = (dates.max() - dates.min()).days
            unique_dates = dates.dt.date.unique()

            # ç”Ÿæˆå®Œæ•´çš„æ—¥æœŸåºåˆ—ï¼ˆæ’é™¤å‘¨æœ«ï¼‰
            full_date_range = pd.date_range(
                start=dates.min(),
                end=dates.max(),
                freq='D'  # æ¯æ—¥é¢‘ç‡
            )

            # è¿‡æ»¤å‘¨æœ«ï¼ˆAè‚¡ä¸åœ¨å‘¨æœ«äº¤æ˜“ï¼‰
            trading_days = [d for d in full_date_range if d.weekday() < 5]
            trading_dates = [d.date() for d in trading_days]

            # æ‰¾å‡ºç¼ºå¤±çš„äº¤æ˜“æ—¥
            missing_dates = set(trading_dates) - set(unique_dates)

            if missing_dates:
                results['is_complete'] = False
                results['missing_dates'] = sorted(list(missing_dates))
                results['completeness_ratio'] = len(unique_dates) / len(trading_dates)

                # æ£€æŸ¥è¿ç»­ç¼ºå¤±çš„æ—¥æœŸæ®µ
                sorted_missing = sorted(missing_dates)
                gaps = []
                if sorted_missing:
                    current_gap = [sorted_missing[0]]
                    for i in range(1, len(sorted_missing)):
                        if sorted_missing[i] == sorted_missing[i-1] + timedelta(days=1):
                            current_gap.append(sorted_missing[i])
                        else:
                            if len(current_gap) > 1:  # åªè®°å½•è¿ç»­å¤šå¤©çš„ç¼ºå¤±
                                gaps.append((current_gap[0], current_gap[-1]))
                            current_gap = [sorted_missing[i]]

                    if len(current_gap) > 1:
                        gaps.append((current_gap[0], current_gap[-1]))

                    results['gaps'] = gaps

        except Exception as e:
            results['is_complete'] = False
            results['missing_dates'].append(f"å®Œæ•´æ€§éªŒè¯å¼‚å¸¸ï¼š{e}")

        return results

    def validate(self, data: pd.DataFrame, data_type: str,
                check_completeness: bool = False, date_column: str = 'date') -> Dict[str, Any]:
        """
        ç»¼åˆéªŒè¯

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            data_type: æ•°æ®ç±»å‹
            check_completeness: æ˜¯å¦æ£€æŸ¥å®Œæ•´æ€§
            date_column: æ—¥æœŸåˆ—å

        Returns:
            ç»¼åˆéªŒè¯ç»“æœ
        """
        logger.info(f"å¼€å§‹éªŒè¯æ•°æ®ï¼š{data_type}, æ•°æ®é‡ï¼š{len(data)}")

        # ç»“æ„éªŒè¯
        structure_result = self.validate_structure(data, data_type)

        # å†…å®¹éªŒè¯
        content_result = self.validate_content(data, data_type)

        # å®Œæ•´æ€§éªŒè¯
        completeness_result = {}
        if check_completeness and date_column in data.columns:
            completeness_result = self.validate_completeness(data, date_column)

        # æ±‡æ€»ç»“æœ
        overall_result = {
            'is_valid': structure_result['is_valid'] and content_result['is_valid'],
            'validation_time': datetime.now().isoformat(),
            'data_type': data_type,
            'structure': structure_result,
            'content': content_result,
            'completeness': completeness_result,
            'summary': {
                'total_errors': len(structure_result['errors']) + len(content_result['errors']),
                'total_warnings': len(structure_result['warnings']) + len(content_result['warnings']),
                'recommendations': []
            }
        }

        # ç”Ÿæˆå»ºè®®
        if overall_result['summary']['total_errors'] > 0:
            overall_result['summary']['recommendations'].append("å­˜åœ¨æ•°æ®é”™è¯¯ï¼Œå»ºè®®æ¸…æ´—åå†ä½¿ç”¨")

        if overall_result['summary']['total_warnings'] > 0:
            overall_result['summary']['recommendations'].append("å­˜åœ¨æ•°æ®è­¦å‘Šï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡")

        if completeness_result.get('completeness_ratio', 1.0) < 0.95:
            overall_result['summary']['recommendations'].append("æ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®")

        # ç¼“å­˜éªŒè¯ç»“æœ
        self.validation_results[f"{data_type}_{datetime.now().isoformat()}"] = overall_result

        logger.info(f"éªŒè¯å®Œæˆï¼šæœ‰æ•ˆ={overall_result['is_valid']}, "
                   f"é”™è¯¯={overall_result['summary']['total_errors']}, "
                   f"è­¦å‘Š={overall_result['summary']['total_warnings']}")

        return overall_result


def test_data_validator():
    """æµ‹è¯•æ•°æ®éªŒè¯å™¨"""
    validator = DataValidator()

    # æµ‹è¯•æ­£å¸¸æ•°æ®
    print("=== æµ‹è¯•æ­£å¸¸ä»·æ ¼æ•°æ® ===")
    normal_data = pd.DataFrame({
        'symbol': ['000001'] * 5,
        'date': pd.date_range('2024-01-01', periods=5),
        'open': [10.0, 10.1, 10.2, 10.1, 10.3],
        'high': [10.2, 10.3, 10.4, 10.2, 10.5],
        'low': [9.8, 9.9, 10.0, 9.9, 10.1],
        'close': [10.1, 10.2, 10.1, 10.0, 10.4],
        'volume': [1000000, 1200000, 800000, 900000, 1100000]
    })

    result = validator.validate(normal_data, 'price_data')
    print(f"éªŒè¯ç»“æœï¼š{result['is_valid']}")
    print(f"é”™è¯¯æ•°ï¼š{result['summary']['total_errors']}")
    print(f"è­¦å‘Šæ•°ï¼š{result['summary']['total_warnings']}")

    # æµ‹è¯•å¼‚å¸¸æ•°æ®
    print("\n=== æµ‹è¯•å¼‚å¸¸ä»·æ ¼æ•°æ® ===")
    abnormal_data = pd.DataFrame({
        'symbol': ['000001'] * 3,
        'date': pd.date_range('2024-01-01', periods=3),
        'open': [10.0, 10.1, -5.0],  # è´Ÿä»·æ ¼
        'high': [9.5, 10.3, 10.2],    # æœ€é«˜ä»·ä½äºå¼€ç›˜ä»·
        'low': [10.2, 9.9, 10.1],     # æœ€ä½ä»·é«˜äºå¼€ç›˜ä»·
        'close': [10.1, 10.2, 10.0],
        'volume': [1000000, 1200000, -100000]  # è´Ÿæˆäº¤é‡
    })

    result = validator.validate(abnormal_data, 'price_data')
    print(f"éªŒè¯ç»“æœï¼š{result['is_valid']}")
    print(f"é”™è¯¯ï¼š{result['content']['errors']}")
    print(f"è­¦å‘Šï¼š{result['content']['warnings']}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_data_validator()
```

### 2. åˆ›å»ºæ•°æ®æ¸…æ´—å™¨

åˆ›å»º `src/data/processors/cleaner.py`ï¼š

```python
"""
æ•°æ®æ¸…æ´—å™¨
æä¾›å„ç§æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
import re

from .validator import DataValidator

logger = logging.getLogger(__name__)


class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ¸…æ´—å™¨"""
        self.validator = DataValidator()
        self.cleaning_log = []

    def remove_duplicates(self, data: pd.DataFrame,
                         subset: List[str] = None,
                         keep: str = 'first') -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        å»é™¤é‡å¤æ•°æ®

        Args:
            data: è¾“å…¥æ•°æ®
            subset: ç”¨äºåˆ¤æ–­é‡å¤çš„åˆ—
            keep: ä¿ç•™ç­–ç•¥ï¼ˆfirst, last, Falseï¼‰

        Returns:
            æ¸…æ´—åçš„æ•°æ®å’Œæ¸…æ´—ä¿¡æ¯
        """
        original_count = len(data)

        if subset is None:
            # é»˜è®¤ä½¿ç”¨æ‰€æœ‰åˆ—
            duplicates = data.duplicated(keep=keep)
        else:
            duplicates = data.duplicated(subset=subset, keep=keep)

        cleaned_data = data[~duplicates].copy()
        removed_count = duplicates.sum()

        cleaning_info = {
            'operation': 'remove_duplicates',
            'original_count': original_count,
            'cleaned_count': len(cleaned_data),
            'removed_count': removed_count,
            'removal_ratio': removed_count / original_count if original_count > 0 else 0,
            'subset_columns': subset,
            'timestamp': datetime.now().isoformat()
        }

        self.cleaning_log.append(cleaning_info)
        logger.info(f"å»é™¤é‡å¤æ•°æ®ï¼š{removed_count}/{original_count} ({cleaning_info['removal_ratio']:.2%})")

        return cleaned_data, cleaning_info

    def handle_missing_values(self, data: pd.DataFrame,
                            strategy: str = 'drop',
                            columns: List[str] = None,
                            fill_value: Any = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        å¤„ç†ç¼ºå¤±å€¼

        Args:
            data: è¾“å…¥æ•°æ®
            strategy: å¤„ç†ç­–ç•¥ï¼ˆdrop, fill, interpolate, forward_fill, backward_fillï¼‰
            columns: æŒ‡å®šå¤„ç†çš„åˆ—ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—
            fill_value: å¡«å……å€¼ï¼ˆå½“strategy='fill'æ—¶ä½¿ç”¨ï¼‰

        Returns:
            æ¸…æ´—åçš„æ•°æ®å’Œæ¸…æ´—ä¿¡æ¯
        """
        if columns is None:
            columns = data.columns.tolist()

        cleaned_data = data.copy()
        missing_info = {}

        # ç»Ÿè®¡åŸå§‹ç¼ºå¤±å€¼
        original_missing = cleaned_data[columns].isnull().sum()
        total_missing_original = original_missing.sum()

        if total_missing_original == 0:
            cleaning_info = {
                'operation': 'handle_missing_values',
                'strategy': strategy,
                'original_missing': 0,
                'handled_missing': 0,
                'timestamp': datetime.now().isoformat()
            }
            self.cleaning_log.append(cleaning_info)
            logger.info("æ•°æ®æ— ç¼ºå¤±å€¼ï¼Œè·³è¿‡å¤„ç†")
            return cleaned_data, cleaning_info

        # æ ¹æ®ç­–ç•¥å¤„ç†ç¼ºå¤±å€¼
        if strategy == 'drop':
            # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
            cleaned_data = cleaned_data.dropna(subset=columns)

        elif strategy == 'fill':
            # ä½¿ç”¨æŒ‡å®šå€¼å¡«å……
            for col in columns:
                if col in cleaned_data.columns:
                    if fill_value is not None:
                        cleaned_data[col] = cleaned_data[col].fillna(fill_value)
                    else:
                        # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°ï¼Œå­—ç¬¦ä¸²åˆ—ç”¨ä¼—æ•°
                        if pd.api.types.is_numeric_dtype(cleaned_data[col]):
                            fill_val = cleaned_data[col].median()
                        else:
                            fill_val = cleaned_data[col].mode().iloc[0] if not cleaned_data[col].mode().empty else 'Unknown'
                        cleaned_data[col] = cleaned_data[col].fillna(fill_val)

        elif strategy == 'interpolate':
            # çº¿æ€§æ’å€¼ï¼ˆä»…é€‚ç”¨äºæ•°å€¼åˆ—ï¼‰
            for col in columns:
                if col in cleaned_data.columns and pd.api.types.is_numeric_dtype(cleaned_data[col]):
                    cleaned_data[col] = cleaned_data[col].interpolate(method='linear')

        elif strategy == 'forward_fill':
            # å‰å‘å¡«å……
            cleaned_data[columns] = cleaned_data[columns].fillna(method='ffill')

        elif strategy == 'backward_fill':
            # åå‘å¡«å……
            cleaned_data[columns] = cleaned_data[columns].fillna(method='bfill')

        # ç»Ÿè®¡å¤„ç†åçš„ç¼ºå¤±å€¼
        final_missing = cleaned_data[columns].isnull().sum()
        total_missing_final = final_missing.sum()

        cleaning_info = {
            'operation': 'handle_missing_values',
            'strategy': strategy,
            'columns': columns,
            'original_missing': total_missing_original,
            'handled_missing': total_missing_original - total_missing_final,
            'remaining_missing': total_missing_final,
            'original_missing_by_column': original_missing.to_dict(),
            'final_missing_by_column': final_missing.to_dict(),
            'timestamp': datetime.now().isoformat()
        }

        self.cleaning_log.append(cleaning_info)
        logger.info(f"å¤„ç†ç¼ºå¤±å€¼ï¼š{strategy}, å¤„ç† {cleaning_info['handled_missing']} ä¸ªç¼ºå¤±å€¼")

        return cleaned_data, cleaning_info

    def detect_and_handle_outliers(self, data: pd.DataFrame,
                                 columns: List[str] = None,
                                 method: str = 'iqr',
                                 action: str = 'cap') -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼

        Args:
            data: è¾“å…¥æ•°æ®
            columns: æŒ‡å®šå¤„ç†çš„åˆ—ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ•°å€¼åˆ—
            method: æ£€æµ‹æ–¹æ³•ï¼ˆiqr, zscore, isolation_forestï¼‰
            action: å¤„ç†æ–¹å¼ï¼ˆremove, cap, transformï¼‰

        Returns:
            æ¸…æ´—åçš„æ•°æ®å’Œå¤„ç†ä¿¡æ¯
        """
        if columns is None:
            columns = data.select_dtypes(include=[np.number]).columns.tolist()

        cleaned_data = data.copy()
        outlier_info = {
            'outliers_by_column': {},
            'total_outliers': 0,
            'handled_outliers': 0
        }

        for col in columns:
            if col not in cleaned_data.columns:
                continue

            original_values = cleaned_data[col].copy()
            outlier_mask = pd.Series(False, index=cleaned_data.index)

            if method == 'iqr':
                Q1 = cleaned_data[col].quantile(0.25)
                Q3 = cleaned_data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outlier_mask = (cleaned_data[col] < lower_bound) | (cleaned_data[col] > upper_bound)

            elif method == 'zscore':
                z_scores = np.abs((cleaned_data[col] - cleaned_data[col].mean()) / cleaned_data[col].std())
                outlier_mask = z_scores > 3

            outlier_count = outlier_mask.sum()
            outlier_info['outliers_by_column'][col] = outlier_count
            outlier_info['total_outliers'] += outlier_count

            if outlier_count > 0:
                if action == 'remove':
                    # åˆ é™¤å¼‚å¸¸å€¼
                    cleaned_data = cleaned_data[~outlier_mask]
                    outlier_info['handled_outliers'] += outlier_count

                elif action == 'cap':
                    # ç›–å¸½æ³•å¤„ç†
                    if method == 'iqr':
                        cleaned_data[col] = cleaned_data[col].clip(lower_bound, upper_bound)
                    else:
                        # Z-scoreæ–¹æ³•ä½¿ç”¨3å€æ ‡å‡†å·®ä½œä¸ºè¾¹ç•Œ
                        mean_val = cleaned_data[col].mean()
                        std_val = cleaned_data[col].std()
                        cleaned_data[col] = cleaned_data[col].clip(mean_val - 3*std_val, mean_val + 3*std_val)
                    outlier_info['handled_outliers'] += outlier_count

                elif action == 'transform':
                    # å¯¹æ•°å˜æ¢ï¼ˆä»…é€‚ç”¨äºæ­£å€¼ï¼‰
                    if (original_values > 0).all():
                        cleaned_data[col] = np.log1p(original_values)
                        outlier_info['handled_outliers'] += outlier_count

        cleaning_info = {
            'operation': 'handle_outliers',
            'method': method,
            'action': action,
            'columns': columns,
            **outlier_info,
            'timestamp': datetime.now().isoformat()
        }

        self.cleaning_log.append(cleaning_info)
        logger.info(f"å¼‚å¸¸å€¼å¤„ç†ï¼š{method}-{action}, "
                   f"æ£€æµ‹åˆ° {outlier_info['total_outliers']} ä¸ªå¼‚å¸¸å€¼ï¼Œ"
                   f"å¤„ç†äº† {outlier_info['handled_outliers']} ä¸ª")

        return cleaned_data, cleaning_info

    def standardize_data_types(self, data: pd.DataFrame,
                             type_mapping: Dict[str, str] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ ‡å‡†åŒ–æ•°æ®ç±»å‹

        Args:
            data: è¾“å…¥æ•°æ®
            type_mapping: ç±»å‹æ˜ å°„å­—å…¸ {åˆ—å: ç›®æ ‡ç±»å‹}

        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®å’Œè½¬æ¢ä¿¡æ¯
        """
        cleaned_data = data.copy()
        conversion_info = {
            'conversions': {},
            'failed_conversions': {}
        }

        # é»˜è®¤çš„ä»·æ ¼æ•°æ®ç±»å‹æ˜ å°„
        if type_mapping is None:
            type_mapping = {
                'symbol': 'string',
                'stock_name': 'string',
                'date': 'datetime64[ns]',
                'datetime': 'datetime64[ns]',
                'open': 'float64',
                'high': 'float64',
                'low': 'float64',
                'close': 'float64',
                'volume': 'int64',
                'amount': 'float64',
                'change_percent': 'float64'
            }

        for col, target_type in type_mapping.items():
            if col not in cleaned_data.columns:
                continue

            try:
                original_type = str(cleaned_data[col].dtype)

                if target_type == 'datetime64[ns]':
                    # æ—¥æœŸæ—¶é—´è½¬æ¢
                    cleaned_data[col] = pd.to_datetime(cleaned_data[col])
                elif target_type == 'string':
                    # å­—ç¬¦ä¸²è½¬æ¢
                    cleaned_data[col] = cleaned_data[col].astype('string')
                else:
                    # æ•°å€¼è½¬æ¢
                    cleaned_data[col] = pd.to_numeric(cleaned_data[col], errors='coerce')

                conversion_info['conversions'][col] = {
                    'from': original_type,
                    'to': str(cleaned_data[col].dtype),
                    'success': True
                }

            except Exception as e:
                conversion_info['failed_conversions'][col] = {
                    'target_type': target_type,
                    'error': str(e)
                }
                logger.warning(f"ç±»å‹è½¬æ¢å¤±è´¥ {col} -> {target_type}: {e}")

        cleaning_info = {
            'operation': 'standardize_data_types',
            'conversions': conversion_info['conversions'],
            'failed_conversions': conversion_info['failed_conversions'],
            'timestamp': datetime.now().isoformat()
        }

        self.cleaning_log.append(cleaning_info)
        logger.info(f"æ•°æ®ç±»å‹æ ‡å‡†åŒ–ï¼šæˆåŠŸ {len(conversion_info['conversions'])} ä¸ªï¼Œå¤±è´¥ {len(conversion_info['failed_conversions'])} ä¸ª")

        return cleaned_data, cleaning_info

    def validate_price_consistency(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        éªŒè¯å’Œä¿®æ­£ä»·æ ¼ä¸€è‡´æ€§

        Args:
            data: åŒ…å«ä»·æ ¼æ•°æ®çš„DataFrame

        Returns:
            ä¿®æ­£åçš„æ•°æ®å’Œä¿®æ­£ä¿¡æ¯
        """
        cleaned_data = data.copy()
        price_columns = ['open', 'high', 'low', 'close']
        price_columns = [col for col in price_columns if col in cleaned_data.columns]

        if len(price_columns) < 4:
            cleaning_info = {
                'operation': 'validate_price_consistency',
                'status': 'skipped',
                'reason': 'ç¼ºå°‘å¿…è¦çš„ä»·æ ¼åˆ—',
                'timestamp': datetime.now().isoformat()
            }
            return cleaned_data, cleaning_info

        corrections = []
        total_corrections = 0

        # æ£€æŸ¥æ¯è¡Œçš„ä»·æ ¼ä¸€è‡´æ€§
        for idx, row in cleaned_data.iterrows():
            row_corrections = 0

            # ç¡®ä¿é«˜ä»·ä¸ä½äºä½ä»·
            if row['high'] < row['low']:
                corrected_high = max(row['high'], row['low'], row['open'], row['close'])
                corrected_low = min(row['high'], row['low'], row['open'], row['close'])

                cleaned_data.at[idx, 'high'] = corrected_high
                cleaned_data.at[idx, 'low'] = corrected_low

                corrections.append({
                    'row': idx,
                    'type': 'high_low_swap',
                    'original': {'high': row['high'], 'low': row['low']},
                    'corrected': {'high': corrected_high, 'low': corrected_low}
                })
                row_corrections += 2

            # ç¡®ä¿å¼€ç›˜ä»·åœ¨é«˜ä½ä»·èŒƒå›´å†…
            if not (row['low'] <= row['open'] <= row['high']):
                corrected_open = np.clip(row['open'], row['low'], row['high'])
                corrections.append({
                    'row': idx,
                    'type': 'open_range',
                    'original': {'open': row['open']},
                    'corrected': {'open': corrected_open}
                })
                cleaned_data.at[idx, 'open'] = corrected_open
                row_corrections += 1

            # ç¡®ä¿æ”¶ç›˜ä»·åœ¨é«˜ä½ä»·èŒƒå›´å†…
            if not (row['low'] <= row['close'] <= row['high']):
                corrected_close = np.clip(row['close'], row['low'], row['high'])
                corrections.append({
                    'row': idx,
                    'type': 'close_range',
                    'original': {'close': row['close']},
                    'corrected': {'close': corrected_close}
                })
                cleaned_data.at[idx, 'close'] = corrected_close
                row_corrections += 1

            total_corrections += row_corrections

        cleaning_info = {
            'operation': 'validate_price_consistency',
            'total_corrections': total_corrections,
            'corrected_rows': len(set([c['row'] for c in corrections])),
            'corrections': corrections[:10],  # åªè®°å½•å‰10ä¸ªä¿®æ­£
            'timestamp': datetime.now().isoformat()
        }

        self.cleaning_log.append(cleaning_info)
        logger.info(f"ä»·æ ¼ä¸€è‡´æ€§æ£€æŸ¥ï¼šä¿®æ­£äº† {total_corrections} ä¸ªä»·æ ¼å¼‚å¸¸ï¼Œæ¶‰åŠ {cleaning_info['corrected_rows']} è¡Œ")

        return cleaned_data, cleaning_info

    def clean_data(self, data: pd.DataFrame, data_type: str = 'price_data',
                  cleaning_steps: List[str] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ¸…æ´—æµç¨‹

        Args:
            data: è¾“å…¥æ•°æ®
            data_type: æ•°æ®ç±»å‹
            cleaning_steps: æ¸…æ´—æ­¥éª¤åˆ—è¡¨

        Returns:
            æ¸…æ´—åçš„æ•°æ®å’Œæ¸…æ´—æŠ¥å‘Š
        """
        if cleaning_steps is None:
            cleaning_steps = [
                'remove_duplicates',
                'standardize_types',
                'handle_missing',
                'handle_outliers',
                'validate_consistency'
            ]

        logger.info(f"å¼€å§‹æ•°æ®æ¸…æ´—ï¼š{data_type}, åŸå§‹æ•°æ®é‡ï¼š{len(data)}")

        cleaned_data = data.copy()
        cleaning_report = {
            'data_type': data_type,
            'original_shape': data.shape,
            'final_shape': None,
            'steps_performed': [],
            'total_removed_rows': 0,
            'cleaning_time_start': datetime.now().isoformat()
        }

        for step in cleaning_steps:
            if step == 'remove_duplicates':
                # å¯¹äºä»·æ ¼æ•°æ®ï¼Œä½¿ç”¨æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç ä½œä¸ºé‡å¤åˆ¤æ–­ä¾æ®
                subset = ['symbol', 'date'] if data_type == 'price_data' else None
                cleaned_data, step_info = self.remove_duplicates(cleaned_data, subset=subset)

            elif step == 'standardize_types':
                cleaned_data, step_info = self.standardize_data_types(cleaned_data)

            elif step == 'handle_missing':
                # å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œä½¿ç”¨æ’å€¼å¤„ç†ç¼ºå¤±å€¼
                strategy = 'interpolate' if data_type == 'price_data' else 'fill'
                cleaned_data, step_info = self.handle_missing_values(cleaned_data, strategy=strategy)

            elif step == 'handle_outliers':
                # åªå¯¹ä»·æ ¼æ•°æ®ä½¿ç”¨IQRæ–¹æ³•ç›–å¸½å¤„ç†
                if data_type == 'price_data':
                    price_columns = ['open', 'high', 'low', 'close']
                    cleaned_data, step_info = self.detect_and_handle_outliers(
                        cleaned_data, columns=price_columns, action='cap'
                    )
                else:
                    step_info = {'operation': 'handle_outliers', 'skipped': True}

            elif step == 'validate_consistency':
                if data_type == 'price_data':
                    cleaned_data, step_info = self.validate_price_consistency(cleaned_data)
                else:
                    step_info = {'operation': 'validate_consistency', 'skipped': True}
            else:
                step_info = {'operation': step, 'skipped': True}

            cleaning_report['steps_performed'].append(step_info)

        # æœ€ç»ˆéªŒè¯
        final_validation = self.validator.validate(cleaned_data, data_type)
        cleaning_report['final_validation'] = final_validation

        cleaning_report['final_shape'] = cleaned_data.shape
        cleaning_report['total_removed_rows'] = data.shape[0] - cleaned_data.shape[0]
        cleaning_report['cleaning_time_end'] = datetime.now().isoformat()

        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼š{data_type}, æœ€ç»ˆæ•°æ®é‡ï¼š{len(cleaned_data)}, "
                   f"åˆ é™¤è¡Œæ•°ï¼š{cleaning_report['total_removed_rows']}")

        return cleaned_data, cleaning_report


def test_data_cleaner():
    """æµ‹è¯•æ•°æ®æ¸…æ´—å™¨"""
    cleaner = DataCleaner()

    # åˆ›å»ºåŒ…å«å„ç§é—®é¢˜çš„æµ‹è¯•æ•°æ®
    print("=== æµ‹è¯•æ•°æ®æ¸…æ´—å™¨ ===")

    test_data = pd.DataFrame({
        'symbol': ['000001', '000001', '000001', '000002', '000002', '000001'],  # åŒ…å«é‡å¤
        'date': ['2024-01-01', '2024-01-02', '2024-01-01', '2024-01-01', '2024-01-02', '2024-01-03'],
        'open': [10.0, 10.1, 10.0, 20.0, None, 10.2],  # åŒ…å«é‡å¤å’Œç¼ºå¤±å€¼
        'high': [9.5, 10.3, 10.2, 20.5, 20.8, 10.4],  # åŒ…å«ä»·æ ¼å¼‚å¸¸
        'low': [10.2, 9.9, 9.8, 19.8, 19.5, 9.9],     # åŒ…å«ä»·æ ¼å¼‚å¸¸
        'close': [10.1, 10.2, 10.1, 20.2, 20.3, 10.3],
        'volume': [1000000, 1200000, 1000000, -500000, 2000000, 800000]  # åŒ…å«è´Ÿå€¼
    })

    print("åŸå§‹æ•°æ®ï¼š")
    print(test_data)
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶ï¼š{test_data.shape}")

    # æ‰§è¡Œæ¸…æ´—
    cleaned_data, report = cleaner.clean_data(test_data, 'price_data')

    print(f"\næ¸…æ´—åæ•°æ®å½¢çŠ¶ï¼š{cleaned_data.shape}")
    print("æ¸…æ´—åæ•°æ®ï¼š")
    print(cleaned_data)

    print(f"\næ¸…æ´—æŠ¥å‘Šï¼š")
    print(f"åˆ é™¤è¡Œæ•°ï¼š{report['total_removed_rows']}")
    for step_info in report['steps_performed']:
        if 'removed_count' in step_info:
            print(f"- {step_info['operation']}: åˆ é™¤ {step_info['removed_count']} è¡Œ")
        if 'handled_missing' in step_info:
            print(f"- {step_info['operation']}: å¤„ç† {step_info['handled_missing']} ä¸ªç¼ºå¤±å€¼")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_data_cleaner()
```

### 3. åˆ›å»ºæ•°æ®è½¬æ¢å™¨

åˆ›å»º `src/data/processors/transformer.py`ï¼š

```python
"""
æ•°æ®è½¬æ¢å™¨
æä¾›æ•°æ®æ ¼å¼è½¬æ¢å’Œç‰¹å¾å·¥ç¨‹åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import logging
import talib

logger = logging.getLogger(__name__)


class DataTransformer:
    """æ•°æ®è½¬æ¢å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–è½¬æ¢å™¨"""
        self.transformation_log = []

    def normalize_price_data(self, data: pd.DataFrame,
                           method: str = 'minmax') -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ä»·æ ¼æ•°æ®æ ‡å‡†åŒ–

        Args:
            data: ä»·æ ¼æ•°æ®
            method: æ ‡å‡†åŒ–æ–¹æ³•ï¼ˆminmax, zscore, robustï¼‰

        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®å’Œè½¬æ¢ä¿¡æ¯
        """
        if data.empty:
            return data.copy(), {}

        price_columns = ['open', 'high', 'low', 'close']
        price_columns = [col for col in price_columns if col in data.columns]

        if not price_columns:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»·æ ¼åˆ—ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
            return data.copy(), {}

        transformed_data = data.copy()
        scaling_params = {}

        for col in price_columns:
            if method == 'minmax':
                min_val = transformed_data[col].min()
                max_val = transformed_data[col].max()
                if max_val > min_val:
                    transformed_data[f'{col}_normalized'] = (transformed_data[col] - min_val) / (max_val - min_val)
                else:
                    transformed_data[f'{col}_normalized'] = 0

                scaling_params[col] = {'min': min_val, 'max': max_val}

            elif method == 'zscore':
                mean_val = transformed_data[col].mean()
                std_val = transformed_data[col].std()
                if std_val > 0:
                    transformed_data[f'{col}_normalized'] = (transformed_data[col] - mean_val) / std_val
                else:
                    transformed_data[f'{col}_normalized'] = 0

                scaling_params[col] = {'mean': mean_val, 'std': std_val}

            elif method == 'robust':
                median_val = transformed_data[col].median()
                mad_val = (transformed_data[col] - median_val).abs().median()  # Median Absolute Deviation
                if mad_val > 0:
                    transformed_data[f'{col}_normalized'] = (transformed_data[col] - median_val) / mad_val
                else:
                    transformed_data[f'{col}_normalized'] = 0

                scaling_params[col] = {'median': median_val, 'mad': mad_val}

        transformation_info = {
            'operation': 'normalize_price_data',
            'method': method,
            'columns': price_columns,
            'scaling_params': scaling_params,
            'timestamp': datetime.now().isoformat()
        }

        self.transformation_log.append(transformation_info)
        logger.info(f"ä»·æ ¼æ•°æ®æ ‡å‡†åŒ–å®Œæˆï¼š{method}, å¤„ç†äº† {len(price_columns)} åˆ—")

        return transformed_data, transformation_info

    def calculate_technical_indicators(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        è®¡ç®—æŠ€æœ¯æŒ‡æ ‡

        Args:
            data: OHLCVæ•°æ®

        Returns:
            åŒ…å«æŠ€æœ¯æŒ‡æ ‡çš„æ•°æ®å’Œè®¡ç®—ä¿¡æ¯
        """
        if data.empty or len(data) < 20:
            logger.warning("æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æŠ€æœ¯æŒ‡æ ‡è®¡ç®—")
            return data.copy(), {'operation': 'calculate_technical_indicators', 'skipped': True}

        required_columns = ['open', 'high', 'low', 'close', 'volume']
        missing_columns = [col for col in required_columns if col not in data.columns]

        if missing_columns:
            logger.warning(f"ç¼ºå°‘å¿…è¦åˆ— {missing_columns}ï¼Œè·³è¿‡æŠ€æœ¯æŒ‡æ ‡è®¡ç®—")
            return data.copy(), {'operation': 'calculate_technical_indicators', 'missing_columns': missing_columns}

        transformed_data = data.copy()
        indicators_calculated = []

        try:
            # ç§»åŠ¨å¹³å‡çº¿
            for period in [5, 10, 20, 60]:
                transformed_data[f'ma_{period}'] = talib.SMA(data['close'].values, timeperiod=period)
                indicators_calculated.append(f'ma_{period}')

            # æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿
            for period in [12, 26]:
                transformed_data[f'ema_{period}'] = talib.EMA(data['close'].values, timeperiod=period)
                indicators_calculated.append(f'ema_{period}')

            # MACD
            macd, macd_signal, macd_hist = talib.MACD(data['close'].values)
            transformed_data['macd'] = macd
            transformed_data['macd_signal'] = macd_signal
            transformed_data['macd_histogram'] = macd_hist
            indicators_calculated.extend(['macd', 'macd_signal', 'macd_histogram'])

            # RSI
            rsi = talib.RSI(data['close'].values)
            transformed_data['rsi'] = rsi
            indicators_calculated.append('rsi')

            # å¸ƒæ—å¸¦
            bb_upper, bb_middle, bb_lower = talib.BBANDS(data['close'].values)
            transformed_data['bb_upper'] = bb_upper
            transformed_data['bb_middle'] = bb_middle
            transformed_data['bb_lower'] = bb_lower
            transformed_data['bb_width'] = (bb_upper - bb_lower) / bb_middle
            transformed_data['bb_position'] = (data['close'] - bb_lower) / (bb_upper - bb_lower)
            indicators_calculated.extend(['bb_upper', 'bb_middle', 'bb_lower', 'bb_width', 'bb_position'])

            # KDJ
            high_values = data['high'].values
            low_values = data['low'].values
            close_values = data['close'].values

            k, d = talib.STOCH(high_values, low_values, close_values)
            transformed_data['stoch_k'] = k
            transformed_data['stoch_d'] = d
            transformed_data['stoch_j'] = 3 * k - 2 * d
            indicators_calculated.extend(['stoch_k', 'stoch_d', 'stoch_j'])

            # æˆäº¤é‡æŒ‡æ ‡
            transformed_data['volume_sma_20'] = talib.SMA(data['volume'].values, timeperiod=20)
            transformed_data['volume_ratio'] = data['volume'] / transformed_data['volume_sma_20']
            indicators_calculated.extend(['volume_sma_20', 'volume_ratio'])

            # ä»·æ ¼å˜åŒ–æŒ‡æ ‡
            transformed_data['price_change'] = data['close'].pct_change()
            transformed_data['price_change_5'] = data['close'].pct_change(5)
            transformed_data['high_low_ratio'] = data['high'] / data['low']
            transformed_data['open_close_ratio'] = data['open'] / data['close']
            indicators_calculated.extend(['price_change', 'price_change_5', 'high_low_ratio', 'open_close_ratio'])

            transformation_info = {
                'operation': 'calculate_technical_indicators',
                'indicators_calculated': indicators_calculated,
                'data_points': len(data),
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼š{e}")
            transformation_info = {
                'operation': 'calculate_technical_indicators',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

        self.transformation_log.append(transformation_info)
        logger.info(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆï¼š{len(indicators_calculated) if 'indicators_calculated' in locals() else 0} ä¸ªæŒ‡æ ‡")

        return transformed_data, transformation_info

    def resample_data(self, data: pd.DataFrame,
                     timeframe: str = 'D',
                     date_column: str = 'date') -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ•°æ®é‡é‡‡æ ·ï¼ˆæ—¶é—´å‘¨æœŸè½¬æ¢ï¼‰

        Args:
            data: è¾“å…¥æ•°æ®
            timeframe: ç›®æ ‡æ—¶é—´å‘¨æœŸï¼ˆT=åˆ†é’Ÿ, H=å°æ—¶, D=å¤©, W=å‘¨, M=æœˆï¼‰
            date_column: æ—¥æœŸåˆ—å

        Returns:
            é‡é‡‡æ ·åçš„æ•°æ®å’Œè½¬æ¢ä¿¡æ¯
        """
        if date_column not in data.columns:
            logger.error(f"æ—¥æœŸåˆ— '{date_column}' ä¸å­˜åœ¨")
            return data.copy(), {'error': f'Missing date column: {date_column}'}

        if data.empty:
            return data.copy(), {'operation': 'resample_data', 'skipped': 'Empty data'}

        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        data_copy = data.copy()
        data_copy[date_column] = pd.to_datetime(data_copy[date_column])
        data_copy = data_copy.set_index(date_column)

        # å®šä¹‰é‡é‡‡æ ·è§„åˆ™
        resample_rules = {
            'T': '1T',    # 1åˆ†é’Ÿ
            '5T': '5T',   # 5åˆ†é’Ÿ
            '15T': '15T', # 15åˆ†é’Ÿ
            '30T': '30T', # 30åˆ†é’Ÿ
            'H': '1H',    # 1å°æ—¶
            'D': '1D',    # 1å¤©
            'W': '1W',    # 1å‘¨
            'M': '1M'     # 1æœˆ
        }

        rule = resample_rules.get(timeframe, timeframe)

        try:
            # OHLCVé‡é‡‡æ ·
            if all(col in data_copy.columns for col in ['open', 'high', 'low', 'close']):
                resampled = data_copy.resample(rule).agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min',
                    'close': 'last',
                    'volume': 'sum'
                })
            else:
                # é€šç”¨é‡é‡‡æ ·
                numeric_columns = data_copy.select_dtypes(include=[np.number]).columns
                resampled = data_copy[numeric_columns].resample(rule).mean()

            # ç§»é™¤ç©ºè¡Œ
            resampled = resampled.dropna()

            # é‡ç½®ç´¢å¼•
            resampled = resampled.reset_index()

            transformation_info = {
                'operation': 'resample_data',
                'original_timeframe': 'original',
                'target_timeframe': timeframe,
                'original_rows': len(data),
                'resampled_rows': len(resampled),
                'compression_ratio': len(resampled) / len(data) if len(data) > 0 else 0,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"æ•°æ®é‡é‡‡æ ·å¤±è´¥ï¼š{e}")
            resampled = data.reset_index()
            transformation_info = {
                'operation': 'resample_data',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

        self.transformation_log.append(transformation_info)
        logger.info(f"æ•°æ®é‡é‡‡æ ·å®Œæˆï¼š{len(data)} -> {len(resampled)} è¡Œ")

        return resampled, transformation_info

    def create_features(self, data: pd.DataFrame,
                       feature_types: List[str] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ç‰¹å¾å·¥ç¨‹

        Args:
            data: è¾“å…¥æ•°æ®
            feature_types: ç‰¹å¾ç±»å‹åˆ—è¡¨

        Returns:
            åŒ…å«æ–°ç‰¹å¾çš„æ•°æ®å’Œç‰¹å¾ä¿¡æ¯
        """
        if feature_types is None:
            feature_types = ['lag', 'rolling', 'time', 'interaction']

        if data.empty:
            return data.copy(), {'operation': 'create_features', 'skipped': 'Empty data'}

        transformed_data = data.copy()
        features_created = []

        try:
            # æ»åç‰¹å¾
            if 'lag' in feature_types and 'close' in transformed_data.columns:
                for lag in [1, 2, 3, 5, 10]:
                    transformed_data[f'close_lag_{lag}'] = transformed_data['close'].shift(lag)
                    features_created.append(f'close_lag_{lag}')

            # æ»šåŠ¨çª—å£ç‰¹å¾
            if 'rolling' in feature_types and 'close' in transformed_data.columns:
                for window in [5, 10, 20]:
                    transformed_data[f'close_mean_{window}'] = transformed_data['close'].rolling(window).mean()
                    transformed_data[f'close_std_{window}'] = transformed_data['close'].rolling(window).std()
                    transformed_data[f'close_max_{window}'] = transformed_data['close'].rolling(window).max()
                    transformed_data[f'close_min_{window}'] = transformed_data['close'].rolling(window).min()

                    features_created.extend([
                        f'close_mean_{window}',
                        f'close_std_{window}',
                        f'close_max_{window}',
                        f'close_min_{window}'
                    ])

            # æ—¶é—´ç‰¹å¾
            if 'time' in feature_types and 'date' in transformed_data.columns:
                transformed_data['date'] = pd.to_datetime(transformed_data['date'])
                transformed_data['day_of_week'] = transformed_data['date'].dt.dayofweek
                transformed_data['month'] = transformed_data['date'].dt.month
                transformed_data['quarter'] = transformed_data['date'].dt.quarter
                transformed_data['is_month_end'] = transformed_data['date'].dt.is_month_end.astype(int)

                features_created.extend(['day_of_week', 'month', 'quarter', 'is_month_end'])

            # äº¤äº’ç‰¹å¾
            if 'interaction' in feature_types:
                if all(col in transformed_data.columns for col in ['high', 'low', 'close']):
                    transformed_data['price_range'] = transformed_data['high'] - transformed_data['low']
                    transformed_data['price_position'] = (transformed_data['close'] - transformed_data['low']) / (transformed['high'] - transformed_data['low'])
                    transformed_data['body_size'] = abs(transformed_data['close'] - transformed_data['open']) / transformed_data['open']

                    features_created.extend(['price_range', 'price_position', 'body_size'])

            transformation_info = {
                'operation': 'create_features',
                'feature_types': feature_types,
                'features_created': features_created,
                'total_features': len(features_created),
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"ç‰¹å¾åˆ›å»ºå¤±è´¥ï¼š{e}")
            transformation_info = {
                'operation': 'create_features',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

        self.transformation_log.append(transformation_info)
        logger.info(f"ç‰¹å¾åˆ›å»ºå®Œæˆï¼š{len(features_created) if 'features_created' in locals() else 0} ä¸ªç‰¹å¾")

        return transformed_data, transformation_info

    def transform_data(self, data: pd.DataFrame,
                      transformations: List[str] = None,
                      data_type: str = 'price_data') -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®è½¬æ¢æµç¨‹

        Args:
            data: è¾“å…¥æ•°æ®
            transformations: è½¬æ¢æ­¥éª¤åˆ—è¡¨
            data_type: æ•°æ®ç±»å‹

        Returns:
            è½¬æ¢åçš„æ•°æ®å’Œè½¬æ¢æŠ¥å‘Š
        """
        if transformations is None:
            transformations = ['normalize', 'indicators', 'features']

        logger.info(f"å¼€å§‹æ•°æ®è½¬æ¢ï¼š{data_type}, æ•°æ®é‡ï¼š{len(data)}")

        transformed_data = data.copy()
        transformation_report = {
            'data_type': data_type,
            'original_shape': data.shape,
            'final_shape': None,
            'transformations_performed': [],
            'transformation_time_start': datetime.now().isoformat()
        }

        for transform in transformations:
            if transform == 'normalize' and data_type == 'price_data':
                transformed_data, step_info = self.normalize_price_data(transformed_data)

            elif transform == 'indicators' and data_type == 'price_data':
                transformed_data, step_info = self.calculate_technical_indicators(transformed_data)

            elif transform == 'features':
                feature_types = ['lag', 'rolling', 'time', 'interaction']
                transformed_data, step_info = self.create_features(transformed_data, feature_types)

            elif transform.startswith('resample_'):
                timeframe = transform.split('_')[1]
                transformed_data, step_info = self.resample_data(transformed_data, timeframe)

            else:
                step_info = {'operation': transform, 'skipped': True}

            transformation_report['transformations_performed'].append(step_info)

        transformation_report['final_shape'] = transformed_data.shape
        transformation_report['transformation_time_end'] = datetime.now().isoformat()

        logger.info(f"æ•°æ®è½¬æ¢å®Œæˆï¼š{data_type}, æœ€ç»ˆæ•°æ®é‡ï¼š{len(transformed_data)}")

        return transformed_data, transformation_report


def test_data_transformer():
    """æµ‹è¯•æ•°æ®è½¬æ¢å™¨"""
    transformer = DataTransformer()

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("=== æµ‹è¯•æ•°æ®è½¬æ¢å™¨ ===")

    # ç”Ÿæˆ60å¤©çš„æ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®
    dates = pd.date_range('2024-01-01', periods=60, freq='D')
    np.random.seed(42)

    # æ¨¡æ‹Ÿä»·æ ¼èµ°åŠ¿
    base_price = 100
    returns = np.random.normal(0, 0.02, 60)
    prices = [base_price]
    for ret in returns[1:]:
        prices.append(prices[-1] * (1 + ret))

    test_data = pd.DataFrame({
        'symbol': ['000001'] * 60,
        'date': dates,
        'open': prices,
        'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
        'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
        'close': prices,
        'volume': np.random.randint(1000000, 5000000, 60)
    })

    print(f"æµ‹è¯•æ•°æ®ï¼š{len(test_data)} è¡Œ")
    print(test_data.head())

    # æ‰§è¡Œè½¬æ¢
    transformed_data, report = transformer.transform_data(test_data, 'price_data')

    print(f"\nè½¬æ¢åæ•°æ®ï¼š{len(transformed_data)} è¡Œ x {len(transformed_data.columns)} åˆ—")
    print("æ–°å¢åˆ—ï¼š", [col for col in transformed_data.columns if col not in test_data.columns][:10])

    if not transformed_data.empty:
        print("\næŠ€æœ¯æŒ‡æ ‡ç¤ºä¾‹ï¼š")
        if 'rsi' in transformed_data.columns:
            print(f"RSIèŒƒå›´ï¼š{transformed_data['rsi'].min():.2f} - {transformed_data['rsi'].max():.2f}")
        if 'ma_20' in transformed_data.columns:
            print(f"MA20èŒƒå›´ï¼š{transformed_data['ma_20'].min():.2f} - {transformed_data['ma_20'].max():.2f}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_data_transformer()
```

---

## ğŸ§ª æµ‹è¯•æ•°æ®æ¸…æ´—æµç¨‹

### 1. åˆ›å»ºç»¼åˆæµ‹è¯•è„šæœ¬

åˆ›å»º `scripts/test_data_processing.py`ï¼š

```python
#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†æµç¨‹ç»¼åˆæµ‹è¯•è„šæœ¬
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging

from src.data.processors.validator import DataValidator
from src.data.processors.cleaner import DataCleaner
from src.data.processors.transformer import DataTransformer

def create_test_data():
    """åˆ›å»ºåŒ…å«å„ç§æ•°æ®è´¨é‡é—®é¢˜çš„æµ‹è¯•æ•°æ®"""

    # ç”Ÿæˆ30å¤©çš„æµ‹è¯•æ•°æ®
    dates = pd.date_range('2024-01-01', periods=30, freq='D')

    # æ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®ï¼ŒåŒ…å«å„ç§é—®é¢˜
    np.random.seed(42)
    base_price = 100.0
    price_changes = np.random.normal(0, 0.03, 30)
    prices = [base_price]
    for change in price_changes[1:]:
        new_price = prices[-1] * (1 + change)
        prices.append(max(new_price, 1.0))  # ç¡®ä¿ä»·æ ¼ä¸ºæ­£

    test_data = pd.DataFrame({
        'symbol': ['000001'] * 30 + ['000001'] + ['000002'] * 30,  # åŒ…å«é‡å¤
        'date': list(dates) + [dates[5]] + list(dates),              # åŒ…å«é‡å¤æ—¥æœŸ
        'open': prices + [None] + [p * 0.95 for p in prices],        # åŒ…å«ç¼ºå¤±å€¼
        'high': [p * 1.05 for p in prices] + [80.0] + [p * 1.08 for p in prices],  # åŒ…å«å¼‚å¸¸å€¼
        'low': [p * 0.95 for p in prices] + [120.0] + [p * 0.92 for p in prices],   # åŒ…å«å¼‚å¸¸å€¼
        'close': prices + [105.0] + prices,                         # åŒ…å«å¼‚å¸¸å€¼
        'volume': np.random.randint(1000000, 5000000, 61)           # æ­£å¸¸æˆäº¤é‡
    })

    # æ·»åŠ ä¸€äº›é¢å¤–çš„é—®é¢˜
    test_data.loc[10, 'volume'] = -1000000      # è´Ÿæˆäº¤é‡
    test_data.loc[15, 'open'] = None           # ç¼ºå¤±å¼€ç›˜ä»·
    test_data.loc[20, 'close'] = 0.01          # å¼‚å¸¸ä½ä»·

    return test_data

def main():
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("ğŸš€ å¼€å§‹æµ‹è¯•æ•°æ®å¤„ç†æµç¨‹...\n")

    # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
    print("=== 1. åˆ›å»ºæµ‹è¯•æ•°æ® ===")
    test_data = create_test_data()
    print(f"åŸå§‹æµ‹è¯•æ•°æ®ï¼š{len(test_data)} è¡Œ x {len(test_data.columns)} åˆ—")
    print("æ•°æ®é¢„è§ˆï¼š")
    print(test_data.head(10))
    print()

    # 2. æ•°æ®éªŒè¯
    print("=== 2. æ•°æ®éªŒè¯ ===")
    validator = DataValidator()

    validation_result = validator.validate(test_data, 'price_data', check_completeness=True)

    print(f"éªŒè¯ç»“æœï¼š{'é€šè¿‡' if validation_result['is_valid'] else 'å¤±è´¥'}")
    print(f"é”™è¯¯æ•°é‡ï¼š{validation_result['summary']['total_errors']}")
    print(f"è­¦å‘Šæ•°é‡ï¼š{validation_result['summary']['total_warnings']}")

    if validation_result['structure']['errors']:
        print("ç»“æ„é”™è¯¯ï¼š", validation_result['structure']['errors'])

    if validation_result['content']['errors']:
        print("å†…å®¹é”™è¯¯ï¼š", validation_result['content']['errors'])

    if validation_result['content']['warnings']:
        print("å†…å®¹è­¦å‘Šï¼š", validation_result['content']['warnings'])

    print()

    # 3. æ•°æ®æ¸…æ´—
    print("=== 3. æ•°æ®æ¸…æ´— ===")
    cleaner = DataCleaner()

    cleaning_steps = [
        'remove_duplicates',
        'standardize_types',
        'handle_missing',
        'handle_outliers',
        'validate_consistency'
    ]

    cleaned_data, cleaning_report = cleaner.clean_data(test_data, 'price_data', cleaning_steps)

    print(f"æ¸…æ´—å‰ï¼š{test_data.shape}")
    print(f"æ¸…æ´—åï¼š{cleaned_data.shape}")
    print(f"åˆ é™¤è¡Œæ•°ï¼š{cleaning_report['total_removed_rows']}")

    # æ˜¾ç¤ºå„æ­¥éª¤çš„æ¸…æ´—ç»“æœ
    for step_info in cleaning_report['steps_performed']:
        if 'removed_count' in step_info:
            print(f"- {step_info['operation']}: åˆ é™¤ {step_info['removed_count']} è¡Œ")
        elif 'handled_missing' in step_info:
            print(f"- {step_info['operation']}: å¤„ç† {step_info['handled_missing']} ä¸ªç¼ºå¤±å€¼")
        elif 'total_corrections' in step_info:
            print(f"- {step_info['operation']}: ä¿®æ­£ {step_info['total_corrections']} ä¸ªå¼‚å¸¸")

    print()

    # 4. éªŒè¯æ¸…æ´—åçš„æ•°æ®
    print("=== 4. æ¸…æ´—åæ•°æ®éªŒè¯ ===")
    post_cleaning_validation = validator.validate(cleaned_data, 'price_data')

    print(f"æ¸…æ´—åéªŒè¯ï¼š{'é€šè¿‡' if post_cleaning_validation['is_valid'] else 'å¤±è´¥'}")
    print(f"å‰©ä½™é”™è¯¯ï¼š{post_cleaning_validation['summary']['total_errors']}")
    print(f"å‰©ä½™è­¦å‘Šï¼š{post_cleaning_validation['summary']['total_warnings']}")

    if post_cleaning_validation['content']['quality_metrics']:
        quality_metrics = post_cleaning_validation['content']['quality_metrics']
        print("è´¨é‡æŒ‡æ ‡ï¼š")
        print(f"  ç¼ºå¤±å€¼æ¯”ä¾‹ï¼š{sum(quality_metrics['missing_ratio'].values()):.4f}")
        print(f"  é‡å¤è¡Œæ¯”ä¾‹ï¼š{quality_metrics['duplicate_ratio']:.4f}")

    print()

    # 5. æ•°æ®è½¬æ¢
    print("=== 5. æ•°æ®è½¬æ¢ ===")
    transformer = DataTransformer()

    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
    if len(cleaned_data) >= 20:
        transformations = ['normalize', 'indicators', 'features']
        transformed_data, transformation_report = transformer.transform_data(
            cleaned_data, 'price_data', transformations
        )

        print(f"è½¬æ¢å‰ï¼š{cleaned_data.shape}")
        print(f"è½¬æ¢åï¼š{transformed_data.shape}")

        # ç»Ÿè®¡æ–°å¢çš„ç‰¹å¾
        original_columns = set(cleaned_data.columns)
        new_columns = set(transformed_data.columns) - original_columns
        print(f"æ–°å¢ç‰¹å¾æ•°é‡ï¼š{len(new_columns)}")
        print("éƒ¨åˆ†æ–°å¢ç‰¹å¾ï¼š", list(new_columns)[:10])

        print()

    # 6. æœ€ç»ˆæ•°æ®è´¨é‡æŠ¥å‘Š
    print("=== 6. æœ€ç»ˆæ•°æ®è´¨é‡æŠ¥å‘Š ===")

    final_data = transformed_data if 'transformed_data' in locals() else cleaned_data
    final_validation = validator.validate(final_data, 'price_data')

    print("ğŸ“Š æ•°æ®å¤„ç†æ€»ç»“ï¼š")
    print(f"  åŸå§‹æ•°æ®é‡ï¼š{len(test_data)} è¡Œ")
    print(f"  æœ€ç»ˆæ•°æ®é‡ï¼š{len(final_data)} è¡Œ")
    print(f"  æ•°æ®ä¿ç•™ç‡ï¼š{len(final_data)/len(test_data):.2%}")
    print(f"  æœ€ç»ˆéªŒè¯çŠ¶æ€ï¼š{'âœ… é€šè¿‡' if final_validation['is_valid'] else 'âŒ å¤±è´¥'}")
    print(f"  æœ€ç»ˆç‰¹å¾æ•°ï¼š{len(final_data.columns)}")

    if final_validation['content']['quality_metrics']:
        metrics = final_validation['content']['quality_metrics']
        print(f"  æ•°æ®å®Œæ•´æ€§ï¼š{(1-metrics['missing_ratio'].get('close', 0)):.2%}")
        print(f"  æ•°æ®ä¸€è‡´æ€§ï¼š{'âœ…' if not final_validation['content']['errors'] else 'âŒ'}")

    print("\nğŸ‰ æ•°æ®å¤„ç†æµç¨‹æµ‹è¯•å®Œæˆï¼")

    # 7. ä¿å­˜å¤„ç†åçš„æ ·æœ¬æ•°æ®ï¼ˆå¯é€‰ï¼‰
    output_path = "data/processed/cleaned_sample_data.csv"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    try:
        final_data.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ å¤„ç†åæ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_path}")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜æ•°æ®å¤±è´¥ï¼š{e}")


if __name__ == "__main__":
    main()
```

### 2. è¿è¡Œæµ‹è¯•

```bash
# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒæ¿€æ´»
source .venv/bin/activate

# å®‰è£…TA-Libï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
# Ubuntu/WSL:
sudo apt-get install build-essential
# ç„¶åå®‰è£…PythonåŒ…
.venv/bin/pip install TA-Lib

# è¿è¡Œæ•°æ®å¤„ç†æµ‹è¯•
python scripts/test_data_processing.py
```

---

## ğŸ“ æ•°æ®è´¨é‡é…ç½®

### 1. åˆ›å»ºæ•°æ®è´¨é‡é…ç½®

åˆ›å»º `config/data_quality.yaml`ï¼š

```yaml
# æ•°æ®è´¨é‡é…ç½®

validation_rules:
  price_data:
    required_columns: ["symbol", "date", "open", "high", "low", "close", "volume"]
    numeric_columns: ["open", "high", "low", "close", "volume"]
    positive_columns: ["open", "high", "low", "close", "volume"]

    price_constraints:
      max_change_percent: 0.20      # æœ€å¤§æ¶¨è·Œå¹…
      min_price: 0.01              # æœ€å°ä»·æ ¼
      max_price: 10000             # æœ€å¤§ä»·æ ¼
      min_volume: 0                # æœ€å°æˆäº¤é‡
      max_volume: 100000000000     # æœ€å¤§æˆäº¤é‡

    quality_thresholds:
      missing_ratio_max: 0.05      # æœ€å¤§ç¼ºå¤±å€¼æ¯”ä¾‹ 5%
      duplicate_ratio_max: 0.01    # æœ€å¤§é‡å¤å€¼æ¯”ä¾‹ 1%
      completeness_ratio_min: 0.95 # æœ€å°å®Œæ•´æ€§ 95%

  stock_info:
    required_columns: ["symbol", "stock_name"]
    string_columns: ["symbol", "stock_name"]
    symbol_pattern: "^\\d{6}$"

    constraints:
      symbol_length: 6
      name_max_length: 50

cleaning_rules:
  missing_values:
    default_strategy: "interpolate"  # interpolate, fill, drop
    fill_methods:
      numeric: "median"
      string: "mode"

  outliers:
    detection_method: "iqr"          # iqr, zscore
    action: "cap"                    # cap, remove, transform
    iqr_multiplier: 1.5

  duplicates:
    strategy: "keep_first"           # keep_first, keep_last, drop_all

  price_consistency:
    enable_correction: true
    correction_method: "clip"        # clip, adjust

transformation_rules:
  technical_indicators:
    enable: true
    indicators:
      - ma: [5, 10, 20, 60]
      - ema: [12, 26]
      - macd: {}
      - rsi: {}
      - bollinger_bands: {}
      - stoch: {}

  feature_engineering:
    lag_features: [1, 2, 3, 5, 10]
    rolling_windows: [5, 10, 20]
    time_features: true
    interaction_features: true

monitoring:
  alerts:
    high_error_rate: 0.10           # é”™è¯¯ç‡è¶…è¿‡10%å‘Šè­¦
    low_completeness: 0.90          # å®Œæ•´æ€§ä½äº90%å‘Šè­¦
    data_freshness: 24               # æ•°æ®è¶…è¿‡24å°æ—¶æœªæ›´æ–°å‘Šè­¦

  logging:
    level: "INFO"
    save_logs: true
    log_retention_days: 30
```

---

## ğŸ¯ æœ¬èŠ‚å°ç»“

### âœ… å®Œæˆå†…å®¹

1. **æ•°æ®éªŒè¯å™¨**: å®ç°äº†ç»“æ„éªŒè¯ã€å†…å®¹éªŒè¯ã€å®Œæ•´æ€§æ£€æŸ¥ç­‰åŠŸèƒ½
2. **æ•°æ®æ¸…æ´—å™¨**: å®ç°äº†å»é‡ã€ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€ä»·æ ¼ä¸€è‡´æ€§éªŒè¯ç­‰åŠŸèƒ½
3. **æ•°æ®è½¬æ¢å™¨**: å®ç°äº†æ ‡å‡†åŒ–ã€æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®é‡é‡‡æ ·ç­‰åŠŸèƒ½
4. **è´¨é‡ç›‘æ§**: å»ºç«‹äº†å®Œæ•´çš„æ•°æ®è´¨é‡è¯„ä¼°å’Œç›‘æ§ä½“ç³»
5. **æµ‹è¯•æ¡†æ¶**: åˆ›å»ºäº†ç»¼åˆæµ‹è¯•è„šæœ¬æ¥éªŒè¯æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹

### ğŸ“Š æ•°æ®è´¨é‡æŒ‡æ ‡

| è´¨é‡ç»´åº¦ | æ£€æŸ¥é¡¹ç›® | å¤„ç†æ–¹æ³• |
|---------|---------|----------|
| å®Œæ•´æ€§ | ç¼ºå¤±å€¼æ£€æµ‹ | æ’å€¼/å¡«å……/åˆ é™¤ |
| ä¸€è‡´æ€§ | ä»·æ ¼å…³ç³»éªŒè¯ | è‡ªåŠ¨ä¿®æ­£ |
| å‡†ç¡®æ€§ | å¼‚å¸¸å€¼æ£€æµ‹ | IQR/Z-scoreæ–¹æ³• |
| å”¯ä¸€æ€§ | é‡å¤æ•°æ®æ£€æµ‹ | ä¿ç•™ç­–ç•¥å¤„ç† |
| åŠæ—¶æ€§ | æ•°æ®æ–°é²œåº¦ | æ—¶é—´æˆ³éªŒè¯ |

### ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

1. **è‡ªåŠ¨åŒ–æ¸…æ´—**: ä¸€é”®æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ¸…æ´—æµç¨‹
2. **æ™ºèƒ½ä¿®å¤**: è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤å¸¸è§çš„æ•°æ®è´¨é‡é—®é¢˜
3. **è´¨é‡è¯„ä¼°**: æä¾›è¯¦ç»†çš„æ•°æ®è´¨é‡æŠ¥å‘Šå’Œå»ºè®®
4. **ç‰¹å¾å·¥ç¨‹**: è‡ªåŠ¨ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡å’Œäº¤æ˜“ç‰¹å¾
5. **é…ç½®åŒ–ç®¡ç†**: é€šè¿‡é…ç½®æ–‡ä»¶çµæ´»æ§åˆ¶å¤„ç†è§„åˆ™

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°æ®ä¾èµ–**: æŠ€æœ¯æŒ‡æ ‡è®¡ç®—éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
2. **æ€§èƒ½è€ƒè™‘**: å¤§æ•°æ®é‡å¤„ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´
3. **ä¸šåŠ¡é€»è¾‘**: ä»·æ ¼ä¿®æ­£è§„åˆ™éœ€è¦æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚è°ƒæ•´
4. **ç›‘æ§å‘Šè­¦**: å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶ï¼ŒåŠæ—¶å‘ç°é—®é¢˜

### ğŸ”„ ä¸‹ä¸€æ­¥

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ **ClickHouseæ•°æ®è¡¨è®¾è®¡**ï¼Œè®¾è®¡é«˜æ€§èƒ½çš„æ—¶åºæ•°æ®å­˜å‚¨ç»“æ„ã€‚

**[â†’ å‰å¾€ 2.3 ClickHouseæ•°æ®è¡¨è®¾è®¡](2.3-ClickHouseæ•°æ®è¡¨è®¾è®¡.md)**

---

## ğŸ†˜ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç¡®ä¿å®‰è£…äº†TA-Libï¼š`pip install TA-Lib`
2. æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„è¡Œæ•°ï¼ˆè‡³å°‘éœ€è¦20è¡Œè®¡ç®—MA20ï¼‰
3. ç¡®ä¿æ•°æ®åŒ…å«OHLCVåˆ—ä¸”æ— å¼‚å¸¸å€¼
4. æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹æ•°æ®

### Q2: æ•°æ®æ¸…æ´—åæ•°æ®é‡å‡å°‘å¤ªå¤šæ€ä¹ˆåŠï¼Ÿ

**A**: å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š
1. **è¿‡åº¦å»é‡**: æ£€æŸ¥é‡å¤åˆ¤æ–­æ¡ä»¶æ˜¯å¦åˆé€‚
2. **å¼‚å¸¸å€¼å¤„ç†è¿‡ä¸¥**: è°ƒæ•´IQRå€æ•°æˆ–Z-scoreé˜ˆå€¼
3. **ç¼ºå¤±å€¼å¤„ç†**: è€ƒè™‘ä½¿ç”¨æ’å€¼è€Œéåˆ é™¤
4. **ä¸šåŠ¡è§„åˆ™**: ç¡®ä¿æ¸…æ´—è§„åˆ™ç¬¦åˆå®é™…ä¸šåŠ¡éœ€æ±‚

### Q3: ç‰¹å¾å·¥ç¨‹è®¡ç®—è€—æ—¶é•¿æ€ä¹ˆåŠï¼Ÿ

**A**: ä¼˜åŒ–å»ºè®®ï¼š
1. å‡å°‘ç‰¹å¾æ•°é‡ï¼Œåªä¿ç•™å…³é”®ç‰¹å¾
2. ä½¿ç”¨å‘é‡åŒ–æ“ä½œæé«˜è®¡ç®—æ•ˆç‡
3. è€ƒè™‘ä½¿ç”¨Rustæ¨¡å—åŠ é€Ÿè®¡ç®—
4. åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†

### Q4: å¦‚ä½•éªŒè¯æ¸…æ´—åçš„æ•°æ®è´¨é‡ï¼Ÿ

**A**: éªŒè¯æ–¹æ³•ï¼š
1. è¿è¡Œæ•°æ®éªŒè¯å™¨è¿›è¡Œå…¨æ–¹ä½æ£€æŸ¥
2. æŸ¥çœ‹æ¸…æ´—æŠ¥å‘Šäº†è§£å„æ­¥éª¤å¤„ç†æƒ…å†µ
3. æŠ½æ ·æ£€æŸ¥æ•°æ®çš„ä¸šåŠ¡åˆç†æ€§
4. å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æŒ‡æ ‡

---

**æœ¬èŠ‚å»ºç«‹äº†å¯é çš„æ•°æ®è´¨é‡ç®¡æ§ä½“ç³»ï¼Œä¸ºåç»­çš„æ•°æ®å­˜å‚¨å’Œé‡åŒ–åˆ†ææä¾›äº†é«˜è´¨é‡çš„æ•°æ®åŸºç¡€ï¼** ğŸ”§âœ¨
