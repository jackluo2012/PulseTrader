# 2.5 è‡ªåŠ¨åŒ–æ•°æ®ç®¡é“

## ğŸ“– æœ¬èŠ‚æ¦‚è¿°

æœ¬èŠ‚å°†æ„å»ºå®Œæ•´çš„è‡ªåŠ¨åŒ–æ•°æ®ç®¡é“ç³»ç»Ÿï¼ŒåŒ…æ‹¬å®šæ—¶ä»»åŠ¡è°ƒåº¦ã€å¢é‡æ•°æ®æ›´æ–°ã€ç›‘æ§å‘Šè­¦ã€å®¹é”™é‡è¯•ç­‰åŠŸèƒ½ã€‚è‡ªåŠ¨åŒ–çš„æ•°æ®ç®¡é“æ˜¯é‡åŒ–äº¤æ˜“ç³»ç»Ÿç¨³å®šè¿è¡Œçš„é‡è¦ä¿éšœã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- âœ… è®¾è®¡å¯æ‰©å±•çš„æ•°æ®ç®¡é“æ¶æ„
- âœ… å®ç°å®šæ—¶ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ
- âœ… å»ºç«‹å¢é‡æ•°æ®æ›´æ–°æœºåˆ¶
- âœ… é…ç½®ç›‘æ§å‘Šè­¦ç³»ç»Ÿ
- âœ… å®ç°å®¹é”™å’Œæ•…éšœæ¢å¤

## â±ï¸ é¢„è®¡æ—¶é—´ï¼š45-55åˆ†é’Ÿ

---

## ğŸ—ï¸ æ•°æ®ç®¡é“æ¶æ„è®¾è®¡

### 1. åˆ›å»ºç®¡é“åŸºç¡€æ¡†æ¶

åˆ›å»º `src/data/pipelines/base_pipeline.py`ï¼š

```python
"""
æ•°æ®ç®¡é“åŸºç±»
æä¾›ç®¡é“æ‰§è¡Œçš„åŸºç¡€æ¡†æ¶å’Œé€šç”¨åŠŸèƒ½
"""

import logging
import time
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from enum import Enum
import traceback
import json
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

class PipelineStatus(Enum):
    """ç®¡é“çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RETRYING = "retrying"

@dataclass
class PipelineResult:
    """ç®¡é“æ‰§è¡Œç»“æœ"""
    pipeline_id: str
    status: PipelineStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    records_processed: int = 0
    records_failed: int = 0
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

        if self.end_time and self.start_time:
            self.duration = (self.end_time - self.start_time).total_seconds()

@dataclass
class PipelineConfig:
    """ç®¡é“é…ç½®"""
    name: str
    enabled: bool = True
    max_retries: int = 3
    retry_delay: float = 60.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    timeout: Optional[float] = None  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    priority: int = 0  # ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    dependencies: List[str] = None  # ä¾èµ–çš„ç®¡é“
    schedule: Optional[str] = None  # è°ƒåº¦è¡¨è¾¾å¼ï¼ˆcronæ ¼å¼ï¼‰
    notification_on_failure: bool = True
    max_execution_time: Optional[float] = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []

class BasePipeline(ABC):
    """æ•°æ®ç®¡é“åŸºç±»"""

    def __init__(self, config: PipelineConfig):
        """
        åˆå§‹åŒ–ç®¡é“

        Args:
            config: ç®¡é“é…ç½®
        """
        self.config = config
        self.status = PipelineStatus.PENDING
        self.result: Optional[PipelineResult] = None
        self.retry_count = 0
        self._callbacks: Dict[str, List[Callable]] = {
            'on_start': [],
            'on_success': [],
            'on_failure': [],
            'on_retry': []
        }

    @property
    def pipeline_id(self) -> str:
        """è·å–ç®¡é“ID"""
        return f"{self.config.name}_{int(time.time())}"

    def add_callback(self, event: str, callback: Callable):
        """
        æ·»åŠ å›è°ƒå‡½æ•°

        Args:
            event: äº‹ä»¶ç±»å‹
            callback: å›è°ƒå‡½æ•°
        """
        if event in self._callbacks:
            self._callbacks[event].append(callback)

    def _trigger_callbacks(self, event: str, result: PipelineResult = None):
        """è§¦å‘å›è°ƒå‡½æ•°"""
        for callback in self._callbacks.get(event, []):
            try:
                callback(self, result)
            except Exception as e:
                logger.error(f"å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥ {event}: {e}")

    def _execute_with_timeout(self, func, *args, **kwargs):
        """
        å¸¦è¶…æ—¶çš„å‡½æ•°æ‰§è¡Œ

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: ä½ç½®å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        if self.config.timeout:
            import signal

            def timeout_handler(signum, frame):
                raise TimeoutError(f"Pipeline execution timeout after {self.config.timeout} seconds")

            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(int(self.config.timeout))

            try:
                result = func(*args, **kwargs)
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                return result
            except TimeoutError:
                signal.alarm(0)
                raise
        else:
            return func(*args, **kwargs)

    def execute(self) -> PipelineResult:
        """
        æ‰§è¡Œç®¡é“

        Returns:
            æ‰§è¡Œç»“æœ
        """
        if not self.config.enabled:
            logger.info(f"ç®¡é“ {self.config.name} å·²ç¦ç”¨ï¼Œè·³è¿‡æ‰§è¡Œ")
            return PipelineResult(
                pipeline_id=self.pipeline_id,
                status=PipelineStatus.CANCELLED,
                start_time=datetime.now()
            )

        # æ£€æŸ¥ä¾èµ–
        if not self._check_dependencies():
            logger.warning(f"ç®¡é“ {self.config.name} ä¾èµ–æœªæ»¡è¶³ï¼Œè·³è¿‡æ‰§è¡Œ")
            return PipelineResult(
                pipeline_id=self.pipeline_id,
                status=PipelineStatus.CANCELLED,
                start_time=datetime.now(),
                error_message="Dependencies not satisfied"
            )

        # å¼€å§‹æ‰§è¡Œ
        self.status = PipelineStatus.RUNNING
        start_time = datetime.now()
        result = PipelineResult(
            pipeline_id=self.pipeline_id,
            status=PipelineStatus.RUNNING,
            start_time=start_time
        )

        try:
            # è§¦å‘å¼€å§‹å›è°ƒ
            self._trigger_callbacks('on_start', result)

            # æ‰§è¡Œå…·ä½“é€»è¾‘
            self._execute_with_timeout(self._run_pipeline)

            # æ‰§è¡ŒæˆåŠŸ
            end_time = datetime.now()
            result.end_time = end_time
            result.status = PipelineStatus.SUCCESS
            self.status = PipelineStatus.SUCCESS
            self.result = result

            # è§¦å‘æˆåŠŸå›è°ƒ
            self._trigger_callbacks('on_success', result)

            logger.info(f"ç®¡é“ {self.config.name} æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {result.duration:.2f}s")

        except Exception as e:
            # æ‰§è¡Œå¤±è´¥
            end_time = datetime.now()
            result.end_time = end_time
            result.status = PipelineStatus.FAILED
            result.error_message = str(e)
            self.status = PipelineStatus.FAILED
            self.result = result

            logger.error(f"ç®¡é“ {self.config.name} æ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

            # è§¦å‘å¤±è´¥å›è°ƒ
            self._trigger_callbacks('on_failure', result)

            # é‡è¯•é€»è¾‘
            if self.retry_count < self.config.max_retries:
                return self._retry()

        return result

    def _retry(self) -> PipelineResult:
        """é‡è¯•æ‰§è¡Œ"""
        self.retry_count += 1
        self.status = PipelineStatus.RETRYING

        logger.info(f"ç®¡é“ {self.config.name} ç¬¬ {self.retry_count} æ¬¡é‡è¯•ï¼Œ"
                   f"å»¶è¿Ÿ {self.config.retry_delay}s")

        # è§¦å‘é‡è¯•å›è°ƒ
        retry_result = PipelineResult(
            pipeline_id=self.pipeline_id,
            status=PipelineStatus.RETRYING,
            start_time=datetime.now()
        )
        self._trigger_callbacks('on_retry', retry_result)

        # ç­‰å¾…é‡è¯•å»¶è¿Ÿ
        time.sleep(self.config.retry_delay)

        # é‡æ–°æ‰§è¡Œ
        return self.execute()

    def _check_dependencies(self) -> bool:
        """
        æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³

        Returns:
            ä¾èµ–æ˜¯å¦æ»¡è¶³
        """
        # ç®€å•å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ£€æŸ¥ä¾èµ–ç®¡é“çš„æ‰§è¡ŒçŠ¶æ€
        return True

    @abstractmethod
    def _run_pipeline(self):
        """å…·ä½“ç®¡é“æ‰§è¡Œé€»è¾‘ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    def get_status(self) -> Dict[str, Any]:
        """
        è·å–ç®¡é“çŠ¶æ€

        Returns:
            çŠ¶æ€ä¿¡æ¯
        """
        return {
            'name': self.config.name,
            'status': self.status.value,
            'retry_count': self.retry_count,
            'enabled': self.config.enabled,
            'last_result': asdict(self.result) if self.result else None
        }


def test_base_pipeline():
    """æµ‹è¯•åŸºç¡€ç®¡é“"""
    from src.data.pipelines.config import PipelineConfig

    class TestPipeline(BasePipeline):
        def _run_pipeline(self):
            logger.info("æ‰§è¡Œæµ‹è¯•ç®¡é“é€»è¾‘...")
            time.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            logger.info("æµ‹è¯•ç®¡é“æ‰§è¡Œå®Œæˆ")

    # åˆ›å»ºé…ç½®
    config = PipelineConfig(
        name="test_pipeline",
        max_retries=2,
        retry_delay=1.0
    )

    # åˆ›å»ºç®¡é“
    pipeline = TestPipeline(config)

    # æ·»åŠ å›è°ƒ
    def on_start(pipe, result):
        print(f"ç®¡é“å¼€å§‹: {result.pipeline_id}")

    def on_success(pipe, result):
        print(f"ç®¡é“æˆåŠŸ: {result.pipeline_id}, è€—æ—¶: {result.duration:.2f}s")

    def on_failure(pipe, result):
        print(f"ç®¡é“å¤±è´¥: {result.pipeline_id}, é”™è¯¯: {result.error_message}")

    pipeline.add_callback('on_start', on_start)
    pipeline.add_callback('on_success', on_success)
    pipeline.add_callback('on_failure', on_failure)

    # æ‰§è¡Œç®¡é“
    result = pipeline.execute()
    print(f"æœ€ç»ˆçŠ¶æ€: {result.status}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_base_pipeline()
```

### 2. åˆ›å»ºè°ƒåº¦å™¨

åˆ›å»º `src/data/pipelines/scheduler.py`ï¼š

```python
"""
æ•°æ®ç®¡é“è°ƒåº¦å™¨
è´Ÿè´£ä»»åŠ¡è°ƒåº¦ã€ä¼˜å…ˆçº§ç®¡ç†å’Œå¹¶å‘æ§åˆ¶
"""

import time
import threading
import heapq
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable
from concurrent.futures import ThreadPoolExecutor, Future
from queue import PriorityQueue, Empty
import logging
import schedule
import json
import os

from .base_pipeline import BasePipeline, PipelineStatus, PipelineResult
from .config import PipelineConfig

logger = logging.getLogger(__name__)

class TaskPriorityQueue:
    """åŸºäºä¼˜å…ˆçº§çš„ä»»åŠ¡é˜Ÿåˆ—"""

    def __init__(self):
        self._queue = PriorityQueue()
        self._counter = 0  # ç”¨äºè§£å†³ä¼˜å…ˆçº§ç›¸åŒæ—¶çš„æ’åº

    def put(self, pipeline: BasePipeline, priority: int = 0):
        """
        æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—

        Args:
            pipeline: ç®¡é“å®ä¾‹
            priority: ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        """
        # ä½¿ç”¨è´Ÿæ•°ä½¿å¤§æ•°å­—å¯¹åº”é«˜ä¼˜å…ˆçº§
        heapq.heappush(self._queue, (-priority, self._counter, pipeline))
        self._counter += 1

    def get(self, timeout: Optional[float] = None) -> Optional[BasePipeline]:
        """
        ä»é˜Ÿåˆ—è·å–ä»»åŠ¡

        Args:
            timeout: è¶…æ—¶æ—¶é—´

        Returns:
            ç®¡é“å®ä¾‹æˆ–None
        """
        try:
            if timeout:
                _, _, pipeline = self._queue.get(timeout=timeout)
            else:
                _, _, pipeline = self._queue.get_nowait()
            return pipeline
        except Empty:
            return None

    def empty(self) -> bool:
        """æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º"""
        return self._queue.empty()

    def size(self) -> int:
        """è·å–é˜Ÿåˆ—å¤§å°"""
        return self._queue.qsize()


class PipelineScheduler:
    """ç®¡é“è°ƒåº¦å™¨"""

    def __init__(self, max_workers: int = 4, max_concurrent: int = 2):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨

        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ‰§è¡Œæ•°
        """
        self.max_workers = max_workers
        self.max_concurrent = max_concurrent
        self.task_queue = TaskPriorityQueue()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.running_pipelines: Dict[str, Future] = {}
        self.completed_pipelines: Dict[str, PipelineResult] = {}
        self.scheduled_pipelines: Dict[str, BasePipeline] = {}
        self.is_running = False
        self.scheduler_thread: Optional[threading.Thread] = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_executed': 0,
            'total_failed': 0,
            'total_success': 0,
            'total_retries': 0,
            'start_time': None,
            'last_execution': None
        }

        # é…ç½®æŒä¹…åŒ–
        self.config_file = "config/pipelines/scheduled_pipelines.json"

    def register_pipeline(self, pipeline: BasePipeline):
        """
        æ³¨å†Œç®¡é“

        Args:
            pipeline: ç®¡é“å®ä¾‹
        """
        self.scheduled_pipelines[pipeline.config.name] = pipeline
        logger.info(f"æ³¨å†Œç®¡é“: {pipeline.config.name}")

    def schedule_pipeline(self, pipeline_name: str, priority: int = 0):
        """
        è°ƒåº¦ç®¡é“æ‰§è¡Œ

        Args:
            pipeline_name: ç®¡é“åç§°
            priority: ä¼˜å…ˆçº§
        """
        if pipeline_name not in self.scheduled_pipelines:
            logger.error(f"ç®¡é“ä¸å­˜åœ¨: {pipeline_name}")
            return

        pipeline = self.scheduled_pipelines[pipeline_name]

        if pipeline_name in self.running_pipelines:
            logger.warning(f"ç®¡é“æ­£åœ¨è¿è¡Œ: {pipeline_name}")
            return

        self.task_queue.put(pipeline, priority)
        logger.info(f"ç®¡é“å·²åŠ å…¥é˜Ÿåˆ—: {pipeline_name}, ä¼˜å…ˆçº§: {priority}")

    def schedule_all(self):
        """è°ƒåº¦æ‰€æœ‰å¯ç”¨çš„ç®¡é“"""
        for name, pipeline in self.scheduled_pipelines.items():
            if pipeline.config.enabled:
                self.schedule_pipeline(name, pipeline.config.priority)

    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹å¾ªç¯"""
        logger.info("è°ƒåº¦å™¨å¼€å§‹è¿è¡Œ...")

        self.stats['start_time'] = datetime.now()

        while self.is_running:
            try:
                # æ£€æŸ¥å¹¶å‘æ‰§è¡Œæ•°
                if len(self.running_pipelines) >= self.max_concurrent:
                    time.sleep(1)
                    continue

                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                pipeline = self.task_queue.get(timeout=1.0)

                if pipeline:
                    self._execute_pipeline(pipeline)

            except Exception as e:
                logger.error(f"è°ƒåº¦å™¨å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
                time.sleep(5)

    def _execute_pipeline(self, pipeline: BasePipeline):
        """
        æ‰§è¡Œç®¡é“

        Args:
            pipeline: ç®¡é“å®ä¾‹
        """
        def pipeline_wrapper():
            try:
                # æ‰§è¡Œç®¡é“
                result = pipeline.execute()

                # è®°å½•æ‰§è¡Œç»“æœ
                self.completed_pipelines[pipeline.pipeline_id] = result
                self.stats['total_executed'] += 1

                # æ›´æ–°ç»Ÿè®¡
                if result.status == PipelineStatus.SUCCESS:
                    self.stats['total_success'] += 1
                elif result.status == PipelineStatus.FAILED:
                    self.stats['total_failed'] += 1
                    self.stats['total_retries'] += pipeline.retry_count

                self.stats['last_execution'] = datetime.now()

                logger.info(f"ç®¡é“æ‰§è¡Œå®Œæˆ: {pipeline.config.name}, çŠ¶æ€: {result.status.value}")

            except Exception as e:
                logger.error(f"ç®¡é“æ‰§è¡Œå¼‚å¸¸: {pipeline.config.name}, é”™è¯¯: {e}")

                # åˆ›å»ºå¤±è´¥ç»“æœ
                result = PipelineResult(
                    pipeline_id=pipeline.pipeline_id,
                    status=PipelineStatus.FAILED,
                    start_time=datetime.now(),
                    end_time=datetime.now(),
                    error_message=str(e)
                )

                self.completed_pipelines[pipeline.pipeline_id] = result
                self.stats['total_failed'] += 1

            finally:
                # ä»è¿è¡Œåˆ—è¡¨ä¸­ç§»é™¤
                if pipeline.pipeline_id in self.running_pipelines:
                    del self.running_pipelines[pipeline.pipeline_id]

        # æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        future = self.executor.submit(pipeline_wrapper)
        self.running_pipelines[pipeline.pipeline_id] = future

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.is_running:
            logger.warning("è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ")
            return

        self.is_running = True
        self.scheduler_thread = threading.Thread(target=self._worker_loop, daemon=True)
        self.scheduler_thread.start()

        logger.info(f"è°ƒåº¦å™¨å·²å¯åŠ¨ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}, æœ€å¤§å¹¶å‘: {self.max_concurrent}")

    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        if not self.is_running:
            logger.warning("è°ƒåº¦å™¨æœªè¿è¡Œ")
            return

        self.is_running = False

        # ç­‰å¾…å½“å‰ä»»åŠ¡å®Œæˆ
        for future in self.running_pipelines.values():
            try:
                future.result(timeout=30)  # ç­‰å¾…æœ€å¤š30ç§’
            except Exception:
                future.cancel()

        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)

        # ç­‰å¾…è°ƒåº¦å™¨çº¿ç¨‹ç»“æŸ
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=10)

        logger.info("è°ƒåº¦å™¨å·²åœæ­¢")

    def get_status(self) -> Dict[str, Any]:
        """
        è·å–è°ƒåº¦å™¨çŠ¶æ€

        Returns:
            çŠ¶æ€ä¿¡æ¯
        """
        return {
            'is_running': self.is_running,
            'queue_size': self.task_queue.size(),
            'running_pipelines': len(self.running_pipelines),
            'completed_pipelines': len(self.completed_pipelines),
            'scheduled_pipelines': len(self.scheduled_pipelines),
            'stats': self.stats.copy(),
            'uptime': (datetime.now() - self.stats['start_time']).total_seconds() if self.stats['start_time'] else 0
        }

    def get_pipeline_status(self, pipeline_name: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šç®¡é“çŠ¶æ€

        Args:
            pipeline_name: ç®¡é“åç§°

        Returns:
            ç®¡é“çŠ¶æ€ä¿¡æ¯
        """
        if pipeline_name not in self.scheduled_pipelines:
            return {'error': f'Pipeline not found: {pipeline_name}'}

        pipeline = self.scheduled_pipelines[pipeline_name]
        base_status = pipeline.get_status()

        # æ·»åŠ è°ƒåº¦å™¨ç‰¹æœ‰ä¿¡æ¯
        is_running = pipeline.pipeline_id in self.running_pipelines
        latest_result = None

        # æŸ¥æ‰¾æœ€æ–°çš„æ‰§è¡Œç»“æœ
        for result_id, result in self.completed_pipelines.items():
            if result_id.startswith(pipeline_name):
                if not latest_result or result.end_time > latest_result.end_time:
                    latest_result = result

        base_status.update({
            'is_currently_running': is_running,
            'queue_position': None,  # å¯ä»¥æ·»åŠ é˜Ÿåˆ—ä½ç½®ä¿¡æ¯
            'latest_result': {
                'status': latest_result.status.value,
                'start_time': latest_result.start_time.isoformat(),
                'end_time': latest_result.end_time.isoformat() if latest_result.end_time else None,
                'duration': latest_result.duration,
                'records_processed': latest_result.records_processed,
                'error_message': latest_result.error_message
            } if latest_result else None
        })

        return base_status

    def save_config(self):
        """ä¿å­˜è°ƒåº¦é…ç½®"""
        os.makedirs(os.path.dirname(self.config_file), exist_ok=True)

        config_data = {}
        for name, pipeline in self.scheduled_pipelines.items():
            config_data[name] = {
                'name': pipeline.config.name,
                'enabled': pipeline.config.enabled,
                'max_retries': pipeline.config.max_retries,
                'retry_delay': pipeline.config.retry_delay,
                'priority': pipeline.config.priority,
                'schedule': pipeline.config.schedule,
                'dependencies': pipeline.config.dependencies
            }

        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            logger.info("è°ƒåº¦é…ç½®å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def load_config(self):
        """åŠ è½½è°ƒåº¦é…ç½®"""
        if not os.path.exists(self.config_file):
            logger.info("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")
            return

        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config_data = json.load(f)

            for name, config in config_data.items():
                if name in self.scheduled_pipelines:
                    pipeline = self.scheduled_pipelines[name]
                    pipeline.config.enabled = config.get('enabled', True)
                    pipeline.config.max_retries = config.get('max_retries', 3)
                    pipeline.config.retry_delay = config.get('retry_delay', 60.0)
                    pipeline.config.priority = config.get('priority', 0)
                    pipeline.config.schedule = config.get('schedule')
                    pipeline.config.dependencies = config.get('dependencies', [])

            logger.info("è°ƒåº¦é…ç½®å·²åŠ è½½")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")


def test_scheduler():
    """æµ‹è¯•è°ƒåº¦å™¨"""
    from src.data.pipelines.config import PipelineConfig

    class TestPipeline(BasePipeline):
        def __init__(self, config: PipelineConfig, name_suffix: str = ""):
            super().__init__(config)
            self.name_suffix = name_suffix

        def _run_pipeline(self):
            logger.info(f"æ‰§è¡Œæµ‹è¯•ç®¡é“: {self.config.name}{self.name_suffix}")
            time.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            logger.info(f"æµ‹è¯•ç®¡é“å®Œæˆ: {self.config.name}{self.name_suffix}")

    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = PipelineScheduler(max_workers=2, max_concurrent=2)

    # åˆ›å»ºæµ‹è¯•ç®¡é“
    pipelines = []
    for i in range(5):
        config = PipelineConfig(
            name=f"test_pipeline_{i}",
            max_retries=1,
            priority=i
        )
        pipeline = TestPipeline(config)
        pipelines.append(pipeline)
        scheduler.register_pipeline(pipeline)

    # å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()

    # è°ƒåº¦æ‰€æœ‰ç®¡é“
    scheduler.schedule_all()

    # ç›‘æ§çŠ¶æ€
    for _ in range(10):
        status = scheduler.get_status()
        print(f"é˜Ÿåˆ—å¤§å°: {status['queue_size']}, è¿è¡Œä¸­: {status['running_pipelines']}, å®Œæˆ: {status['completed_pipelines']}")
        time.sleep(1)

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    time.sleep(10)

    # åœæ­¢è°ƒåº¦å™¨
    scheduler.stop()

    print("æœ€ç»ˆç»Ÿè®¡:", scheduler.get_status()['stats'])


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_scheduler()
```

---

## ğŸ“ˆ å…·ä½“ç®¡é“å®ç°

### 1. æ—¥çº¿æ•°æ®æ›´æ–°ç®¡é“

åˆ›å»º `src/data/pipelines/daily_update.py`ï¼š

```python
"""
æ—¥çº¿æ•°æ®æ›´æ–°ç®¡é“
è´Ÿè´£æ¯æ—¥è‚¡ç¥¨è¡Œæƒ…æ•°æ®çš„è‡ªåŠ¨æ›´æ–°
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
import pandas as pd

from ..collectors.historical import HistoricalCollector
from ..processors.cleaner import DataCleaner
from ..storage.batch_insert import BatchInserter
from .base_pipeline import BasePipeline, PipelineResult
from .config import PipelineConfig

logger = logging.getLogger(__name__)

class DailyDataUpdatePipeline(BasePipeline):
    """æ—¥çº¿æ•°æ®æ›´æ–°ç®¡é“"""

    def __init__(self, config: PipelineConfig = None):
        """
        åˆå§‹åŒ–æ—¥çº¿æ•°æ®æ›´æ–°ç®¡é“

        Args:
            config: ç®¡é“é…ç½®
        """
        if config is None:
            config = PipelineConfig(
                name="daily_data_update",
                max_retries=3,
                retry_delay=300.0,  # 5åˆ†é’Ÿé‡è¯•
                priority=10,
                schedule="0 18 * * 1-5",  # å·¥ä½œæ—¥18:00æ‰§è¡Œ
                max_execution_time=3600  # æœ€å¤§æ‰§è¡Œæ—¶é—´1å°æ—¶
            )

        super().__init__(config)

        # åˆå§‹åŒ–ç»„ä»¶
        self.collector = HistoricalCollector()
        self.cleaner = DataCleaner()
        self.inserter = None  # å»¶è¿Ÿåˆå§‹åŒ–

    def _run_pipeline(self):
        """æ‰§è¡Œæ—¥çº¿æ•°æ®æ›´æ–°é€»è¾‘"""
        logger.info("å¼€å§‹æ‰§è¡Œæ—¥çº¿æ•°æ®æ›´æ–°ç®¡é“")

        # åˆå§‹åŒ–æ’å…¥å™¨
        from ..database import db_manager
        self.inserter = BatchInserter(db_manager, batch_size=5000)

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_symbols = self._get_stock_symbols()
        if not stock_symbols:
            raise Exception("æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")

        logger.info(f"å¾…æ›´æ–°è‚¡ç¥¨æ•°é‡: {len(stock_symbols)}")

        # è·å–äº¤æ˜“æ—¥å†
        trading_dates = self._get_recent_trading_days()
        if not trading_dates:
            raise Exception("æœªè·å–åˆ°äº¤æ˜“æ—¥å†")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        latest_trading_date = trading_dates[-1]
        if self._is_data_updated(latest_trading_date):
            logger.info(f"æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°: {latest_trading_date}")
            return

        # è·å–éœ€è¦æ›´æ–°çš„æ—¥æœŸèŒƒå›´
        update_start_date = self._get_update_start_date(latest_trading_date)

        logger.info(f"æ›´æ–°æ•°æ®èŒƒå›´: {update_start_date} ~ {latest_trading_date}")

        # æ‰¹é‡ä¸‹è½½å’Œæ›´æ–°æ•°æ®
        total_processed = 0
        total_failed = 0

        for i in range(0, len(stock_symbols), self.config.batch_size or 50):
            batch_symbols = stock_symbols[i:i + (self.config.batch_size or 50)]

            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//len(batch_symbols) + 1}: {len(batch_symbols)} åªè‚¡ç¥¨")

            try:
                batch_result = self._process_stock_batch(
                    batch_symbols,
                    update_start_date,
                    latest_trading_date
                )

                total_processed += batch_result['processed']
                total_failed += batch_result['failed']

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i + len(batch_symbols) < len(stock_symbols):
                    import time
                    time.sleep(1.0)

            except Exception as e:
                logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                total_failed += len(batch_symbols)

        # æ›´æ–°ç®¡é“å…ƒæ•°æ®
        if self.result:
            self.result.records_processed = total_processed
            self.result.records_failed = total_failed

        logger.info(f"æ—¥çº¿æ•°æ®æ›´æ–°å®Œæˆï¼Œå¤„ç†: {total_processed}, å¤±è´¥: {total_failed}")

    def _get_stock_symbols(self) -> List[str]:
        """
        è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        try:
            # ä»æ•°æ®åº“è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨
            sql = """
            SELECT DISTINCT symbol
            FROM pulse_trader.stock_info
            WHERE is_active = 1
            ORDER BY symbol
            """

            result = self.inserter.db_client.execute_query(sql)
            if result:
                return [row[0] for row in result]

        except Exception as e:
            logger.warning(f"ä»æ•°æ®åº“è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç¡¬ç¼–ç çš„ä¸»è¦è‚¡ç¥¨ä»£ç 
        return [
            '000001', '000002', '000858', '002415',  # å¹³å®‰é“¶è¡Œã€ä¸‡ç§‘Aã€äº”ç²®æ¶²ã€æµ·åº·å¨è§†
            '600000', '600036', '600519', '600887',  # æµ¦å‘é“¶è¡Œã€æ‹›å•†é“¶è¡Œã€è´µå·èŒ…å°ã€ä¼Šåˆ©è‚¡ä»½
            '300015', '300059', '300750', '300760'   # çˆ±å°”çœ¼ç§‘ã€ä¸œæ–¹è´¢å¯Œã€å®å¾·æ—¶ä»£ã€è¿ˆç‘åŒ»ç–—
        ]

    def _get_recent_trading_days(self, days: int = 30) -> List[str]:
        """
        è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥

        Args:
            days: è·å–å¤©æ•°

        Returns:
            äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        """
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days * 2)  # å¤šå–ä¸€äº›å¤©ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„äº¤æ˜“æ—¥

            # æŸ¥è¯¢äº¤æ˜“æ—¥å†
            sql = """
            SELECT date
            FROM pulse_trader.trading_calendar
            WHERE date BETWEEN %s AND %s
              AND is_trading_day = 1
            ORDER BY date
            """

            result = self.inserter.db_client.execute_query(sql, (start_date.date(), end_date.date()))
            if result:
                trading_days = [row[0].strftime('%Y%m%d') for row in result]
                return trading_days[-days:]  # è¿”å›æœ€è¿‘çš„äº¤æ˜“æ—¥

        except Exception as e:
            logger.warning(f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šç”Ÿæˆæ—¥æœŸåˆ—è¡¨ï¼ˆæ’é™¤å‘¨æœ«ï¼‰
        trading_days = []
        current_date = datetime.now() - timedelta(days=days)

        for _ in range(days):
            if current_date.weekday() < 5:  # å·¥ä½œæ—¥
                trading_days.append(current_date.strftime('%Y%m%d'))
            current_date += timedelta(days=1)

        return trading_days

    def _is_data_updated(self, trading_date: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šäº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦å·²æ›´æ–°

        Args:
            trading_date: äº¤æ˜“æ—¥ï¼ˆYYYYMMDDæ ¼å¼ï¼‰

        Returns:
            æ•°æ®æ˜¯å¦å·²æ›´æ–°
        """
        try:
            date_obj = datetime.strptime(trading_date, '%Y%m%d').date()

            sql = """
            SELECT count(*)
            FROM pulse_trader.daily_quotes
            WHERE date = %s
            """

            result = self.inserter.db_client.execute_query(sql, (date_obj,))
            if result and result[0][0] > 1000:  # å‡è®¾æœ‰è¶…è¿‡1000åªè‚¡ç¥¨çš„æ•°æ®
                return True

        except Exception as e:
            logger.warning(f"æ£€æŸ¥æ•°æ®æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")

        return False

    def _get_update_start_date(self, latest_trading_date: str) -> str:
        """
        è·å–æ›´æ–°å¼€å§‹æ—¥æœŸ

        Args:
            latest_trading_date: æœ€æ–°äº¤æ˜“æ—¥

        Returns:
            å¼€å§‹æ—¥æœŸ
        """
        try:
            # æŸ¥è¯¢æ•°æ®åº“ä¸­æœ€æ–°çš„æ•°æ®æ—¥æœŸ
            sql = """
            SELECT max(date)
            FROM pulse_trader.daily_quotes
            """

            result = self.inserter.db_client.execute_query(sql)
            if result and result[0][0]:
                latest_db_date = result[0][0]
                # ä»æ•°æ®åº“æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹æ›´æ–°
                start_date = latest_db_date + timedelta(days=1)
                return start_date.strftime('%Y%m%d')

        except Exception as e:
            logger.warning(f"æŸ¥è¯¢æ•°æ®åº“æœ€æ–°æ—¥æœŸå¤±è´¥: {e}")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»7å¤©å‰å¼€å§‹
        start_date = datetime.now() - timedelta(days=7)
        return start_date.strftime('%Y%m%d')

    def _process_stock_batch(self, symbols: List[str], start_date: str, end_date: str) -> Dict[str, int]:
        """
        å¤„ç†è‚¡ç¥¨æ‰¹æ¬¡æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        processed = 0
        failed = 0

        for symbol in symbols:
            try:
                # ä¸‹è½½æ•°æ®
                historical_data = self.collector.collect_daily_kline(
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date
                )

                if historical_data is not None and not historical_data.empty:
                    # æ•°æ®æ¸…æ´—
                    cleaned_data, _ = self.cleaner.clean_data(historical_data, 'price_data')

                    if not cleaned_data.empty:
                        # æ·»åŠ å¸‚åœºä¿¡æ¯
                        cleaned_data['market'] = 'SH' if symbol.startswith('6') else 'SZ'

                        # æ’å…¥æ•°æ®åº“
                        insert_result = self.inserter.insert_daily_quotes(cleaned_data)

                        if insert_result.get('success'):
                            processed += 1
                        else:
                            failed += 1
                            logger.warning(f"æ’å…¥æ•°æ®å¤±è´¥: {symbol}")
                    else:
                        failed += 1
                        logger.warning(f"æ•°æ®æ¸…æ´—åä¸ºç©º: {symbol}")
                else:
                    failed += 1
                    logger.warning(f"ä¸‹è½½æ•°æ®ä¸ºç©º: {symbol}")

            except Exception as e:
                failed += 1
                logger.error(f"å¤„ç†è‚¡ç¥¨æ•°æ®å¤±è´¥ {symbol}: {e}")

        return {
            'processed': processed,
            'failed': failed,
            'total': len(symbols)
        }


def test_daily_update_pipeline():
    """æµ‹è¯•æ—¥çº¿æ•°æ®æ›´æ–°ç®¡é“"""
    config = PipelineConfig(
        name="test_daily_update",
        max_retries=1,
        batch_size=3
    )

    pipeline = DailyDataUpdatePipeline(config)
    result = pipeline.execute()

    print(f"ç®¡é“æ‰§è¡ŒçŠ¶æ€: {result.status}")
    print(f"å¤„ç†è®°å½•æ•°: {result.records_processed}")
    print(f"å¤±è´¥è®°å½•æ•°: {result.records_failed}")
    if result.error_message:
        print(f"é”™è¯¯ä¿¡æ¯: {result.error_message}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_daily_update_pipeline()
```

### 2. å®æ—¶æ•°æ®åŒæ­¥ç®¡é“

åˆ›å»º `src/data/pipelines/realtime_sync.py`ï¼š

```python
"""
å®æ—¶æ•°æ®åŒæ­¥ç®¡é“
è´Ÿè´£å®æ—¶è¡Œæƒ…æ•°æ®çš„åŒæ­¥å’Œæ›´æ–°
"""

import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Set
import threading
from queue import Queue, Empty

from ..collectors.realtime import RealtimeCollector
from ..storage.batch_insert import BatchInserter
from .base_pipeline import BasePipeline, PipelineResult, PipelineStatus
from .config import PipelineConfig

logger = logging.getLogger(__name__)

class RealtimeSyncPipeline(BasePipeline):
    """å®æ—¶æ•°æ®åŒæ­¥ç®¡é“"""

    def __init__(self, config: PipelineConfig = None):
        """
        åˆå§‹åŒ–å®æ—¶æ•°æ®åŒæ­¥ç®¡é“

        Args:
            config: ç®¡é“é…ç½®
        """
        if config is None:
            config = PipelineConfig(
                name="realtime_sync",
                max_retries=3,
                retry_delay=60.0,
                priority=5,
                max_execution_time=None  # å®æ—¶ç®¡é“æŒç»­è¿è¡Œ
            )

        super().__init__(config)

        # åˆå§‹åŒ–ç»„ä»¶
        self.collector = RealtimeCollector()
        self.inserter = None  # å»¶è¿Ÿåˆå§‹åŒ–

        # å®æ—¶åŒæ­¥æ§åˆ¶
        self.is_syncing = False
        self.sync_thread: Optional[threading.Thread] = None
        self.data_queue = Queue(maxsize=1000)
        self.monitored_symbols: Set[str] = set()

        # åŒæ­¥ç»Ÿè®¡
        self.sync_stats = {
            'total_updates': 0,
            'successful_updates': 0,
            'failed_updates': 0,
            'last_update_time': None,
            'sync_duration': 0.0
        }

    def _run_pipeline(self):
        """æ‰§è¡Œå®æ—¶æ•°æ®åŒæ­¥é€»è¾‘"""
        logger.info("å¼€å§‹å®æ—¶æ•°æ®åŒæ­¥ç®¡é“")

        # åˆå§‹åŒ–æ’å…¥å™¨
        from ..database import db_manager
        self.inserter = BatchInserter(db_manager, batch_size=100)

        # è·å–ç›‘æ§è‚¡ç¥¨åˆ—è¡¨
        self.monitored_symbols = self._get_monitored_symbols()

        if not self.monitored_symbols:
            logger.warning("æœªæ‰¾åˆ°ç›‘æ§è‚¡ç¥¨ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨")
            self.monitored_symbols = {
                '000001', '000002', '000858', '002415',  # ä¸»è¦Aè‚¡
                '600000', '600036', '600519', '600887'
            }

        logger.info(f"ç›‘æ§è‚¡ç¥¨æ•°é‡: {len(self.monitored_symbols)}")

        # å¯åŠ¨åŒæ­¥çº¿ç¨‹
        self.is_syncing = True
        self.sync_thread = threading.Thread(target=self._sync_loop, daemon=True)
        self.sync_thread.start()

        # ä¸»çº¿ç¨‹ç­‰å¾…åŒæ­¥å®Œæˆæˆ–è¢«åœæ­¢
        try:
            while self.is_syncing and self.status == PipelineStatus.RUNNING:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢å®æ—¶åŒæ­¥...")
            self.stop_sync()

        # ç­‰å¾…åŒæ­¥çº¿ç¨‹ç»“æŸ
        if self.sync_thread:
            self.sync_thread.join(timeout=10)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if self.result:
            self.result.records_processed = self.sync_stats['successful_updates']
            self.result.records_failed = self.sync_stats['failed_updates']
            self.result.metadata.update(self.sync_stats)

        logger.info(f"å®æ—¶æ•°æ®åŒæ­¥å®Œæˆï¼Œæ›´æ–°: {self.sync_stats['successful_updates']}, å¤±è´¥: {self.sync_stats['failed_updates']}")

    def _get_monitored_symbols(self) -> Set[str]:
        """
        è·å–éœ€è¦ç›‘æ§çš„è‚¡ç¥¨åˆ—è¡¨

        Returns:
            è‚¡ç¥¨ä»£ç é›†åˆ
        """
        try:
            # ä»æ•°æ®åº“è·å–çƒ­é—¨è‚¡ç¥¨
            sql = """
            SELECT DISTINCT symbol
            FROM pulse_trader.daily_quotes
            WHERE date >= today() - INTERVAL 7 DAY
              AND volume > 1000000
            ORDER BY volume DESC
            LIMIT 100
            """

            result = self.inserter.db_client.execute_query(sql)
            if result:
                return set(row[0] for row in result)

        except Exception as e:
            logger.warning(f"è·å–ç›‘æ§è‚¡ç¥¨å¤±è´¥: {e}")

        return set()

    def _sync_loop(self):
        """å®æ—¶åŒæ­¥å¾ªç¯"""
        logger.info("å®æ—¶åŒæ­¥å¾ªç¯å¼€å§‹")

        sync_start_time = time.time()

        while self.is_syncing:
            try:
                # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´
                if not self.collector.is_trading_time():
                    logger.debug("éäº¤æ˜“æ—¶é—´ï¼Œæš‚åœåŒæ­¥")
                    time.sleep(60)  # éäº¤æ˜“æ—¶é—´ä¼‘çœ 1åˆ†é’Ÿ
                    continue

                # æ‰¹é‡è·å–å®æ—¶è¡Œæƒ…
                symbols_list = list(self.monitored_symbols)
                batch_size = 50  # æ¯æ‰¹å¤„ç†50åªè‚¡ç¥¨

                for i in range(0, len(symbols_list), batch_size):
                    if not self.is_syncing:
                        break

                    batch_symbols = symbols_list[i:i + batch_size]
                    self._process_realtime_batch(batch_symbols)

                    # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                    time.sleep(2.0)

            except Exception as e:
                logger.error(f"å®æ—¶åŒæ­¥å¼‚å¸¸: {e}")
                time.sleep(10)  # å¼‚å¸¸æ—¶ç­‰å¾…10ç§’

        self.sync_stats['sync_duration'] = time.time() - sync_start_time
        logger.info(f"å®æ—¶åŒæ­¥å¾ªç¯ç»“æŸï¼Œè¿è¡Œæ—¶é•¿: {self.sync_stats['sync_duration']:.2f}ç§’")

    def _process_realtime_batch(self, symbols: List[str]):
        """
        å¤„ç†å®æ—¶æ•°æ®æ‰¹æ¬¡

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        try:
            # è·å–å®æ—¶è¡Œæƒ…
            realtime_data = self.collector.collect_batch_quotes(symbols)

            if realtime_data is not None and not realtime_data.empty:
                # æ ‡å‡†åŒ–åˆ—å
                column_mapping = {
                    'ä»£ç ': 'symbol',
                    'åç§°': 'stock_name',
                    'æœ€æ–°ä»·': 'current_price',
                    'æ¶¨è·Œå¹…': 'change_percent',
                    'æ¶¨è·Œé¢': 'change_amount',
                    'æˆäº¤é‡': 'volume',
                    'æˆäº¤é¢': 'amount',
                    'æ¢æ‰‹ç‡': 'turnover_rate',
                    'å¸‚ç›ˆç‡-åŠ¨æ€': 'pe_ratio'
                }

                # åº”ç”¨åˆ—åæ˜ å°„
                for old_col, new_col in column_mapping.items():
                    if old_col in realtime_data.columns:
                        realtime_data[new_col] = realtime_data[old_col]

                # æ·»åŠ å¿…è¦å­—æ®µ
                realtime_data['updated_at'] = datetime.now()

                # è®¡ç®—ä»Šæ—¥å¼€é«˜ä½æ”¶ï¼ˆå¦‚æœæœ‰åˆ†æ—¶æ•°æ®çš„è¯ï¼‰
                if 'ä»Šå¼€' in realtime_data.columns:
                    realtime_data['open_price'] = realtime_data['ä»Šå¼€']
                if 'æœ€é«˜' in realtime_data.columns:
                    realtime_data['high_price'] = realtime_data['æœ€é«˜']
                if 'æœ€ä½' in realtime_data.columns:
                    realtime_data['low_price'] = realtime_data['æœ€ä½']

                # æ’å…¥æ•°æ®åº“
                insert_result = self.inserter.insert_realtime_snapshot(realtime_data)

                # æ›´æ–°ç»Ÿè®¡
                self.sync_stats['total_updates'] += len(realtime_data)
                if insert_result.get('success'):
                    self.sync_stats['successful_updates'] += insert_result.get('inserted_rows', 0)
                else:
                    self.sync_stats['failed_updates'] += len(realtime_data)

                self.sync_stats['last_update_time'] = datetime.now()

                logger.debug(f"å¤„ç†å®æ—¶æ•°æ®æ‰¹æ¬¡: {len(realtime_data)} åªè‚¡ç¥¨")

        except Exception as e:
            logger.error(f"å¤„ç†å®æ—¶æ•°æ®æ‰¹æ¬¡å¤±è´¥: {e}")
            self.sync_stats['failed_updates'] += len(symbols)

    def stop_sync(self):
        """åœæ­¢å®æ—¶åŒæ­¥"""
        logger.info("æ­£åœ¨åœæ­¢å®æ—¶åŒæ­¥...")
        self.is_syncing = False

        # å¤„ç†é˜Ÿåˆ—ä¸­çš„å‰©ä½™æ•°æ®
        try:
            while not self.data_queue.empty():
                self.data_queue.get_nowait()
        except Empty:
            pass

    def add_symbol(self, symbol: str):
        """
        æ·»åŠ ç›‘æ§è‚¡ç¥¨

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
        """
        self.monitored_symbols.add(symbol)
        logger.info(f"æ·»åŠ ç›‘æ§è‚¡ç¥¨: {symbol}")

    def remove_symbol(self, symbol: str):
        """
        ç§»é™¤ç›‘æ§è‚¡ç¥¨

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
        """
        self.monitored_symbols.discard(symbol)
        logger.info(f"ç§»é™¤ç›‘æ§è‚¡ç¥¨: {symbol}")

    def get_sync_status(self) -> Dict[str, Any]:
        """
        è·å–åŒæ­¥çŠ¶æ€

        Returns:
            åŒæ­¥çŠ¶æ€ä¿¡æ¯
        """
        return {
            'is_syncing': self.is_syncing,
            'monitored_symbols_count': len(self.monitored_symbols),
            'queue_size': self.data_queue.qsize(),
            'stats': self.sync_stats.copy()
        }

    def force_update_symbol(self, symbol: str) -> bool:
        """
        å¼ºåˆ¶æ›´æ–°æŒ‡å®šè‚¡ç¥¨çš„å®æ—¶æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            realtime_data = self.collector.collect_realtime_quote(symbol)

            if realtime_data is not None and not realtime_data.empty:
                realtime_data['updated_at'] = datetime.now()
                insert_result = self.inserter.insert_realtime_snapshot(realtime_data)
                return insert_result.get('success', False)

        except Exception as e:
            logger.error(f"å¼ºåˆ¶æ›´æ–°è‚¡ç¥¨æ•°æ®å¤±è´¥ {symbol}: {e}")

        return False


def test_realtime_sync_pipeline():
    """æµ‹è¯•å®æ—¶æ•°æ®åŒæ­¥ç®¡é“"""
    config = PipelineConfig(
        name="test_realtime_sync",
        max_retries=1
    )

    pipeline = RealtimeSyncPipeline(config)

    # æ·»åŠ æµ‹è¯•è‚¡ç¥¨
    test_symbols = ['000001', '000002']
    for symbol in test_symbols:
        pipeline.add_symbol(symbol)

    # å¯åŠ¨åŒæ­¥
    sync_thread = threading.Thread(target=pipeline.execute, daemon=True)
    sync_thread.start()

    # ç›‘æ§åŒæ­¥çŠ¶æ€
    for i in range(30):  # ç›‘æ§30ç§’
        status = pipeline.get_sync_status()
        print(f"åŒæ­¥çŠ¶æ€: {status['is_syncing']}, ç›‘æ§è‚¡ç¥¨: {status['monitored_symbols_count']}, "
              f"æˆåŠŸæ›´æ–°: {status['stats']['successful_updates']}")
        time.sleep(1)

    # åœæ­¢åŒæ­¥
    pipeline.stop_sync()
    sync_thread.join(timeout=5)

    print("æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_realtime_sync_pipeline()
```

---

## ğŸ“± ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ

### 1. åˆ›å»ºç›‘æ§ç»„ä»¶

åˆ›å»º `src/data/pipelines/monitoring.py`ï¼š

```python
"""
æ•°æ®ç®¡é“ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
"""

import logging
import time
import smtplib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
from dataclasses import dataclass
import json
import os

from .scheduler import PipelineScheduler

logger = logging.getLogger(__name__)

@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    name: str
    condition: str  # å‘Šè­¦æ¡ä»¶
    threshold: float  # é˜ˆå€¼
    severity: str  # ä¸¥é‡ç¨‹åº¦: low, medium, high, critical
    enabled: bool = True
    cooldown_minutes: int = 30  # å†·å´æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    last_triggered: Optional[datetime] = None

@dataclass
class NotificationConfig:
    """é€šçŸ¥é…ç½®"""
    email_enabled: bool = False
    email_smtp_server: str = "smtp.gmail.com"
    email_smtp_port: int = 587
    email_username: str = ""
    email_password: str = ""
    email_recipients: List[str] = None

    webhook_enabled: bool = False
    webhook_url: str = ""

    def __post_init__(self):
        if self.email_recipients is None:
            self.email_recipients = []

class PipelineMonitor:
    """ç®¡é“ç›‘æ§å™¨"""

    def __init__(self, scheduler: PipelineScheduler, config: NotificationConfig = None):
        """
        åˆå§‹åŒ–ç›‘æ§å™¨

        Args:
            scheduler: ç®¡é“è°ƒåº¦å™¨
            config: é€šçŸ¥é…ç½®
        """
        self.scheduler = scheduler
        self.config = config or NotificationConfig()

        # å‘Šè­¦è§„åˆ™
        self.alert_rules = self._load_default_alert_rules()

        # ç›‘æ§çŠ¶æ€
        self.monitoring_stats = {
            'alerts_triggered': 0,
            'notifications_sent': 0,
            'last_check_time': None,
            'monitoring_start_time': datetime.now()
        }

    def _load_default_alert_rules(self) -> Dict[str, AlertRule]:
        """åŠ è½½é»˜è®¤å‘Šè­¦è§„åˆ™"""
        return {
            'high_failure_rate': AlertRule(
                name="ç®¡é“é«˜å¤±è´¥ç‡",
                condition="failure_rate",
                threshold=0.1,  # 10%å¤±è´¥ç‡
                severity="high"
            ),
            'long_execution_time': AlertRule(
                name="ç®¡é“æ‰§è¡Œæ—¶é—´è¿‡é•¿",
                condition="execution_time",
                threshold=1800,  # 30åˆ†é’Ÿ
                severity="medium"
            ),
            'queue_backlog': AlertRule(
                name="ä»»åŠ¡é˜Ÿåˆ—ç§¯å‹",
                condition="queue_size",
                threshold=50,
                severity="medium"
            ),
            'pipeline_stuck': AlertRule(
                name="ç®¡é“å¡æ­»",
                condition="stuck_pipeline",
                threshold=3600,  # 1å°æ—¶æ— å“åº”
                severity="critical"
            ),
            'data_freshness': AlertRule(
                name="æ•°æ®æ–°é²œåº¦é—®é¢˜",
                condition="data_age",
                threshold=24,  # 24å°æ—¶
                severity="high"
            )
        }

    def start_monitoring(self, check_interval: int = 60):
        """
        å¼€å§‹ç›‘æ§

        Args:
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        logger.info(f"å¼€å§‹ç®¡é“ç›‘æ§ï¼Œæ£€æŸ¥é—´éš”: {check_interval}ç§’")

        while True:
            try:
                self._check_all_rules()
                self.monitoring_stats['last_check_time'] = datetime.now()
                time.sleep(check_interval)

            except Exception as e:
                logger.error(f"ç›‘æ§æ£€æŸ¥å¼‚å¸¸: {e}")
                time.sleep(check_interval)

    def _check_all_rules(self):
        """æ£€æŸ¥æ‰€æœ‰å‘Šè­¦è§„åˆ™"""
        current_time = datetime.now()

        # è·å–è°ƒåº¦å™¨çŠ¶æ€
        scheduler_status = self.scheduler.get_status()

        # æ£€æŸ¥æ¯ä¸ªå‘Šè­¦è§„åˆ™
        for rule_name, rule in self.alert_rules.items():
            if not rule.enabled:
                continue

            # æ£€æŸ¥å†·å´æ—¶é—´
            if (rule.last_triggered and
                (current_time - rule.last_triggered).total_seconds() < rule.cooldown_minutes * 60):
                continue

            # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
            if self._evaluate_rule(rule, scheduler_status):
                self._trigger_alert(rule, scheduler_status)
                rule.last_triggered = current_time
                self.monitoring_stats['alerts_triggered'] += 1

    def _evaluate_rule(self, rule: AlertRule, status: Dict[str, Any]) -> bool:
        """
        è¯„ä¼°å‘Šè­¦è§„åˆ™

        Args:
            rule: å‘Šè­¦è§„åˆ™
            status: è°ƒåº¦å™¨çŠ¶æ€

        Returns:
            æ˜¯å¦è§¦å‘å‘Šè­¦
        """
        try:
            if rule.condition == "failure_rate":
                total_executed = status['stats']['total_executed']
                total_failed = status['stats']['total_failed']

                if total_executed > 0:
                    failure_rate = total_failed / total_executed
                    return failure_rate > rule.threshold

            elif rule.condition == "execution_time":
                # æ£€æŸ¥è¿è¡Œä¸­çš„ç®¡é“æ‰§è¡Œæ—¶é—´
                for pipeline_id, future in status.get('running_pipelines', {}).items():
                    # è¿™é‡Œéœ€è¦å®é™…çš„æ‰§è¡Œæ—¶é—´ä¿¡æ¯
                    pass

            elif rule.condition == "queue_size":
                return status['queue_size'] > rule.threshold

            elif rule.condition == "stuck_pipeline":
                # æ£€æŸ¥æ˜¯å¦æœ‰ç®¡é“è¿è¡Œæ—¶é—´è¿‡é•¿
                uptime = status.get('uptime', 0)
                running_pipelines = status.get('running_pipelines', 0)

                if running_pipelines > 0 and uptime > rule.threshold:
                    return True

            elif rule.condition == "data_freshness":
                # æ£€æŸ¥æ•°æ®æ–°é²œåº¦ï¼ˆéœ€è¦å®é™…å®ç°ï¼‰
                return self._check_data_freshness(rule.threshold)

        except Exception as e:
            logger.error(f"è¯„ä¼°å‘Šè­¦è§„åˆ™å¤±è´¥ {rule.condition}: {e}")

        return False

    def _check_data_freshness(self, max_age_hours: int) -> bool:
        """
        æ£€æŸ¥æ•°æ®æ–°é²œåº¦

        Args:
            max_age_hours: æœ€å¤§æ•°æ®å¹´é¾„ï¼ˆå°æ—¶ï¼‰

        Returns:
            æ•°æ®æ˜¯å¦è¿‡æœŸ
        """
        try:
            # æ£€æŸ¥æ—¥çº¿æ•°æ®æœ€æ–°æ—¥æœŸ
            from ..database import db_manager

            sql = """
            SELECT max(date) as latest_date
            FROM pulse_trader.daily_quotes
            """

            result = db_manager.execute_query(sql)
            if result and result[0][0]:
                latest_date = result[0][0]
                age_hours = (datetime.now() - latest_date).total_seconds() / 3600
                return age_hours > max_age_hours

        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®æ–°é²œåº¦å¤±è´¥: {e}")

        return False

    def _trigger_alert(self, rule: AlertRule, status: Dict[str, Any]):
        """
        è§¦å‘å‘Šè­¦

        Args:
            rule: å‘Šè­¦è§„åˆ™
            status: ç³»ç»ŸçŠ¶æ€
        """
        alert_message = self._generate_alert_message(rule, status)

        logger.warning(f"è§¦å‘å‘Šè­¦: {rule.name} - {alert_message}")

        # å‘é€é€šçŸ¥
        if self.config.email_enabled:
            self._send_email_alert(rule, alert_message, status)

        if self.config.webhook_enabled:
            self._send_webhook_alert(rule, alert_message, status)

    def _generate_alert_message(self, rule: AlertRule, status: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆå‘Šè­¦æ¶ˆæ¯

        Args:
            rule: å‘Šè­¦è§„åˆ™
            status: ç³»ç»ŸçŠ¶æ€

        Returns:
            å‘Šè­¦æ¶ˆæ¯
        """
        message_parts = [
            f"å‘Šè­¦åç§°: {rule.name}",
            f"ä¸¥é‡ç¨‹åº¦: {rule.severity}",
            f"è§¦å‘æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "ç³»ç»ŸçŠ¶æ€:",
            f"  è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if status['is_running'] else 'å·²åœæ­¢'}",
            f"  é˜Ÿåˆ—å¤§å°: {status['queue_size']}",
            f"  è¿è¡Œä¸­ç®¡é“: {status['running_pipelines']}",
            f"  å·²å®Œæˆç®¡é“: {status['completed_pipelines']}",
            "",
            "æ‰§è¡Œç»Ÿè®¡:",
            f"  æ€»æ‰§è¡Œæ•°: {status['stats']['total_executed']}",
            f"  æˆåŠŸæ•°: {status['stats']['total_success']}",
            f"  å¤±è´¥æ•°: {status['stats']['total_failed']}",
            f"  é‡è¯•æ•°: {status['stats']['total_retries']}"
        ]

        if status.get('stats', {}).get('last_execution'):
            last_exec = status['stats']['last_execution']
            message_parts.append(f"  æœ€åæ‰§è¡Œ: {last_exec.strftime('%Y-%m-%d %H:%M:%S')}")

        return "\n".join(message_parts)

    def _send_email_alert(self, rule: AlertRule, message: str, status: Dict[str, Any]):
        """
        å‘é€é‚®ä»¶å‘Šè­¦

        Args:
            rule: å‘Šè­¦è§„åˆ™
            message: å‘Šè­¦æ¶ˆæ¯
            status: ç³»ç»ŸçŠ¶æ€
        """
        try:
            # åˆ›å»ºé‚®ä»¶
            msg = MimeMultipart()
            msg['From'] = self.config.email_username
            msg['To'] = ", ".join(self.config.email_recipients)
            msg['Subject'] = f"[PulseTraderå‘Šè­¦] {rule.name} ({rule.severity})"

            # æ·»åŠ é‚®ä»¶æ­£æ–‡
            body = MimeText(message, 'plain', 'utf-8')
            msg.attach(body)

            # å‘é€é‚®ä»¶
            with smtplib.SMTP(self.config.email_smtp_server, self.config.email_smtp_port) as server:
                server.starttls()
                server.login(self.config.email_username, self.config.email_password)
                server.send_message(msg)

            logger.info(f"é‚®ä»¶å‘Šè­¦å‘é€æˆåŠŸ: {rule.name}")
            self.monitoring_stats['notifications_sent'] += 1

        except Exception as e:
            logger.error(f"å‘é€é‚®ä»¶å‘Šè­¦å¤±è´¥: {e}")

    def _send_webhook_alert(self, rule: AlertRule, message: str, status: Dict[str, Any]):
        """
        å‘é€Webhookå‘Šè­¦

        Args:
            rule: å‘Šè­¦è§„åˆ™
            message: å‘Šè­¦æ¶ˆæ¯
            status: ç³»ç»ŸçŠ¶æ€
        """
        try:
            import requests

            payload = {
                'alert_name': rule.name,
                'severity': rule.severity,
                'message': message,
                'timestamp': datetime.now().isoformat(),
                'system_status': status
            }

            response = requests.post(
                self.config.webhook_url,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                logger.info(f"Webhookå‘Šè­¦å‘é€æˆåŠŸ: {rule.name}")
                self.monitoring_stats['notifications_sent'] += 1
            else:
                logger.warning(f"Webhookå‘Šè­¦å‘é€å¤±è´¥: {response.status_code}")

        except Exception as e:
            logger.error(f"å‘é€Webhookå‘Šè­¦å¤±è´¥: {e}")

    def add_alert_rule(self, rule: AlertRule):
        """
        æ·»åŠ å‘Šè­¦è§„åˆ™

        Args:
            rule: å‘Šè­¦è§„åˆ™
        """
        self.alert_rules[rule.name] = rule
        logger.info(f"æ·»åŠ å‘Šè­¦è§„åˆ™: {rule.name}")

    def remove_alert_rule(self, rule_name: str):
        """
        ç§»é™¤å‘Šè­¦è§„åˆ™

        Args:
            rule_name: è§„åˆ™åç§°
        """
        if rule_name in self.alert_rules:
            del self.alert_rules[rule_name]
            logger.info(f"ç§»é™¤å‘Šè­¦è§„åˆ™: {rule_name}")

    def get_monitoring_status(self) -> Dict[str, Any]:
        """
        è·å–ç›‘æ§çŠ¶æ€

        Returns:
            ç›‘æ§çŠ¶æ€ä¿¡æ¯
        """
        return {
            'alert_rules_count': len(self.alert_rules),
            'enabled_rules_count': len([r for r in self.alert_rules.values() if r.enabled]),
            'monitoring_stats': self.monitoring_stats.copy(),
            'notification_config': {
                'email_enabled': self.config.email_enabled,
                'webhook_enabled': self.config.webhook_enabled
            }
        }


def test_pipeline_monitoring():
    """æµ‹è¯•ç®¡é“ç›‘æ§"""
    from .scheduler import PipelineScheduler
    from .config import PipelineConfig

    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = PipelineScheduler(max_workers=2, max_concurrent=1)

    # åˆ›å»ºç›‘æ§å™¨
    notification_config = NotificationConfig(
        email_enabled=False,  # æµ‹è¯•æ—¶ä¸å¯ç”¨é‚®ä»¶
        webhook_enabled=False
    )
    monitor = PipelineMonitor(scheduler, notification_config)

    # æ·»åŠ æµ‹è¯•å‘Šè­¦è§„åˆ™
    test_rule = AlertRule(
        name="æµ‹è¯•å‘Šè­¦",
        condition="queue_size",
        threshold=1,
        severity="low"
    )
    monitor.add_alert_rule(test_rule)

    # å¯åŠ¨ç›‘æ§
    import threading
    monitor_thread = threading.Thread(
        target=monitor.start_monitoring,
        args=(10,),  # 10ç§’æ£€æŸ¥ä¸€æ¬¡
        daemon=True
    )
    monitor_thread.start()

    print("ç›‘æ§å·²å¯åŠ¨ï¼Œ10ç§’ååœæ­¢...")
    time.sleep(10)

    print("ç›‘æ§çŠ¶æ€:")
    print(monitor.get_monitoring_status())


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_pipeline_monitoring()
```

---

## ğŸ¯ æœ¬èŠ‚å°ç»“

### âœ… å®Œæˆå†…å®¹

1. **ç®¡é“æ¡†æ¶**: å®ç°äº†å¯æ‰©å±•çš„æ•°æ®ç®¡é“åŸºç¡€æ¶æ„ï¼Œæ”¯æŒå›è°ƒã€é‡è¯•ã€è¶…æ—¶ç­‰åŠŸèƒ½
2. **è°ƒåº¦ç³»ç»Ÿ**: æ„å»ºäº†åŸºäºä¼˜å…ˆçº§çš„ä»»åŠ¡è°ƒåº¦å™¨ï¼Œæ”¯æŒå¹¶å‘æ§åˆ¶å’Œä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
3. **å…·ä½“ç®¡é“**: å®ç°äº†æ—¥çº¿æ•°æ®æ›´æ–°å’Œå®æ—¶æ•°æ®åŒæ­¥ä¸¤å¤§æ ¸å¿ƒç®¡é“
4. **ç›‘æ§å‘Šè­¦**: å»ºç«‹äº†å®Œå–„çš„ç›‘æ§å‘Šè­¦ç³»ç»Ÿï¼Œæ”¯æŒé‚®ä»¶å’ŒWebhooké€šçŸ¥
5. **å®¹é”™æœºåˆ¶**: å®ç°äº†é‡è¯•ã€è¶…æ—¶ã€é”™è¯¯å¤„ç†ç­‰å®¹é”™åŠŸèƒ½

### ğŸ“Š ç®¡é“ç³»ç»Ÿæ¶æ„

| ç»„ä»¶ | åŠŸèƒ½ | ç‰¹æ€§ |
|------|------|------|
| BasePipeline | ç®¡é“åŸºç¡€æ¡†æ¶ | å›è°ƒæœºåˆ¶ã€é‡è¯•ã€è¶…æ—¶ |
| PipelineScheduler | ä»»åŠ¡è°ƒåº¦å™¨ | ä¼˜å…ˆçº§é˜Ÿåˆ—ã€å¹¶å‘æ§åˆ¶ |
| DailyDataUpdatePipeline | æ—¥çº¿æ•°æ®æ›´æ–° | æ‰¹é‡å¤„ç†ã€å¢é‡æ›´æ–° |
| RealtimeSyncPipeline | å®æ—¶æ•°æ®åŒæ­¥ | äº¤æ˜“æ—¶é—´æ£€æµ‹ã€æŒç»­è¿è¡Œ |
| PipelineMonitor | ç›‘æ§å‘Šè­¦ | è§„åˆ™å¼•æ“ã€å¤šæ¸ é“é€šçŸ¥ |

### ğŸ”§ æ ¸å¿ƒç‰¹æ€§

1. **è‡ªåŠ¨åŒ–è°ƒåº¦**: åŸºäºCronè¡¨è¾¾å¼çš„å®šæ—¶ä»»åŠ¡è°ƒåº¦
2. **ä¼˜å…ˆçº§ç®¡ç†**: é«˜ä¼˜å…ˆçº§ä»»åŠ¡ä¼˜å…ˆæ‰§è¡Œ
3. **å¹¶å‘æ§åˆ¶**: é™åˆ¶åŒæ—¶è¿è¡Œçš„ç®¡é“æ•°é‡
4. **å¢é‡æ›´æ–°**: æ™ºèƒ½æ£€æµ‹æ•°æ®æ›´æ–°éœ€æ±‚
5. **å®¹é”™æ¢å¤**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **èµ„æºç®¡ç†**: åˆç†é…ç½®å·¥ä½œçº¿ç¨‹æ•°å’Œå¹¶å‘æ•°
2. **APIé™åˆ¶**: æ³¨æ„æ•°æ®æºæ¥å£çš„è°ƒç”¨é¢‘ç‡é™åˆ¶
3. **ç›‘æ§å‘Šè­¦**: åŠæ—¶å¤„ç†å‘Šè­¦ä¿¡æ¯ï¼Œé¿å…é—®é¢˜ç§¯ç´¯
4. **æ•°æ®ä¸€è‡´æ€§**: ç¡®ä¿æ•°æ®æ›´æ–°çš„äº‹åŠ¡æ€§å’Œä¸€è‡´æ€§

### ğŸš€ ä¸‹ä¸€æ­¥

è‡³æ­¤ï¼Œç¬¬äºŒç« æ•°æ®é‡‡é›†ä¸å­˜å‚¨çš„å†…å®¹å·²ç»å®Œæˆï¼æ‚¨å·²ç»å»ºç«‹äº†å®Œæ•´çš„æ•°æ®ç®¡é“ä½“ç³»ï¼ŒåŒ…æ‹¬ï¼š

âœ… **2.1 Akshareæ•°æ®æ¥å£é›†æˆ** - æ•°æ®è·å–èƒ½åŠ›
âœ… **2.2 æ•°æ®æ¸…æ´—ä¸éªŒè¯** - æ•°æ®è´¨é‡ä¿éšœ
âœ… **2.3 ClickHouseæ•°æ®è¡¨è®¾è®¡** - é«˜æ€§èƒ½å­˜å‚¨
âœ… **2.4 æ•°æ®æ’å…¥ä¸æŸ¥è¯¢ä¼˜åŒ–** - æ•°æ®æ“ä½œä¼˜åŒ–
âœ… **2.5 è‡ªåŠ¨åŒ–æ•°æ®ç®¡é“** - è‡ªåŠ¨åŒ–è¿è¡Œæœºåˆ¶

å‡†å¤‡å¥½è¿›å…¥ç¬¬ä¸‰ç« **Rustæ€§èƒ½æ¨¡å—**çš„å­¦ä¹ äº†å—ï¼Ÿ

**[â†’ å‰å¾€ ç¬¬ä¸‰ç« ï¼šRustæ€§èƒ½æ¨¡å—](../03-Rustæ¨¡å—å¼€å‘/README.md)**

---

## ğŸ†˜ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: ç®¡é“æ‰§è¡Œå¤±è´¥å¦‚ä½•æ’æŸ¥ï¼Ÿ

**A**: æ’æŸ¥æ­¥éª¤ï¼š
1. æŸ¥çœ‹ç®¡é“æ‰§è¡Œæ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯
2. æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€
3. éªŒè¯æ•°æ®æºAPIå¯ç”¨æ€§
4. ç¡®è®¤æ•°æ®æ ¼å¼å’Œç±»å‹åŒ¹é…
5. æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

### Q2: ä»»åŠ¡å †ç§¯æ€ä¹ˆåŠï¼Ÿ

**A**: è§£å†³æ–¹æ¡ˆï¼š
1. å¢åŠ å·¥ä½œçº¿ç¨‹æ•°å’Œå¹¶å‘æ•°
2. ä¼˜åŒ–ç®¡é“æ‰§è¡Œæ•ˆç‡
3. æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ—¶é—´è¿è¡Œçš„ç®¡é“
4. è€ƒè™‘ä»»åŠ¡åˆ†æ‰¹å¤„ç†
5. å¢åŠ ç³»ç»Ÿèµ„æº

### Q3: å®æ—¶æ•°æ®åŒæ­¥å»¶è¿Ÿé«˜å¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**A**: ä¼˜åŒ–å»ºè®®ï¼š
1. å‡å°‘ç›‘æ§è‚¡ç¥¨æ•°é‡
2. ä¼˜åŒ–æ‰¹é‡å¤„ç†å¤§å°
3. ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®åº“è¿æ¥æ± 
4. è€ƒè™‘ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—ç¼“å†²
5. ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç´¢å¼•

### Q4: å‘Šè­¦è¿‡äºé¢‘ç¹æ€ä¹ˆåŠï¼Ÿ

**A**: è°ƒæ•´ç­–ç•¥ï¼š
1. å¢åŠ å‘Šè­¦è§„åˆ™çš„å†·å´æ—¶é—´
2. è°ƒæ•´å‘Šè­¦é˜ˆå€¼
3. å®æ–½å‘Šè­¦èšåˆå’Œé™å™ª
4. æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†çº§å¤„ç†
5. ä¼˜åŒ–å‘Šè­¦æ¶ˆæ¯å†…å®¹

---

**æ­å–œï¼æ‚¨å·²ç»å®Œæˆäº†å®Œæ•´çš„æ•°æ®é‡‡é›†ä¸å­˜å‚¨ç³»ç»Ÿå»ºè®¾ï¼Œä¸ºé‡åŒ–äº¤æ˜“æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ï¼** ğŸ‰âœ¨
