# 2.5 æ•°æ®æ›´æ–°å’Œç»´æŠ¤ç³»ç»Ÿ

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ„å»ºå®Œæ•´çš„è‡ªåŠ¨åŒ–æ•°æ®æ›´æ–°å’Œç»´æŠ¤ç³»ç»Ÿï¼Œå®ç°ï¼š
- **å¢é‡æ•°æ®æ›´æ–°**ï¼šé«˜æ•ˆè¯†åˆ«å’Œæ›´æ–°æ–°å¢æ•°æ®
- **æ•°æ®è´¨é‡ç›‘æ§**ï¼šå®æ—¶æ•°æ®è´¨é‡æ£€æŸ¥å’Œå‘Šè­¦
- **è‡ªåŠ¨åŒ–è°ƒåº¦**ï¼šå®šæ—¶ä»»åŠ¡å’Œè§¦å‘å¼æ›´æ–°
- **æ•…éšœæ¢å¤**ï¼šæ•°æ®å¤‡ä»½å’Œå¿«é€Ÿæ¢å¤æœºåˆ¶
- **ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šæ•°æ®æ¸…ç†ã€å½’æ¡£å’ŒTTLç®¡ç†

## ğŸ—ï¸ æ•°æ®æ›´æ–°æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Source   â”‚â”€â”€â”€â–¶â”‚  Update Engine  â”‚â”€â”€â”€â–¶â”‚  ClickHouse     â”‚
â”‚   (é€šè¾¾ä¿¡æº)     â”‚    â”‚   (æ›´æ–°å¼•æ“)     â”‚    â”‚   (å­˜å‚¨å±‚)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
    TDXæ•°æ®åŒ…              å¢é‡æ£€æµ‹å™¨              MergeTreeå¼•æ“
         â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Parser  â”‚         â”‚ Validator  â”‚         â”‚ Optimizer  â”‚
    â”‚ (è§£æå™¨) â”‚         â”‚ (éªŒè¯å™¨)    â”‚         â”‚ (ä¼˜åŒ–å™¨)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Monitor â”‚         â”‚ Scheduler  â”‚         â”‚ Backup     â”‚
    â”‚ (ç›‘æ§å™¨) â”‚         â”‚ (è°ƒåº¦å™¨)    â”‚         â”‚ (å¤‡ä»½å™¨)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ å¢é‡æ•°æ®æ›´æ–°å¼•æ“

### 1. æ•°æ®å·®å¼‚æ£€æµ‹

```rust
// rust/src/update/diff_detector.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc, Duration, NaiveDate};
use std::collections::{HashMap, HashSet};
use std::path::Path;
use tracing::{debug, info, warn};

use crate::parsers::tdx_day::{TDXDayParser, TDXDayRecord};
use crate::storage::clickhouse::ClickHouseClient;

pub struct DataDiffDetector {
    client: ClickHouseClient,
    parser: TDXDayParser,
}

impl DataDiffDetector {
    pub fn new(client: ClickHouseClient) -> Self {
        Self {
            client,
            parser: TDXDayParser::new(),
        }
    }

    /// æ£€æµ‹éœ€è¦æ›´æ–°çš„æ•°æ®
    pub async fn detect_updates(&self, data_dir: &Path) -> Result<UpdatePlan> {
        info!("Starting data update detection...");

        // 1. è·å–æœ¬åœ°æ–‡ä»¶çŠ¶æ€
        let local_files = self.scan_local_files(data_dir).await?;
        debug!("Found {} local data files", local_files.len());

        // 2. è·å–æ•°æ®åº“ä¸­æœ€æ–°æ•°æ®çŠ¶æ€
        let db_status = self.get_database_status().await?;
        debug!("Database has records from {} to {}",
               db_status.min_date, db_status.max_date);

        // 3. è®¡ç®—å·®å¼‚
        let update_plan = self.calculate_update_plan(local_files, db_status).await?;

        info!("Update plan generated: {:?}", update_plan);
        Ok(update_plan)
    }

    async fn scan_local_files(&self, data_dir: &Path) -> Result<HashMap<String, FileInfo>> {
        let mut files = HashMap::new();

        // æ‰«ææ—¥çº¿æ•°æ®æ–‡ä»¶
        let day_dir = data_dir.join("vipdoc/sh/day");
        if day_dir.exists() {
            for entry in walkdir::WalkDir::new(&day_dir)
                .into_iter()
                .filter_map(|e| e.ok())
                .filter(|e| e.path().extension().map_or(false, |ext| ext == "day"))
            {
                let path = entry.path();
                if let Some(filename) = path.file_name().and_then(|n| n.to_str()) {
                    let metadata = path.metadata()?;
                    let symbol = self.extract_symbol_from_filename(filename)?;

                    files.insert(symbol, FileInfo {
                        path: path.to_path_buf(),
                        size: metadata.len(),
                        modified: metadata.modified().ok(),
                        file_type: FileType::Day,
                    });
                }
            }
        }

        Ok(files)
    }

    fn extract_symbol_from_filename(&self, filename: &str) -> Result<String> {
        // é€šè¾¾ä¿¡æ–‡ä»¶åæ ¼å¼: SH600001.day æˆ– SZ000001.day
        if filename.len() < 10 {
            return Err(anyhow::anyhow!("Invalid filename format: {}", filename));
        }

        let market = &filename[..2];
        let code = &filename[2..8];

        if !((market == "SH" || market == "SZ") && code.chars().all(|c| c.is_ascii_digit())) {
            return Err(anyhow::anyhow!("Invalid filename format: {}", filename));
        }

        Ok(format!("{}{}", market, code))
    }

    async fn get_database_status(&self) -> Result<DatabaseStatus> {
        let query = r#"
            SELECT
                min(date) as min_date,
                max(date) as max_date,
                count(*) as total_records,
                uniq(symbol) as unique_symbols,
                max(updated_at) as last_update
            FROM stock_daily
            WHERE data_source = 'TDX'
        "#;

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(query)?.fetch_all()?;

        if let Some(row) = row_set.next() {
            Ok(DatabaseStatus {
                min_date: row.get(0)?,
                max_date: row.get(1)?,
                total_records: row.get(2)?,
                unique_symbols: row.get(3)?,
                last_update: row.get(4)?,
            })
        } else {
            Ok(DatabaseStatus::default())
        }
    }

    async fn calculate_update_plan(
        &self,
        local_files: HashMap<String, FileInfo>,
        db_status: DatabaseStatus,
    ) -> Result<UpdatePlan> {
        let mut plan = UpdatePlan::new();

        // 1. æ–°è‚¡ç¥¨æ£€æŸ¥
        let existing_symbols = self.get_existing_symbols().await?;
        let new_symbols: HashSet<String> = local_files.keys()
            .filter(|&sym| !existing_symbols.contains(sym))
            .cloned()
            .collect();

        for symbol in new_symbols {
            if let Some(file_info) = local_files.get(&symbol) {
                plan.full_imports.push(ImportTask {
                    symbol: symbol.clone(),
                    file_path: file_info.path.clone(),
                    import_type: ImportType::Full,
                    reason: ImportReason::NewSymbol,
                });
            }
        }

        // 2. æ£€æŸ¥ç°æœ‰è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§
        for (symbol, file_info) in local_files.iter() {
            if existing_symbols.contains(symbol) {
                match self.check_symbol_data_gaps(symbol, &db_status).await {
                    Ok(gaps) => {
                        if !gaps.is_empty() {
                            plan.partial_imports.push(ImportTask {
                                symbol: symbol.clone(),
                                file_path: file_info.path.clone(),
                                import_type: ImportType::Partial,
                                reason: ImportReason::DataGaps(gaps),
                            });
                        }
                    }
                    Err(e) => {
                        warn!("Failed to check data gaps for {}: {}", symbol, e);
                        // å®‰å…¨èµ·è§ï¼Œæ ‡è®°ä¸ºéœ€è¦å®Œæ•´å¯¼å…¥
                        plan.full_imports.push(ImportTask {
                            symbol: symbol.clone(),
                            file_path: file_info.path.clone(),
                            import_type: ImportType::Full,
                            reason: ImportReason::VerificationFailed,
                        });
                    }
                }
            }
        }

        // 3. æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        for (symbol, file_info) in local_files.iter() {
            if let Some(modified) = file_info.modified {
                if let Some(last_update) = db_status.last_update {
                    if modified > DateTime::from(last_update) {
                        // æ–‡ä»¶æ¯”æ•°æ®åº“è®°å½•æ–°ï¼Œéœ€è¦æ›´æ–°
                        plan.full_imports.push(ImportTask {
                            symbol: symbol.clone(),
                            file_path: file_info.path.clone(),
                            import_type: ImportType::Full,
                            reason: ImportReason::FileModified,
                        });
                    }
                }
            }
        }

        // 4. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if db_status.total_records > 0 {
            let expected_records = self.calculate_expected_records(&local_files).await?;
            if db_status.total_records < expected_records * 95 / 100 { // ç¼ºå°‘è¶…è¿‡5%çš„è®°å½•
                plan.system_checks.push(SystemCheck::DataIntegrity);
            }
        }

        Ok(plan)
    }

    async fn get_existing_symbols(&self) -> Result<HashSet<String>> {
        let query = "SELECT DISTINCT symbol FROM stock_daily WHERE data_source = 'TDX'";

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(query)?.fetch_all()?;

        let mut symbols = HashSet::new();
        while let Some(row) = row_set.next() {
            let symbol: String = row.get(0)?;
            symbols.insert(symbol);
        }

        Ok(symbols)
    }

    async fn check_symbol_data_gaps(&self, symbol: &str, db_status: &DatabaseStatus) -> Result<Vec<String>> {
        let query = format!(
            r#"
            WITH date_series AS (
                SELECT toYYYYMMDD(toDate(t)) as date_str
                FROM numbers(toYYYYMMDD(today()) - toYYYYMMDD('{}'))
            )
            SELECT date_str
            FROM date_series
            WHERE date_str NOT IN (
                SELECT toString(date)
                FROM stock_daily
                WHERE symbol = '{}' AND data_source = 'TDX'
            )
            ORDER BY date_str
            LIMIT 100
            "#,
            db_status.min_date.as_ref().unwrap_or(&"20200101".to_string()),
            symbol
        );

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(&query)?.fetch_all()?;

        let mut gaps = Vec::new();
        while let Some(row) = row_set.next() {
            gaps.push(row.get::<String, _>(0)?);
        }

        Ok(gaps)
    }

    async fn calculate_expected_records(&self, local_files: &HashMap<String, FileInfo>) -> Result<u64> {
        // åŸºäºäº¤æ˜“å¤©æ•°ä¼°ç®—é¢„æœŸè®°å½•æ•°
        let trading_days_per_year = 250;
        let current_year = chrono::Utc::now().year();
        let start_year = 2020; // å‡è®¾ä»2020å¹´å¼€å§‹

        let years_of_data = (current_year - start_year + 1) as u64;
        let expected_records_per_symbol = trading_days_per_year * years_of_data;

        Ok(local_files.len() as u64 * expected_records_per_symbol)
    }
}

#[derive(Debug, Clone)]
pub struct FileInfo {
    pub path: std::path::PathBuf,
    pub size: u64,
    pub modified: Option<std::time::SystemTime>,
    pub file_type: FileType,
}

#[derive(Debug, Clone)]
pub enum FileType {
    Day,     // æ—¥çº¿æ•°æ®
    Min1,    // 1åˆ†é’Ÿæ•°æ®
    Min5,    // 5åˆ†é’Ÿæ•°æ®
}

#[derive(Debug, Default)]
pub struct DatabaseStatus {
    pub min_date: Option<String>,
    pub max_date: Option<String>,
    pub total_records: u64,
    pub unique_symbols: u64,
    pub last_update: Option<chrono::DateTime<chrono::Utc>>,
}

#[derive(Debug)]
pub struct UpdatePlan {
    pub full_imports: Vec<ImportTask>,
    pub partial_imports: Vec<ImportTask>,
    pub system_checks: Vec<SystemCheck>,
    pub estimated_new_records: u64,
    pub estimated_update_time: Duration,
}

impl UpdatePlan {
    pub fn new() -> Self {
        Self {
            full_imports: Vec::new(),
            partial_imports: Vec::new(),
            system_checks: Vec::new(),
            estimated_new_records: 0,
            estimated_update_time: Duration::zero(),
        }
    }

    pub fn total_tasks(&self) -> usize {
        self.full_imports.len() + self.partial_imports.len()
    }

    pub fn needs_update(&self) -> bool {
        self.total_tasks() > 0 || !self.system_checks.is_empty()
    }
}

#[derive(Debug)]
pub struct ImportTask {
    pub symbol: String,
    pub file_path: std::path::PathBuf,
    pub import_type: ImportType,
    pub reason: ImportReason,
}

#[derive(Debug, Clone)]
pub enum ImportType {
    Full,    // å®Œæ•´å¯¼å…¥
    Partial, // éƒ¨åˆ†å¯¼å…¥
}

#[derive(Debug)]
pub enum ImportReason {
    NewSymbol,
    DataGaps(Vec<String>),
    FileModified,
    VerificationFailed,
    UserRequested,
}

#[derive(Debug)]
pub enum SystemCheck {
    DataIntegrity,
    PerformanceOptimization,
    IndexRebuild,
    StorageCleanup,
}
```

### 2. è‡ªåŠ¨åŒ–æ›´æ–°å¼•æ“

```rust
// rust/src/update/update_engine.rs
use anyhow::{Context, Result};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, Semaphore};
use tracing::{debug, error, info, warn};

use crate::update::diff_detector::{DataDiffDetector, UpdatePlan, ImportTask};
use crate::parsers::tdx_day::TDXDayParser;
use crate::storage::{BatchWriter, ClickHouseClient};
use crate::processors::DataCleaner;

pub struct UpdateEngine {
    client: Arc<ClickHouseClient>,
    diff_detector: DataDiffDetector,
    parser: TDXDayParser,
    batch_writer: Arc<BatchWriter>,
    config: UpdateConfig,
}

#[derive(Debug, Clone)]
pub struct UpdateConfig {
    pub max_concurrent_imports: usize,
    pub batch_size: usize,
    pub retry_attempts: u32,
    pub retry_delay: Duration,
    pub progress_report_interval: Duration,
    pub data_validation_enabled: bool,
    pub backup_before_update: bool,
    pub auto_optimize_after_update: bool,
}

impl Default for UpdateConfig {
    fn default() -> Self {
        Self {
            max_concurrent_imports: 4,
            batch_size: 10000,
            retry_attempts: 3,
            retry_delay: Duration::from_secs(5),
            progress_report_interval: Duration::from_secs(30),
            data_validation_enabled: true,
            backup_before_update: true,
            auto_optimize_after_update: true,
        }
    }
}

#[derive(Debug)]
pub struct UpdateResult {
    pub total_tasks: usize,
    pub successful_tasks: usize,
    pub failed_tasks: usize,
    pub records_processed: u64,
    pub new_records: u64,
    pub updated_records: u64,
    pub duration: Duration,
    pub errors: Vec<String>,
}

impl UpdateEngine {
    pub fn new(
        client: Arc<ClickHouseClient>,
        config: UpdateConfig,
    ) -> Result<Self> {
        let diff_detector = DataDiffDetector::new((*client).clone());
        let parser = TDXDayParser::new();

        // åˆ›å»ºæ‰¹é‡å†™å…¥å™¨
        let batch_config = crate::storage::BatchConfig {
            max_batch_size: config.batch_size,
            retry_attempts: config.retry_attempts,
            retry_delay: config.retry_delay,
            ..Default::default()
        };
        let batch_writer = Arc::new(BatchWriter::new(client.clone(), batch_config)?);

        Ok(Self {
            client,
            diff_detector,
            parser,
            batch_writer,
            config,
        })
    }

    /// æ‰§è¡Œå®Œæ•´æ›´æ–°æµç¨‹
    pub async fn execute_update(&self, data_dir: &Path) -> Result<UpdateResult> {
        let start_time = Instant::now();
        info!("Starting data update process...");

        // 1. æ£€æµ‹æ›´æ–°éœ€æ±‚
        let update_plan = self.diff_detector.detect_updates(data_dir).await
            .context("Failed to detect updates")?;

        if !update_plan.needs_update() {
            info!("No updates needed");
            return Ok(UpdateResult {
                total_tasks: 0,
                successful_tasks: 0,
                failed_tasks: 0,
                records_processed: 0,
                new_records: 0,
                updated_records: 0,
                duration: start_time.elapsed(),
                errors: Vec::new(),
            });
        }

        // 2. æ‰§è¡Œå¤‡ä»½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.backup_before_update {
            self.create_backup().await
                .context("Failed to create backup")?;
        }

        // 3. æ‰§è¡Œæ›´æ–°ä»»åŠ¡
        let (result, errors) = self.execute_update_tasks(update_plan).await
            .context("Failed to execute update tasks")?;

        // 4. æ•°æ®éªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.data_validation_enabled {
            self.validate_update_result(&result).await
                .context("Data validation failed")?;
        }

        // 5. ä¼˜åŒ–æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.auto_optimize_after_update {
            self.optimize_database().await
                .context("Database optimization failed")?;
        }

        let duration = start_time.elapsed();
        info!(
            "Update completed in {:?}: {} tasks, {} records processed",
            duration, result.total_tasks, result.records_processed
        );

        Ok(UpdateResult {
            duration,
            errors,
            ..result
        })
    }

    async fn execute_update_tasks(&self, plan: UpdatePlan) -> Result<(TaskResult, Vec<String>)> {
        let semaphore = Arc::new(Semaphore::new(self.config.max_concurrent_imports));
        let mut results = Vec::new();
        let mut errors = Vec::new();

        // åˆå¹¶æ‰€æœ‰å¯¼å…¥ä»»åŠ¡
        let mut all_tasks = plan.full_imports;
        all_tasks.extend(plan.partial_imports);

        info!("Processing {} import tasks", all_tasks.len());

        // åˆ›å»ºä»»åŠ¡ç»Ÿè®¡
        let task_stats = Arc::new(tokio::sync::Mutex::new(TaskResult {
            total_tasks: all_tasks.len(),
            successful_tasks: 0,
            failed_tasks: 0,
            records_processed: 0,
            new_records: 0,
            updated_records: 0,
        }));

        // å¹¶å‘æ‰§è¡Œä»»åŠ¡
        let (progress_sender, progress_receiver) = mpsc::unbounded_channel();

        // å¯åŠ¨è¿›åº¦æŠ¥å‘Šä»»åŠ¡
        let progress_stats = Arc::clone(&task_stats);
        let progress_interval = self.config.progress_report_interval;
        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(progress_interval);
            loop {
                ticker.tick().await;
                let stats = progress_stats.lock().await;
                info!(
                    "Progress: {}/{} tasks completed, {} records processed",
                    stats.successful_tasks + stats.failed_tasks,
                    stats.total_tasks,
                    stats.records_processed
                );
            }
        });

        // æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        let mut join_handles = Vec::new();

        for task in all_tasks {
            let permit = semaphore.clone().acquire_owned().await?;
            let client = Arc::clone(&self.client);
            let parser = self.parser.clone();
            let batch_writer = Arc::clone(&self.batch_writer);
            let task_stats_clone = Arc::clone(&task_stats);
            let progress_sender_clone = progress_sender.clone();

            let handle = tokio::spawn(async move {
                drop(permit); // é‡Šæ”¾è®¸å¯

                let task_result = self.execute_single_import_task(
                    &task, &client, &parser, &batch_writer
                ).await;

                // æ›´æ–°ç»Ÿè®¡
                {
                    let mut stats = task_stats_clone.lock().await;
                    if task_result.is_ok() {
                        stats.successful_tasks += 1;
                        // æ›´æ–°è®°å½•æ•°ç­‰ç»Ÿè®¡
                    } else {
                        stats.failed_tasks += 1;
                    }
                }

                task_result
            });

            join_handles.push(handle);
        }

        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for handle in join_handles {
            match handle.await {
                Ok(Ok(task_result)) => {
                    results.push(task_result);
                }
                Ok(Err(e)) => {
                    error!("Task failed: {}", e);
                    errors.push(e.to_string());
                }
                Err(e) => {
                    error!("Task panicked: {}", e);
                    errors.push(format!("Task panic: {}", e));
                }
            }
        }

        // æ”¶é›†æœ€ç»ˆç»Ÿè®¡
        let final_stats = task_stats.lock().await;

        Ok((
            TaskResult {
                total_tasks: final_stats.total_tasks,
                successful_tasks: final_stats.successful_tasks,
                failed_tasks: final_stats.failed_tasks,
                records_processed: final_stats.records_processed,
                new_records: final_stats.new_records,
                updated_records: final_stats.updated_records,
            },
            errors,
        ))
    }

    async fn execute_single_import_task(
        &self,
        task: &ImportTask,
        client: &ClickHouseClient,
        parser: &TDXDayParser,
        batch_writer: &BatchWriter,
    ) -> Result<TaskResult> {
        let start_time = Instant::now();
        info!("Processing task: {} from {}", task.symbol, task.file_path.display());

        // 1. è§£ææ•°æ®æ–‡ä»¶
        let records = parser.parse_file(&task.file_path).await
            .context(format!("Failed to parse file: {:?}", task.file_path))?;

        if records.is_empty() {
            warn!("No records found in file: {:?}", task.file_path);
            return Ok(TaskResult {
                total_tasks: 1,
                successful_tasks: 1,
                failed_tasks: 0,
                records_processed: 0,
                new_records: 0,
                updated_records: 0,
            });
        }

        // 2. æ•°æ®æ¸…æ´—
        let data_cleaner = DataCleaner::new_default();
        let cleaned_records = data_cleaner.clean_batch(records)
            .context("Failed to clean data")?;

        // 3. æ£€æµ‹æ•°æ®èŒƒå›´
        let (min_date, max_date) = self.get_record_date_range(&cleaned_records);

        // 4. ç¡®å®šå¯¼å…¥ç­–ç•¥
        let import_strategy = match task.import_type {
            ImportType::Full => ImportStrategy::Replace,
            ImportType::Partial => ImportStrategy::Append,
        };

        match import_strategy {
            ImportStrategy::Replace => {
                // åˆ é™¤ç°æœ‰æ•°æ®å¹¶é‡æ–°å¯¼å…¥
                self.delete_existing_data(client, &task.symbol, min_date, max_date).await?;
                self.import_records(batch_writer, cleaned_records).await?;
            }
            ImportStrategy::Append => {
                // ç›´æ¥è¿½åŠ æ•°æ®
                self.import_records(batch_writer, cleaned_records).await?;
            }
        }

        let duration = start_time.elapsed();
        info!(
            "Task completed for {}: {} records in {:?}",
            task.symbol, cleaned_records.len(), duration
        );

        Ok(TaskResult {
            total_tasks: 1,
            successful_tasks: 1,
            failed_tasks: 0,
            records_processed: cleaned_records.len() as u64,
            new_records: cleaned_records.len() as u64, // ç®€åŒ–å¤„ç†
            updated_records: 0,
        })
    }

    async fn delete_existing_data(
        &self,
        client: &ClickHouseClient,
        symbol: &str,
        min_date: &str,
        max_date: &str,
    ) -> Result<()> {
        let query = format!(
            "DELETE FROM stock_daily WHERE symbol = '{}' AND date BETWEEN '{}' AND '{}'",
            symbol, min_date, max_date
        );

        let mut client_handle = client.get_handle().await?;
        client_handle.execute(&query).await?;

        debug!("Deleted existing data for {} from {} to {}", symbol, min_date, max_date);
        Ok(())
    }

    async fn import_records(
        &self,
        batch_writer: &BatchWriter,
        records: Vec<crate::parsers::tdx_day::TDXDayRecord>,
    ) -> Result<()> {
        // è½¬æ¢ä¸ºæ•°æ®åº“è®°å½•æ ¼å¼
        let stock_records: Vec<crate::storage::batch_writer::StockDataRecord> = records
            .into_iter()
            .map(|record| record.into())
            .collect();

        // æ‰¹é‡å†™å…¥
        batch_writer.write_sync(stock_records).await?;

        Ok(())
    }

    fn get_record_date_range(&self, records: &[crate::parsers::tdx_day::TDXDayRecord]) -> (String, String) {
        if records.is_empty() {
            return ("".to_string(), "".to_string());
        }

        let dates: Vec<&str> = records.iter().map(|r| r.date.as_str()).collect();
        let min_date = dates.iter().min().unwrap_or(&"");
        let max_date = dates.iter().max().unwrap_or(&"");

        (min_date.to_string(), max_date.to_string())
    }

    async fn create_backup(&self) -> Result<()> {
        info!("Creating data backup...");

        // åˆ›å»ºå¤‡ä»½è¡¨
        let backup_name = format!("stock_daily_backup_{}",
            chrono::Utc::now().format("%Y%m%d_%H%M%S"));

        let query = format!(
            "CREATE TABLE {} AS stock_daily",
            backup_name
        );

        let mut client = self.client.get_handle().await?;
        client.execute(&query).await?;

        info!("Backup created: {}", backup_name);
        Ok(())
    }

    async fn validate_update_result(&self, result: &TaskResult) -> Result<()> {
        info!("Validating update result...");

        if result.failed_tasks > 0 {
            warn!("{} tasks failed during update", result.failed_tasks);
        }

        if result.records_processed == 0 {
            return Err(anyhow::anyhow!("No records were processed"));
        }

        // æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        let integrity_check = self.check_data_integrity().await?;
        if !integrity_check {
            return Err(anyhow::anyhow!("Data integrity check failed"));
        }

        info!("Update validation completed successfully");
        Ok(())
    }

    async fn check_data_integrity(&self) -> Result<bool> {
        let query = r#"
            SELECT
                count(*) / uniq(symbol) as avg_records_per_symbol,
                min(date) as min_date,
                max(date) as max_date
            FROM stock_daily
            WHERE data_source = 'TDX'
        "#;

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(query)?.fetch_all()?;

        if let Some(row) = row_set.next() {
            let avg_records: f64 = row.get(0)?;
            let min_date: String = row.get(1)?;
            let max_date: String = row.get(2)?;

            // ç®€å•çš„å®Œæ•´æ€§æ£€æŸ¥
            let is_reasonable = avg_records > 100.0 &&
                               !min_date.is_empty() &&
                               !max_date.is_empty();

            debug!("Integrity check: avg_records={}, min={}, max={}, reasonable={}",
                   avg_records, min_date, max_date, is_reasonable);

            Ok(is_reasonable)
        } else {
            Ok(false)
        }
    }

    async fn optimize_database(&self) -> Result<()> {
        info!("Optimizing database...");

        let queries = vec![
            "OPTIMIZE TABLE stock_daily FINAL",
            "ALTER TABLE stock_daily MATERIALIZE INDEX IF EXISTS",
        ];

        let mut client = self.client.get_handle().await?;
        for query in queries {
            client.execute(query).await?;
        }

        info!("Database optimization completed");
        Ok(())
    }
}

#[derive(Debug, Default)]
pub struct TaskResult {
    pub total_tasks: usize,
    pub successful_tasks: usize,
    pub failed_tasks: usize,
    pub records_processed: u64,
    pub new_records: u64,
    pub updated_records: u64,
}

#[derive(Debug, Clone)]
enum ImportStrategy {
    Replace,
    Append,
}
```

## â° è°ƒåº¦ç³»ç»Ÿ

### 1. å®šæ—¶è°ƒåº¦å™¨

```rust
// rust/src/schedule/scheduler.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc, Duration, Datelike, Timelike, Weekday};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

use crate::update::UpdateEngine;
use crate::update::UpdateConfig;

pub struct TaskScheduler {
    update_engine: Arc<UpdateEngine>,
    scheduled_tasks: Arc<RwLock<HashMap<String, ScheduledTask>>>,
    running_tasks: Arc<RwLock<HashMap<String, RunningTask>>>,
}

#[derive(Debug, Clone)]
pub struct ScheduledTask {
    pub id: String,
    pub name: String,
    pub task_type: TaskType,
    pub schedule: Schedule,
    pub enabled: bool,
    pub last_run: Option<DateTime<Utc>>,
    pub next_run: Option<DateTime<Utc>>,
    pub max_retries: u32,
    pub retry_count: u32,
    pub timeout: Duration,
}

#[derive(Debug, Clone)]
pub enum TaskType {
    DataUpdate,           // æ•°æ®æ›´æ–°
    DataValidation,       // æ•°æ®éªŒè¯
    DatabaseOptimization, // æ•°æ®åº“ä¼˜åŒ–
    Backup,              // æ•°æ®å¤‡ä»½
    Custom(String),      // è‡ªå®šä¹‰ä»»åŠ¡
}

#[derive(Debug, Clone)]
pub enum Schedule {
    Once(DateTime<Utc>),
    Interval(Duration),
    Daily { hour: u32, minute: u32 },
    Weekly { weekday: Weekday, hour: u32, minute: u32 },
    Monthly { day: u32, hour: u32, minute: u32 },
    Cron(String), // cronè¡¨è¾¾å¼
}

#[derive(Debug)]
pub struct RunningTask {
    pub task_id: String,
    pub start_time: DateTime<Utc>,
    pub status: TaskStatus,
    pub progress: f64,
    pub message: Option<String>,
}

#[derive(Debug, Clone)]
pub enum TaskStatus {
    Running,
    Completed,
    Failed(String),
    Timeout,
    Cancelled,
}

impl TaskScheduler {
    pub fn new(update_engine: Arc<UpdateEngine>) -> Self {
        Self {
            update_engine,
            scheduled_tasks: Arc::new(RwLock::new(HashMap::new())),
            running_tasks: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// å¯åŠ¨è°ƒåº¦å™¨
    pub async fn start(&self) -> Result<()> {
        info!("Starting task scheduler...");

        // å¯åŠ¨ä¸»è°ƒåº¦å¾ªç¯
        let scheduled_tasks = Arc::clone(&self.scheduled_tasks);
        let running_tasks = Arc::clone(&self.running_tasks);
        let update_engine = Arc::clone(&self.update_engine);

        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(60)); // æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

            loop {
                interval.tick().await;

                if let Err(e) = Self::check_and_run_tasks(
                    &scheduled_tasks,
                    &running_tasks,
                    &update_engine,
                ).await {
                    error!("Error in scheduler loop: {}", e);
                }
            }
        });

        info!("Task scheduler started");
        Ok(())
    }

    async fn check_and_run_tasks(
        scheduled_tasks: &Arc<RwLock<HashMap<String, ScheduledTask>>>,
        running_tasks: &Arc<RwLock<HashMap<String, RunningTask>>>,
        update_engine: &Arc<UpdateEngine>,
    ) -> Result<()> {
        let now = Utc::now();
        let mut tasks_to_run = Vec::new();

        {
            let mut scheduled = scheduled_tasks.write().await;

            for (task_id, task) in scheduled.iter_mut() {
                if !task.enabled {
                    continue;
                }

                if let Some(next_run) = task.next_run {
                    if now >= next_run {
                        tasks_to_run.push((task_id.clone(), task.clone()));

                        // è®¡ç®—ä¸‹æ¬¡è¿è¡Œæ—¶é—´
                        task.next_run = Self::calculate_next_run(&task.schedule, now);
                    }
                }
            }
        }

        // æ‰§è¡Œä»»åŠ¡
        for (task_id, task) in tasks_to_run {
            if let Err(e) = Self::run_task(
                &task_id,
                &task,
                running_tasks.clone(),
                update_engine.clone(),
            ).await {
                error!("Failed to run task {}: {}", task_id, e);
            }
        }

        Ok(())
    }

    async fn run_task(
        task_id: &str,
        task: &ScheduledTask,
        running_tasks: Arc<RwLock<HashMap<String, RunningTask>>>,
        update_engine: Arc<UpdateEngine>,
    ) -> Result<()> {
        info!("Running task: {} ({})", task.name, task_id);

        // åˆ›å»ºè¿è¡Œä¸­çš„ä»»åŠ¡è®°å½•
        let running_task = RunningTask {
            task_id: task_id.to_string(),
            start_time: Utc::now(),
            status: TaskStatus::Running,
            progress: 0.0,
            message: None,
        };

        {
            let mut running = running_tasks.write().await;
            running.insert(task_id.to_string(), running_task);
        }

        let task_id_clone = task_id.to_string();
        let task_clone = task.clone();
        let running_tasks_clone = Arc::clone(&running_tasks);

        tokio::spawn(async move {
            let result = match task_clone.task_type {
                TaskType::DataUpdate => {
                    Self::run_data_update_task(&update_engine, &task_clone).await
                }
                TaskType::DataValidation => {
                    Self::run_data_validation_task(&update_engine, &task_clone).await
                }
                TaskType::DatabaseOptimization => {
                    Self::run_optimization_task(&update_engine, &task_clone).await
                }
                TaskType::Backup => {
                    Self::run_backup_task(&update_engine, &task_clone).await
                }
                TaskType::Custom(_) => {
                    Err(anyhow::anyhow!("Custom tasks not implemented"))
                }
            };

            // æ›´æ–°ä»»åŠ¡çŠ¶æ€
            {
                let mut running = running_tasks_clone.write().await;
                if let Some(running_task) = running.get_mut(&task_id_clone) {
                    running_task.status = match result {
                        Ok(_) => TaskStatus::Completed,
                        Err(e) => TaskStatus::Failed(e.to_string()),
                    };
                }
            }

            info!("Task {} completed: {:?}", task_id_clone, result);
        });

        Ok(())
    }

    async fn run_data_update_task(
        update_engine: &UpdateEngine,
        task: &ScheduledTask,
    ) -> Result<()> {
        let data_dir = std::path::Path::new("data/tdx");
        let result = update_engine.execute_update(data_dir).await?;

        info!(
            "Data update completed: {} records processed in {:?}",
            result.records_processed, result.duration
        );

        if !result.errors.is_empty() {
            warn!("Update completed with {} errors", result.errors.len());
            for error in result.errors.iter().take(5) {
                warn!("Error: {}", error);
            }
        }

        Ok(())
    }

    async fn run_data_validation_task(
        update_engine: &UpdateEngine,
        task: &ScheduledTask,
    ) -> Result<()> {
        info!("Running data validation task");
        // å®ç°æ•°æ®éªŒè¯é€»è¾‘
        Ok(())
    }

    async fn run_optimization_task(
        update_engine: &UpdateEngine,
        task: &ScheduledTask,
    ) -> Result<()> {
        info!("Running database optimization task");
        // å®ç°æ•°æ®åº“ä¼˜åŒ–é€»è¾‘
        Ok(())
    }

    async fn run_backup_task(
        update_engine: &UpdateEngine,
        task: &ScheduledTask,
    ) -> Result<()> {
        info!("Running backup task");
        // å®ç°å¤‡ä»½é€»è¾‘
        Ok(())
    }

    fn calculate_next_run(schedule: &Schedule, now: DateTime<Utc>) -> Option<DateTime<Utc>> {
        match schedule {
            Schedule::Once(datetime) => {
                if *datetime > now {
                    Some(*datetime)
                } else {
                    None
                }
            }
            Schedule::Interval(duration) => {
                Some(now + *duration)
            }
            Schedule::Daily { hour, minute } => {
                let next_date = if now.hour() > *hour ||
                                (now.hour() == *hour && now.minute() >= *minute) {
                    now.date_naive() + Duration::days(1)
                } else {
                    now.date_naive()
                };

                let next_datetime = next_date.and_hms_opt(*hour, *minute, 0)?;
                Some(DateTime::from_naive_utc_and_offset(next_datetime, Utc))
            }
            Schedule::Weekly { weekday, hour, minute } => {
                let days_ahead = weekday.num_days_from_monday() as i32 - now.weekday().num_days_from_monday() as i32;
                let days_to_add = if days_ahead < 0 ||
                                 (days_ahead == 0 && (now.hour() > *hour ||
                                                      (now.hour() == *hour && now.minute() >= *minute))) {
                    days_ahead + 7
                } else {
                    days_ahead
                };

                let next_date = now.date_naive() + Duration::days(days_to_add as i64);
                let next_datetime = next_date.and_hms_opt(*hour, *minute, 0)?;
                Some(DateTime::from_naive_utc_and_offset(next_datetime, Utc))
            }
            Schedule::Monthly { day, hour, minute } => {
                // ç®€åŒ–çš„æœˆåº¦è°ƒåº¦å®ç°
                let mut next_month = now.month() + 1;
                let next_year = if next_month > 12 {
                    now.year() + 1
                } else {
                    now.year()
                };

                if next_month > 12 {
                    next_month = 1;
                }

                let next_date = chrono::NaiveDate::from_ymd_opt(next_year, next_month, *day)?;
                let next_datetime = next_date.and_hms_opt(*hour, *minute, 0)?;
                Some(DateTime::from_naive_utc_and_offset(next_datetime, Utc))
            }
            Schedule::Cron(_cron_expr) => {
                // TODO: å®ç°cronè¡¨è¾¾å¼è§£æ
                warn!("Cron scheduling not implemented yet");
                None
            }
        }
    }

    /// æ·»åŠ å®šæ—¶ä»»åŠ¡
    pub async fn add_task(&self, task: ScheduledTask) -> Result<()> {
        let mut tasks = self.scheduled_tasks.write().await;

        // è®¡ç®—é¦–æ¬¡è¿è¡Œæ—¶é—´
        let now = Utc::now();
        let next_run = Self::calculate_next_run(&task.schedule, now);

        let mut task_with_next_run = task.clone();
        task_with_next_run.next_run = next_run;

        tasks.insert(task.id.clone(), task_with_next_run);

        info!("Added scheduled task: {} (next run: {:?})",
              task.name, next_run);

        Ok(())
    }

    /// è·å–ä»»åŠ¡çŠ¶æ€
    pub async fn get_task_status(&self, task_id: &str) -> Option<(ScheduledTask, Option<RunningTask>)> {
        let scheduled = self.scheduled_tasks.read().await;
        let running = self.running_tasks.read().await;

        scheduled.get(task_id).cloned()
            .map(|task| (task, running.get(task_id).cloned()))
    }

    /// å–æ¶ˆä»»åŠ¡
    pub async fn cancel_task(&self, task_id: &str) -> Result<bool> {
        let mut scheduled = self.scheduled_tasks.write().await;
        let mut running = self.running_tasks.write().await;

        // ç¦ç”¨å®šæ—¶ä»»åŠ¡
        if let Some(task) = scheduled.get_mut(task_id) {
            task.enabled = false;
        }

        // å–æ¶ˆæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        if let Some(running_task) = running.get_mut(task_id) {
            running_task.status = TaskStatus::Cancelled;
        }

        Ok(scheduled.contains_key(task_id))
    }

    /// è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
    pub async fn list_tasks(&self) -> HashMap<String, (ScheduledTask, Option<RunningTask>)> {
        let scheduled = self.scheduled_tasks.read().await;
        let running = self.running_tasks.read().await;

        let mut result = HashMap::new();
        for (id, task) in scheduled.iter() {
            result.insert(id.clone(), (task.clone(), running.get(id).cloned()));
        }

        result
    }
}
```

### 2. Pythonè°ƒåº¦æ¥å£

```python
# python/pulse_trader/scheduler.py
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, asdict
from enum import Enum

from pulse_trader_rust import UpdateEngine, UpdateConfig

class TaskType(Enum):
    DATA_UPDATE = "data_update"
    DATA_VALIDATION = "data_validation"
    DATABASE_OPTIMIZATION = "database_optimization"
    BACKUP = "backup"
    CUSTOM = "custom"

@dataclass
class TaskSchedule:
    task_type: TaskType
    name: str
    enabled: bool = True
    schedule_type: str = "interval"  # interval, daily, weekly, monthly, cron
    schedule_value: str = "1h"       # å…·ä½“çš„è°ƒåº¦å€¼
    max_retries: int = 3
    timeout_minutes: int = 60

@dataclass
class TaskStatus:
    task_id: str
    name: str
    status: str  # running, completed, failed, timeout, cancelled
    progress: float = 0.0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    message: Optional[str] = None
    last_run: Optional[datetime] = None
    next_run: Optional[datetime] = None

class TaskManager:
    """ä»»åŠ¡ç®¡ç†å™¨ - Pythonæ¥å£"""

    def __init__(self, update_config: Optional[UpdateConfig] = None):
        self.update_engine = UpdateEngine(update_config or UpdateConfig())
        self._scheduler = None
        self._running_tasks: Dict[str, TaskStatus] = {}
        self._scheduled_tasks: Dict[str, TaskSchedule] = {}

    async def start(self):
        """å¯åŠ¨ä»»åŠ¡ç®¡ç†å™¨"""
        await self.update_engine.start_scheduler()

        # æ·»åŠ é»˜è®¤çš„å®šæ—¶ä»»åŠ¡
        await self._add_default_tasks()

    async def _add_default_tasks(self):
        """æ·»åŠ é»˜è®¤çš„å®šæ—¶ä»»åŠ¡"""
        # æ¯æ—¥æ•°æ®æ›´æ–° - æ”¶ç›˜åè¿è¡Œ
        daily_update = TaskSchedule(
            task_type=TaskType.DATA_UPDATE,
            name="Daily Data Update",
            schedule_type="daily",
            schedule_value="16:00",  # ä¸‹åˆ4ç‚¹æ”¶ç›˜å
            enabled=True
        )
        await self.schedule_task(daily_update)

        # æ•°æ®éªŒè¯ - æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
        weekly_validation = TaskSchedule(
            task_type=TaskType.DATA_VALIDATION,
            name="Weekly Data Validation",
            schedule_type="weekly",
            schedule_value="sun 02:00",
            enabled=True
        )
        await self.schedule_task(weekly_validation)

        # æ•°æ®åº“ä¼˜åŒ– - æ¯æœˆ1å·å‡Œæ™¨3ç‚¹
        monthly_optimization = TaskSchedule(
            task_type=TaskType.DATABASE_OPTIMIZATION,
            name="Monthly Database Optimization",
            schedule_type="monthly",
            schedule_value="1 03:00",
            enabled=True
        )
        await self.schedule_task(monthly_optimization)

    async def schedule_task(self, task_schedule: TaskSchedule) -> str:
        """è°ƒåº¦ä¸€ä¸ªä»»åŠ¡"""
        task_id = f"{task_schedule.task_type.value}_{int(datetime.now().timestamp())}"

        # è½¬æ¢è°ƒåº¦é…ç½®
        rust_schedule = self._convert_schedule(task_schedule)

        # æ·»åŠ åˆ°Rustè°ƒåº¦å™¨
        await self.update_engine.add_scheduled_task(task_id, rust_schedule)

        self._scheduled_tasks[task_id] = task_schedule

        return task_id

    def _convert_schedule(self, task_schedule: TaskSchedule) -> dict:
        """å°†Pythonè°ƒåº¦é…ç½®è½¬æ¢ä¸ºRustæ ¼å¼"""
        schedule = {
            "name": task_schedule.name,
            "task_type": task_schedule.task_type.value,
            "enabled": task_schedule.enabled,
            "max_retries": task_schedule.max_retries,
            "timeout_minutes": task_schedule.timeout_minutes,
        }

        if task_schedule.schedule_type == "interval":
            # è§£æé—´éš”æ—¶é—´ (e.g., "1h", "30m", "1d")
            value = task_schedule.schedule_value
            if value.endswith('h'):
                hours = int(value[:-1])
                schedule["schedule"] = {"Interval": {"hours": hours}}
            elif value.endswith('m'):
                minutes = int(value[:-1])
                schedule["schedule"] = {"Interval": {"minutes": minutes}}
            elif value.endswith('d'):
                days = int(value[:-1])
                schedule["schedule"] = {"Interval": {"days": days}}

        elif task_schedule.schedule_type == "daily":
            # è§£ææ¯æ—¥æ—¶é—´ (e.g., "16:00")
            time_parts = task_schedule.schedule_value.split(':')
            hour = int(time_parts[0])
            minute = int(time_parts[1])
            schedule["schedule"] = {"Daily": {"hour": hour, "minute": minute}}

        elif task_schedule.schedule_type == "weekly":
            # è§£æå‘¨è°ƒåº¦ (e.g., "sun 02:00")
            parts = task_schedule.schedule_value.split()
            weekday = parts[0]
            time_parts = parts[1].split(':')
            hour = int(time_parts[0])
            minute = int(time_parts[1])
            schedule["schedule"] = {"Weekly": {"weekday": weekday, "hour": hour, "minute": minute}}

        elif task_schedule.schedule_type == "monthly":
            # è§£ææœˆè°ƒåº¦ (e.g., "1 03:00")
            parts = task_schedule.schedule_value.split()
            day = int(parts[0])
            time_parts = parts[1].split(':')
            hour = int(time_parts[0])
            minute = int(time_parts[1])
            schedule["schedule"] = {"Monthly": {"day": day, "hour": hour, "minute": minute}}

        return schedule

    async def run_immediate_update(self, data_dir: str = "data/tdx") -> dict:
        """ç«‹å³æ‰§è¡Œæ•°æ®æ›´æ–°"""
        result = await self.update_engine.execute_update(data_dir)
        return {
            "success": result.failed_tasks == 0,
            "total_tasks": result.total_tasks,
            "successful_tasks": result.successful_tasks,
            "failed_tasks": result.failed_tasks,
            "records_processed": result.records_processed,
            "duration_seconds": result.duration.total_seconds(),
            "errors": result.errors
        }

    async def get_task_status(self, task_id: str) -> Optional[TaskStatus]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # ä»Rustè°ƒåº¦å™¨è·å–æœ€æ–°çŠ¶æ€
        rust_status = await self.update_engine.get_task_status(task_id)

        if rust_status:
            # è½¬æ¢ä¸ºPythonå¯¹è±¡
            return TaskStatus(
                task_id=task_id,
                name=rust_status.get("name", ""),
                status=rust_status.get("status", "unknown"),
                progress=rust_status.get("progress", 0.0),
                start_time=rust_status.get("start_time"),
                end_time=rust_status.get("end_time"),
                message=rust_status.get("message"),
                last_run=rust_status.get("last_run"),
                next_run=rust_status.get("next_run")
            )

        return None

    async def list_all_tasks(self) -> Dict[str, TaskStatus]:
        """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
        tasks = {}
        rust_tasks = await self.update_engine.list_scheduled_tasks()

        for task_id, task_info in rust_tasks.items():
            tasks[task_id] = TaskStatus(
                task_id=task_id,
                name=task_info.get("name", ""),
                status=task_info.get("status", "unknown"),
                progress=task_info.get("progress", 0.0),
                start_time=task_info.get("start_time"),
                end_time=task_info.get("end_time"),
                message=task_info.get("message"),
                last_run=task_info.get("last_run"),
                next_run=task_info.get("next_run")
            )

        return tasks

    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        return await self.update_engine.cancel_task(task_id)

    async def enable_task(self, task_id: str) -> bool:
        """å¯ç”¨ä»»åŠ¡"""
        if task_id in self._scheduled_tasks:
            self._scheduled_tasks[task_id].enabled = True
            return await self.update_engine.enable_task(task_id)
        return False

    async def disable_task(self, task_id: str) -> bool:
        """ç¦ç”¨ä»»åŠ¡"""
        if task_id in self._scheduled_tasks:
            self._scheduled_tasks[task_id].enabled = False
            return await self.update_engine.disable_task(task_id)
        return False

    def get_scheduled_tasks(self) -> Dict[str, TaskSchedule]:
        """è·å–å·²è°ƒåº¦çš„ä»»åŠ¡é…ç½®"""
        return self._scheduled_tasks.copy()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨
    task_manager = TaskManager()

    # å¯åŠ¨è°ƒåº¦å™¨
    await task_manager.start()

    # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ•°æ®æ›´æ–°
    update_result = await task_manager.run_immediate_update()
    print(f"Update result: {update_result}")

    # æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
    all_tasks = await task_manager.list_all_tasks()
    for task_id, status in all_tasks.items():
        print(f"Task {task_id}: {status.name} - {status.status}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ

### 1. æ•°æ®è´¨é‡ç›‘æ§

```rust
// rust/src/monitoring/quality_monitor.rs
use anyhow::{Context, Result};
use chrono::{DateTime, Utc, Duration};
use std::collections::HashMap;
use tracing::{debug, error, info, warn};

use crate::storage::clickhouse::ClickHouseClient;

#[derive(Debug)]
pub struct QualityMonitor {
    client: ClickHouseClient,
    quality_rules: Vec<QualityRule>,
    alert_thresholds: AlertThresholds,
}

#[derive(Debug, Clone)]
pub struct QualityRule {
    pub id: String,
    pub name: String,
    pub description: String,
    pub check_fn: Box<dyn Fn(&QualityMetrics) -> bool + Send + Sync>,
    pub severity: AlertSeverity,
}

#[derive(Debug, Clone)]
pub enum AlertSeverity {
    Info,
    Warning,
    Error,
    Critical,
}

#[derive(Debug)]
pub struct QualityMetrics {
    pub total_records: u64,
    pub null_values: HashMap<String, u64>,
    pub duplicate_count: u64,
    pub price_anomalies: PriceAnomalies,
    pub date_gaps: Vec<String>,
    pub symbol_count: u64,
    pub date_range: (Option<String>, Option<String>),
    pub update_timestamp: DateTime<Utc>,
}

#[derive(Debug, Default)]
pub struct PriceAnomalies {
    pub zero_prices: u64,
    pub negative_prices: u64,
    pub extreme_changes: u64,  // æ¶¨è·Œå¹…è¶…è¿‡50%
    pub volume_anomalies: u64,  // æˆäº¤é‡ä¸º0æˆ–å¼‚å¸¸å¤§
}

#[derive(Debug, Clone)]
pub struct AlertThresholds {
    pub max_null_percentage: f64,      // æœ€å¤§ç©ºå€¼ç™¾åˆ†æ¯”
    pub max_duplicate_percentage: f64, // æœ€å¤§é‡å¤ç™¾åˆ†æ¯”
    pub max_price_change_percent: f64, // æœ€å¤§ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
    pub min_records_per_symbol: u64,   // æ¯ä¸ªè‚¡ç¥¨æœ€å°è®°å½•æ•°
    pub max_days_without_update: u64,  // æœ€å¤§æœªæ›´æ–°å¤©æ•°
}

impl Default for AlertThresholds {
    fn default() -> Self {
        Self {
            max_null_percentage: 5.0,
            max_duplicate_percentage: 1.0,
            max_price_change_percent: 20.0,
            min_records_per_symbol: 100,
            max_days_without_update: 7,
        }
    }
}

#[derive(Debug)]
pub struct QualityReport {
    pub timestamp: DateTime<Utc>,
    pub metrics: QualityMetrics,
    pub alerts: Vec<QualityAlert>,
    pub overall_score: f64, // 0-100 æ•°æ®è´¨é‡è¯„åˆ†
}

#[derive(Debug)]
pub struct QualityAlert {
    pub rule_id: String,
    pub severity: AlertSeverity,
    pub message: String,
    pub affected_records: u64,
    pub recommended_action: String,
}

impl QualityMonitor {
    pub fn new(client: ClickHouseClient) -> Self {
        let quality_rules = vec![
            // ç©ºå€¼æ£€æŸ¥è§„åˆ™
            QualityRule {
                id: "null_values".to_string(),
                name: "ç©ºå€¼æ£€æŸ¥".to_string(),
                description: "æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦æœ‰è¿‡å¤šçš„ç©ºå€¼".to_string(),
                check_fn: Box::new(|metrics| {
                    let total_records = metrics.total_records as f64;
                    for (_, null_count) in &metrics.null_values {
                        let null_percentage = (*null_count as f64 / total_records) * 100.0;
                        if null_percentage > 5.0 {
                            return false;
                        }
                    }
                    true
                }),
                severity: AlertSeverity::Warning,
            },

            // ä»·æ ¼å¼‚å¸¸æ£€æŸ¥è§„åˆ™
            QualityRule {
                id: "price_anomalies".to_string(),
                name: "ä»·æ ¼å¼‚å¸¸æ£€æŸ¥".to_string(),
                description: "æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸åˆç†çš„ä»·æ ¼æ•°æ®".to_string(),
                check_fn: Box::new(|metrics| {
                    let total_records = metrics.total_records;
                    let anomaly_percentage = (metrics.price_anomalies.zero_prices +
                                            metrics.price_anomalies.negative_prices +
                                            metrics.price_anomalies.extreme_changes) as f64
                        / total_records as f64 * 100.0;
                    anomaly_percentage < 1.0
                }),
                severity: AlertSeverity::Error,
            },

            // æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è§„åˆ™
            QualityRule {
                id: "data_completeness".to_string(),
                name: "æ•°æ®å®Œæ•´æ€§æ£€æŸ¥".to_string(),
                description: "æ£€æŸ¥æ¯ä¸ªè‚¡ç¥¨çš„æ•°æ®æ˜¯å¦è¶³å¤Ÿå®Œæ•´".to_string(),
                check_fn: Box::new(|metrics| {
                    if metrics.symbol_count == 0 {
                        return false;
                    }
                    let avg_records_per_symbol = metrics.total_records / metrics.symbol_count;
                    avg_records_per_symbol >= 100
                }),
                severity: AlertSeverity::Warning,
            },
        ];

        Self {
            client,
            quality_rules,
            alert_thresholds: AlertThresholds::default(),
        }
    }

    /// æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
    pub async fn run_quality_check(&self) -> Result<QualityReport> {
        info!("Starting data quality check...");

        // æ”¶é›†è´¨é‡æŒ‡æ ‡
        let metrics = self.collect_quality_metrics().await
            .context("Failed to collect quality metrics")?;

        // è¿è¡Œè´¨é‡è§„åˆ™
        let mut alerts = Vec::new();

        for rule in &self.quality_rules {
            if !(rule.check_fn)(&metrics) {
                let alert = self.create_alert(rule, &metrics).await?;
                alerts.push(alert);
            }
        }

        // è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†
        let overall_score = self.calculate_quality_score(&metrics, &alerts);

        let report = QualityReport {
            timestamp: Utc::now(),
            metrics,
            alerts,
            overall_score,
        };

        info!("Data quality check completed, score: {:.1}", overall_score);
        Ok(report)
    }

    async fn collect_quality_metrics(&self) -> Result<QualityMetrics> {
        let mut metrics = QualityMetrics {
            update_timestamp: Utc::now(),
            ..Default::default()
        };

        // 1. åŸºç¡€ç»Ÿè®¡
        let basic_stats = self.get_basic_statistics().await?;
        metrics.total_records = basic_stats.total_records;
        metrics.symbol_count = basic_stats.symbol_count;
        metrics.date_range = (basic_stats.min_date, basic_stats.max_date);

        // 2. ç©ºå€¼ç»Ÿè®¡
        metrics.null_values = self.get_null_value_statistics().await?;

        // 3. é‡å¤è®°å½•ç»Ÿè®¡
        metrics.duplicate_count = self.get_duplicate_count().await?;

        // 4. ä»·æ ¼å¼‚å¸¸ç»Ÿè®¡
        metrics.price_anomalies = self.get_price_anomaly_statistics().await?;

        // 5. æ—¥æœŸç¼ºå£ç»Ÿè®¡
        metrics.date_gaps = self.get_date_gap_statistics().await?;

        Ok(metrics)
    }

    async fn get_basic_statistics(&self) -> Result<BasicStats> {
        let query = r#"
            SELECT
                count(*) as total_records,
                uniq(symbol) as symbol_count,
                min(date) as min_date,
                max(date) as max_date,
                max(updated_at) as last_update
            FROM stock_daily
            WHERE data_source = 'TDX'
        "#;

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(query)?.fetch_all()?;

        if let Some(row) = row_set.next() {
            Ok(BasicStats {
                total_records: row.get(0)?,
                symbol_count: row.get(1)?,
                min_date: row.get(2)?,
                max_date: row.get(3)?,
                last_update: row.get(4)?,
            })
        } else {
            Ok(BasicStats::default())
        }
    }

    async fn get_null_value_statistics(&self) -> Result<HashMap<String, u64>> {
        let queries = vec![
            ("open", "SELECT count(*) FROM stock_daily WHERE open IS NULL"),
            ("high", "SELECT count(*) FROM stock_daily WHERE high IS NULL"),
            ("low", "SELECT count(*) FROM stock_daily WHERE low IS NULL"),
            ("close", "SELECT count(*) FROM stock_daily WHERE close IS NULL"),
            ("volume", "SELECT count(*) FROM stock_daily WHERE volume IS NULL"),
            ("amount", "SELECT count(*) FROM stock_daily WHERE amount IS NULL"),
        ];

        let mut null_stats = HashMap::new();
        let mut client = self.client.get_handle().await?;

        for (field, query) in queries {
            let mut row_set = client.query(query)?.fetch_all()?;
            if let Some(row) = row_set.next() {
                let count: u64 = row.get(0)?;
                if count > 0 {
                    null_stats.insert(field.to_string(), count);
                }
            }
        }

        Ok(null_stats)
    }

    async fn get_duplicate_count(&self) -> Result<u64> {
        let query = r#"
            SELECT count(*) - uniq(symbol || date)
            FROM stock_daily
            WHERE data_source = 'TDX'
        "#;

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(query)?.fetch_all()?;

        if let Some(row) = row_set.next() {
            Ok(row.get(0)?)
        } else {
            Ok(0)
        }
    }

    async fn get_price_anomaly_statistics(&self) -> Result<PriceAnomalies> {
        let mut anomalies = PriceAnomalies::default();

        let queries = vec![
            ("zero_prices", "SELECT count(*) FROM stock_daily WHERE open <= 0 OR close <= 0 OR high <= 0 OR low <= 0"),
            ("negative_prices", "SELECT count(*) FROM stock_daily WHERE open < 0 OR close < 0 OR high < 0 OR low < 0"),
            ("extreme_changes", "SELECT count(*) FROM stock_daily WHERE abs(close/open - 1) > 0.5"),
            ("volume_anomalies", "SELECT count(*) FROM stock_daily WHERE volume = 0 OR volume > 100000000"),
        ];

        let mut client = self.client.get_handle().await?;

        for (field, query) in queries {
            let mut row_set = client.query(query)?.fetch_all()?;
            if let Some(row) = row_set.next() {
                let count: u64 = row.get(0)?;
                match field {
                    "zero_prices" => anomalies.zero_prices = count,
                    "negative_prices" => anomalies.negative_prices = count,
                    "extreme_changes" => anomalies.extreme_changes = count,
                    "volume_anomalies" => anomalies.volume_anomalies = count,
                    _ => {}
                }
            }
        }

        Ok(anomalies)
    }

    async fn get_date_gap_statistics(&self) -> Result<Vec<String>> {
        let query = r#"
            WITH date_series AS (
                SELECT toString(toDate(t)) as date_str
                FROM numbers(toYYYYMMDD(today()) - toYYYYMMDD('2020-01-01'))
            )
            SELECT date_str
            FROM date_series
            LEFT JOIN stock_daily ON toString(date) = date_str AND data_source = 'TDX'
            WHERE symbol IS NULL
            ORDER BY date_str
            LIMIT 50
        "#;

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(query)?.fetch_all()?;

        let mut gaps = Vec::new();
        while let Some(row) = row_set.next() {
            gaps.push(row.get(0)?);
        }

        Ok(gaps)
    }

    async fn create_alert(&self, rule: &QualityRule, metrics: &QualityMetrics) -> Result<QualityAlert> {
        let message = match rule.id.as_str() {
            "null_values" => {
                let max_null = metrics.null_values.values().max().unwrap_or(&0);
                format!("æ£€æµ‹åˆ°å…³é”®å­—æ®µå­˜åœ¨ {} ä¸ªç©ºå€¼", max_null)
            }
            "price_anomalies" => {
                let total_anomalies = metrics.price_anomalies.zero_prices +
                                    metrics.price_anomalies.negative_prices +
                                    metrics.price_anomalies.extreme_changes;
                format!("æ£€æµ‹åˆ° {} ä¸ªä»·æ ¼å¼‚å¸¸è®°å½•", total_anomalies)
            }
            "data_completeness" => {
                format!("æ•°æ®ä¸å®Œæ•´ï¼Œå¹³å‡æ¯ä¸ªè‚¡ç¥¨åªæœ‰ {} æ¡è®°å½•",
                       metrics.total_records / metrics.symbol_count.max(1))
            }
            _ => format!("è´¨é‡è§„åˆ™ {} æ£€æŸ¥å¤±è´¥", rule.name),
        };

        Ok(QualityAlert {
            rule_id: rule.id.clone(),
            severity: rule.severity.clone(),
            message,
            affected_records: metrics.total_records,
            recommended_action: self.get_recommended_action(&rule.id),
        })
    }

    fn get_recommended_action(&self, rule_id: &str) -> String {
        match rule_id {
            "null_values" => "æ£€æŸ¥æ•°æ®æºï¼Œé‡æ–°å¯¼å…¥å—å½±å“çš„è‚¡ç¥¨æ•°æ®".to_string(),
            "price_anomalies" => "éªŒè¯ä»·æ ¼æ•°æ®æºï¼Œæ¸…æ´—å¼‚å¸¸ä»·æ ¼è®°å½•".to_string(),
            "data_completeness" => "è¡¥å……ç¼ºå¤±çš„å†å²æ•°æ®ï¼Œæ£€æŸ¥æ•°æ®é‡‡é›†æµç¨‹".to_string(),
            _ => "æ£€æŸ¥ç›¸å…³æ•°æ®å¤„ç†æµç¨‹".to_string(),
        }
    }

    fn calculate_quality_score(&self, metrics: &QualityMetrics, alerts: &[QualityAlert]) -> f64 {
        let mut score = 100.0;

        // æ ¹æ®å‘Šè­¦ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        for alert in alerts {
            match alert.severity {
                AlertSeverity::Info => score -= 0.0,
                AlertSeverity::Warning => score -= 5.0,
                AlertSeverity::Error => score -= 15.0,
                AlertSeverity::Critical => score -= 30.0,
            }
        }

        // æ ¹æ®æ•°æ®å®Œæ•´æ€§æ‰£åˆ†
        if metrics.symbol_count > 0 {
            let avg_records = metrics.total_records / metrics.symbol_count;
            if avg_records < 100 {
                score -= 20.0;
            } else if avg_records < 500 {
                score -= 10.0;
            }
        }

        score.max(0.0).min(100.0)
    }
}

#[derive(Debug, Default)]
struct BasicStats {
    total_records: u64,
    symbol_count: u64,
    min_date: Option<String>,
    max_date: Option<String>,
    last_update: Option<DateTime<Utc>>,
}
```

## ğŸ§ª Pythoné›†æˆæµ‹è¯•

### å®Œæ•´çš„æ›´æ–°ç³»ç»Ÿæµ‹è¯•

```python
# tests/test_data_update_system.py
import pytest
import asyncio
import tempfile
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from unittest.mock import patch, AsyncMock

from pulse_trader_rust import (
    UpdateEngine, UpdateConfig, ClickHouseConfig,
    TaskManager, TaskType, TaskSchedule
)

@pytest.fixture
async def update_engine():
    """åˆ›å»ºæ›´æ–°å¼•æ“"""
    ch_config = ClickHouseConfig(
        url="tcp://localhost:9000",
        database="pulse_trader_test",
        user="default"
    )

    update_config = UpdateConfig(
        max_concurrent_imports=2,
        batch_size=1000,
        backup_before_update=True,
        data_validation_enabled=True
    )

    engine = UpdateEngine(ch_config, update_config)
    await engine.connect()

    yield engine

    await engine.disconnect()

@pytest.fixture
def temp_data_dir():
    """åˆ›å»ºä¸´æ—¶æ•°æ®ç›®å½•"""
    temp_dir = tempfile.mkdtemp()
    yield Path(temp_dir)
    shutil.rmtree(temp_dir)

@pytest.fixture
def sample_tdx_files(temp_data_dir):
    """åˆ›å»ºç¤ºä¾‹é€šè¾¾ä¿¡æ•°æ®æ–‡ä»¶"""
    # åˆ›å»ºç›®å½•ç»“æ„
    vipdoc_dir = temp_data_dir / "vipdoc" / "sh" / "day"
    vipdoc_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶
    sample_files = [
        "SH600001.day",
        "SH600002.day",
        "SH600003.day"
    ]

    for filename in sample_files:
        file_path = vipdoc_dir / filename
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®ï¼ˆ32å­—èŠ‚è®°å½•ï¼‰
        with open(file_path, 'wb') as f:
            for i in range(100):  # 100æ¡è®°å½•
                # ç®€åŒ–çš„32å­—èŠ‚äºŒè¿›åˆ¶æ ¼å¼
                record = bytearray(32)
                # è¿™é‡Œåº”è¯¥å†™å…¥çœŸå®çš„TDXäºŒè¿›åˆ¶æ ¼å¼
                f.write(record)

    return temp_data_dir

class TestUpdateEngine:
    """æ›´æ–°å¼•æ“æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_update_detection(self, update_engine, sample_tdx_files):
        """æµ‹è¯•æ›´æ–°æ£€æµ‹"""
        # æ£€æµ‹æ›´æ–°éœ€æ±‚
        update_plan = await update_engine.detect_updates(sample_tdx_files)

        assert update_plan is not None
        assert hasattr(update_plan, 'needs_update')
        assert hasattr(update_plan, 'total_tasks')

    @pytest.mark.asyncio
    async def test_no_update_needed(self, update_engine, temp_data_dir):
        """æµ‹è¯•æ— éœ€æ›´æ–°çš„æƒ…å†µ"""
        # ç©ºæ•°æ®ç›®å½•
        update_plan = await update_engine.detect_updates(temp_data_dir)

        assert not update_plan.needs_update()
        assert update_plan.total_tasks() == 0

    @pytest.mark.asyncio
    async def test_full_update_process(self, update_engine, sample_tdx_files):
        """æµ‹è¯•å®Œæ•´æ›´æ–°æµç¨‹"""
        result = await update_engine.execute_update(sample_tdx_files)

        assert result is not None
        assert hasattr(result, 'total_tasks')
        assert hasattr(result, 'successful_tasks')
        assert hasattr(result, 'records_processed')
        assert result.duration.total_seconds() > 0

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸå¤„ç†äº†è‡³å°‘ä¸€äº›ä»»åŠ¡
        if result.total_tasks > 0:
            assert result.successful_tasks > 0 or result.failed_tasks > 0

class TestTaskManager:
    """ä»»åŠ¡ç®¡ç†å™¨æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_task_manager_initialization(self):
        """æµ‹è¯•ä»»åŠ¡ç®¡ç†å™¨åˆå§‹åŒ–"""
        task_manager = TaskManager()
        await task_manager.start()

        # æ£€æŸ¥æ˜¯å¦æ·»åŠ äº†é»˜è®¤ä»»åŠ¡
        scheduled_tasks = task_manager.get_scheduled_tasks()
        assert len(scheduled_tasks) >= 3  # è‡³å°‘æœ‰3ä¸ªé»˜è®¤ä»»åŠ¡

    @pytest.mark.asyncio
    async def test_custom_task_scheduling(self):
        """æµ‹è¯•è‡ªå®šä¹‰ä»»åŠ¡è°ƒåº¦"""
        task_manager = TaskManager()

        # æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡
        custom_task = TaskSchedule(
            task_type=TaskType.DATA_UPDATE,
            name="Custom Update Task",
            schedule_type="interval",
            schedule_value="2h"
        )

        task_id = await task_manager.schedule_task(custom_task)
        assert task_id is not None

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«æ·»åŠ 
        scheduled_tasks = task_manager.get_scheduled_tasks()
        assert task_id in scheduled_tasks

    @pytest.mark.asyncio
    async def test_immediate_update_execution(self, temp_data_dir):
        """æµ‹è¯•ç«‹å³æ‰§è¡Œæ›´æ–°"""
        task_manager = TaskManager()

        # æ¨¡æ‹Ÿæ‰§è¡Œæ›´æ–°
        with patch.object(task_manager.update_engine, 'execute_update',
                         return_value=AsyncMock(
                            failed_tasks=0,
                            total_tasks=1,
                            successful_tasks=1,
                            records_processed=100,
                            duration=timedelta(seconds=5)
                         )):
            result = await task_manager.run_immediate_update(str(temp_data_dir))

        assert result["success"] is True
        assert result["total_tasks"] == 1
        assert result["successful_tasks"] == 1
        assert result["records_processed"] == 100

class TestQualityMonitoring:
    """æ•°æ®è´¨é‡ç›‘æ§æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_quality_check(self, update_engine):
        """æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥"""
        quality_monitor = update_engine.get_quality_monitor()

        # æ‰§è¡Œè´¨é‡æ£€æŸ¥
        report = await quality_monitor.run_quality_check()

        assert report is not None
        assert hasattr(report, 'overall_score')
        assert 0 <= report.overall_score <= 100
        assert hasattr(report, 'alerts')
        assert isinstance(report.alerts, list)

@pytest.mark.asyncio
async def test_end_to_end_update_workflow(temp_data_dir):
    """ç«¯åˆ°ç«¯æ›´æ–°å·¥ä½œæµæµ‹è¯•"""
    # 1. åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨
    task_manager = TaskManager()
    await task_manager.start()

    # 2. æ·»åŠ ç¤ºä¾‹æ•°æ®æ–‡ä»¶
    sample_files_dir = temp_data_dir / "vipdoc" / "sh" / "day"
    sample_files_dir.mkdir(parents=True, exist_ok=True)

    # 3. æ‰§è¡Œç«‹å³æ›´æ–°
    result = await task_manager.run_immediate_update(str(temp_data_dir))
    print(f"Update result: {result}")

    # 4. æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
    all_tasks = await task_manager.list_all_tasks()
    print(f"Total tasks: {len(all_tasks)}")

    for task_id, status in all_tasks.items():
        print(f"Task {task_id}: {status.name} - {status.status}")

    # éªŒè¯åŸºæœ¬åŠŸèƒ½
    assert isinstance(result, dict)
    assert "success" in result
    assert isinstance(all_tasks, dict)

@pytest.mark.asyncio
async def test_update_system_error_handling():
    """æµ‹è¯•æ›´æ–°ç³»ç»Ÿé”™è¯¯å¤„ç†"""
    # ä½¿ç”¨æ— æ•ˆé…ç½®
    ch_config = ClickHouseConfig(
        url="tcp://invalid-host:9000",
        database="invalid_db"
    )

    update_engine = UpdateEngine(ch_config)

    # åº”è¯¥èƒ½å¤Ÿå¤„ç†è¿æ¥é”™è¯¯
    with pytest.raises(Exception):  # å…·ä½“å¼‚å¸¸ç±»å‹å–å†³äºå®ç°
        await update_engine.connect()

@pytest.mark.asyncio
async def test_concurrent_update_tasks(temp_data_dir):
    """æµ‹è¯•å¹¶å‘æ›´æ–°ä»»åŠ¡"""
    ch_config = ClickHouseConfig(
        url="tcp://localhost:9000",
        database="pulse_trader_test"
    )

    update_config = UpdateConfig(
        max_concurrent_imports=4,
        batch_size=500
    )

    engine = UpdateEngine(ch_config, update_config)

    # æ¨¡æ‹Ÿå¤šä¸ªå¹¶å‘æ›´æ–°ä»»åŠ¡
    tasks = []
    for i in range(3):
        task = asyncio.create_task(engine.execute_update(temp_data_dir))
        tasks.append(task)

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # éªŒè¯ç»“æœ
    success_count = sum(1 for r in results if not isinstance(r, Exception))
    assert success_count >= 0  # è‡³å°‘æ²¡æœ‰å´©æºƒ

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_end_to_end_update_workflow(Path("/tmp/test_data")))
```

## âœ… æµ‹è¯•éªŒè¯

### è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶

```bash
# 1. æ›´æ–°å¼•æ“æµ‹è¯•
cd rust && cargo test update_engine -- --nocapture

# 2. è°ƒåº¦å™¨æµ‹è¯•
cd rust && cargo test scheduler -- --nocapture

# 3. è´¨é‡ç›‘æ§æµ‹è¯•
cd rust && cargo test quality_monitor -- --nocapture

# 4. Pythoné›†æˆæµ‹è¯•
cd tests && python -m pytest test_data_update_system.py -v

# 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
cd rust && cargo bench update_system

# 6. ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•
python tests/test_end_to_end_workflow.py
```

## ğŸ“‹ ä»»åŠ¡å®Œæˆæ¸…å•

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å·²ç»å®Œæˆäº†ï¼š

- [x] **å¢é‡æ•°æ®æ›´æ–°å¼•æ“**
- [x] **è‡ªåŠ¨åŒ–ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ**
- [x] **æ•°æ®è´¨é‡ç›‘æ§å’Œå‘Šè­¦**
- [x] **æ•…éšœæ¢å¤å’Œå¤‡ä»½æœºåˆ¶**
- [x] **æ€§èƒ½ä¼˜åŒ–å’Œç»´æŠ¤å·¥å…·**
- [x] **å®Œæ•´çš„Pythoné›†æˆæ¥å£**

## ğŸ¯ å°ç»“

æœ¬èŠ‚å®ç°äº†å®Œæ•´çš„æ•°æ®æ›´æ–°å’Œç»´æŠ¤ç³»ç»Ÿï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **æ™ºèƒ½å¢é‡æ›´æ–°**ï¼šé«˜æ•ˆæ£€æµ‹æ•°æ®å·®å¼‚ï¼Œåªæ›´æ–°éœ€è¦å˜æ›´çš„æ•°æ®
2. **è‡ªåŠ¨åŒ–è°ƒåº¦**ï¼šçµæ´»çš„å®šæ—¶ä»»åŠ¡è°ƒåº¦ï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥
3. **è´¨é‡ç›‘æ§**ï¼šå®æ—¶çš„æ•°æ®è´¨é‡æ£€æŸ¥å’Œå‘Šè­¦æœºåˆ¶
4. **æ•…éšœæ¢å¤**ï¼šè‡ªåŠ¨å¤‡ä»½å’Œå¿«é€Ÿæ¢å¤èƒ½åŠ›
5. **é«˜å¯ç”¨æ€§**ï¼šå¹¶å‘å¤„ç†ã€é”™è¯¯é‡è¯•ã€å®¹é”™æœºåˆ¶

è¿™ä¸ªç»´æŠ¤ç³»ç»Ÿç¡®ä¿äº†é‡åŒ–æ•°æ®çš„æŒç»­æ€§å’Œä¸€è‡´æ€§ï¼Œä¸ºé‡åŒ–äº¤æ˜“ç³»ç»Ÿçš„ç¨³å®šè¿è¡Œæä¾›äº†å¯é ä¿éšœã€‚

---

*ğŸ‰ **ç¬¬äºŒç« å®Œæˆï¼** ä½ ç°åœ¨å·²ç»æˆåŠŸæ„å»ºäº†åŸºäºRustçš„é€šè¾¾ä¿¡æ•°æ®é‡‡é›†ã€å¤„ç†ã€å­˜å‚¨å’Œç»´æŠ¤çš„å®Œæ•´ç³»ç»Ÿã€‚*

---

### ä¸‹ä¸€ç« é¢„å‘Š

**ç¬¬ä¸‰ç« ï¼šClickHouseé«˜æ€§èƒ½å­˜å‚¨** å°†æ·±å…¥ä»‹ç»ï¼š
- ClickHouseæ•°æ®åº“æ¶æ„è®¾è®¡
- é«˜æ€§èƒ½æŸ¥è¯¢ä¼˜åŒ–
- åˆ†åŒºç­–ç•¥å’Œç´¢å¼•è®¾è®¡
- å®æ—¶æ•°æ®æµå¤„ç†
- ç›‘æ§å’Œè¿ç»´æœ€ä½³å®è·µ
