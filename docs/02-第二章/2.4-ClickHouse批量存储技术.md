# 2.4 ClickHouseæ‰¹é‡å­˜å‚¨æŠ€æœ¯

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æŒæ¡åŸºäºRustçš„ClickHouseé«˜æ€§èƒ½æ‰¹é‡å­˜å‚¨æŠ€æœ¯ï¼Œå®ç°ï¼š
- **æ‰¹é‡å†™å…¥ä¼˜åŒ–**ï¼šæ¯ç§’ç™¾ä¸‡çº§è®°å½•å†™å…¥èƒ½åŠ›
- **å†…å­˜ç®¡ç†**ï¼šé›¶æ‹·è´æ•°æ®ä¼ è¾“å’Œæµå¼å¤„ç†
- **å®¹é”™æœºåˆ¶**ï¼šæ–­çº¿é‡è¿å’Œæ•°æ®å®Œæ•´æ€§ä¿è¯
- **æ€§èƒ½è°ƒä¼˜**ï¼šå‚æ•°ä¼˜åŒ–å’ŒåŸºå‡†æµ‹è¯•

## ğŸ—ï¸ ClickHouseå­˜å‚¨æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Rust Parser   â”‚â”€â”€â”€â–¶â”‚  Batch Buffer   â”‚â”€â”€â”€â–¶â”‚ ClickHouse     â”‚
â”‚   (è§£æå™¨)       â”‚    â”‚   (æ‰¹é‡ç¼“å†²åŒº)    â”‚    â”‚   (æ•°æ®åº“)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
    32-byte records        Memory pool          MergeTreeå¼•æ“
         â”‚                  é¢„åˆ†é…å†…å­˜              åˆ†åŒºå­˜å‚¨
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Parallel â”‚         â”‚ Stream     â”‚         â”‚ Replicatedâ”‚
    â”‚ Processingâ”‚       â”‚ Processing â”‚        â”‚   Table   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®åº“è¡¨ç»“æ„è®¾è®¡

#### 1. æ—¥çº¿æ•°æ®è¡¨

```sql
-- åˆ›å»ºæ—¥çº¿æ•°æ®è¡¨
CREATE TABLE stock_daily (
    symbol String,           -- è‚¡ç¥¨ä»£ç 
    date Date,              -- äº¤æ˜“æ—¥æœŸ
    open Float64,           -- å¼€ç›˜ä»·
    high Float64,           -- æœ€é«˜ä»·
    low Float64,            -- æœ€ä½ä»·
    close Float64,          -- æ”¶ç›˜ä»·
    volume Float64,         -- æˆäº¤é‡ï¼ˆæ‰‹ï¼‰
    amount Float64,         -- æˆäº¤é¢ï¼ˆå…ƒï¼‰
    change_rate Float64,    -- æ¶¨è·Œå¹…
    amplitude Float64,      -- æŒ¯å¹…
    turnover_rate Float64,  -- æ¢æ‰‹ç‡
    pe_ratio Float64,       -- å¸‚ç›ˆç‡
    pb_ratio Float64,       -- å¸‚å‡€ç‡

    -- æŠ€æœ¯æŒ‡æ ‡
    ma5 Float64,            -- 5æ—¥å‡çº¿
    ma10 Float64,           -- 10æ—¥å‡çº¿
    ma20 Float64,           -- 20æ—¥å‡çº¿
    ma60 Float64,           -- 60æ—¥å‡çº¿
    ema12 Float64,          -- 12æ—¥æŒ‡æ•°å‡çº¿
    ema26 Float64,          -- 26æ—¥æŒ‡æ•°å‡çº¿
    macd Float64,           -- MACD
    macd_signal Float64,    -- MACDä¿¡å·çº¿
    macd_histogram Float64, -- MACDæŸ±çŠ¶å›¾
    rsi Float64,            -- RSIæŒ‡æ ‡

    -- æ•°æ®æºå’Œæ›´æ–°æ—¶é—´
    data_source String,     -- æ•°æ®æºæ ‡è¯†
    updated_at DateTime64(3) DEFAULT now64(),

    -- åˆ†åŒºå­—æ®µ
    year Month,
    market Enum8('SH' = 1, 'SZ' = 2)
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/stock_daily', '{replica}')
PARTITION BY (year, market)
ORDER BY (symbol, date)
SETTINGS index_granularity = 8192;
```

#### 2. åˆ†é’Ÿæ•°æ®è¡¨

```sql
-- åˆ›å»ºåˆ†é’Ÿçº¿æ•°æ®è¡¨
CREATE TABLE stock_minute (
    symbol String,
    datetime DateTime64(3),
    open Float64,
    high Float64,
    low Float64,
    close Float64,
    volume Float64,
    amount Float64,
    data_source String,
    updated_at DateTime64(3) DEFAULT now64(),

    -- åˆ†åŒºå­—æ®µ
    date Date,
    hour UInt8,
    market Enum8('SH' = 1, 'SZ' = 2)
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/stock_minute', '{replica}')
PARTITION BY (date, market)
ORDER BY (symbol, datetime)
SETTINGS index_granularity = 16384;
```

## ğŸš€ Rustæ‰¹é‡å­˜å‚¨å®ç°

### æ ¸å¿ƒå­˜å‚¨æ¨¡å—ç»“æ„

```rust
// rust/src/storage/mod.rs
pub mod clickhouse;
pub mod batch_writer;
pub mod connection_pool;
pub mod table_manager;
pub mod compression;

pub use clickhouse::{ClickHouseClient, ClickHouseConfig};
pub use batch_writer::{BatchWriter, BatchConfig, WriteMode};
pub use connection_pool::{ConnectionPool, PoolConfig};
pub use table_manager::{TableManager, TableSchema};
pub use compression::{Compressor, CompressionType};
```

### 1. ClickHouseå®¢æˆ·ç«¯å®ç°

```rust
// rust/src/storage/clickhouse.rs
use anyhow::{Context, Result};
use clickhouse_rs::{Client, Pool};
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tokio::time::timeout;
use tracing::{debug, error, info, warn};

#[derive(Debug, Clone)]
pub struct ClickHouseConfig {
    pub url: String,
    pub database: String,
    pub user: String,
    pub password: String,
    pub pool_size: u32,
    pub connection_timeout: Duration,
    pub query_timeout: Duration,
    pub compression: bool,
}

impl Default for ClickHouseConfig {
    fn default() -> Self {
        Self {
            url: "tcp://localhost:9000".to_string(),
            database: "pulse_trader".to_string(),
            user: "default".to_string(),
            password: String::new(),
            pool_size: 10,
            connection_timeout: Duration::from_secs(30),
            query_timeout: Duration::from_secs(300),
            compression: true,
        }
    }
}

pub struct ClickHouseClient {
    pool: Pool,
    config: ClickHouseConfig,
}

impl ClickHouseClient {
    pub async fn new(config: ClickHouseConfig) -> Result<Self> {
        let dsn = format!(
            "tcp://{}:{}@{}:{}?compression={}&connect_timeout={}&query_timeout={}",
            config.user,
            if config.password.is_empty() { "" } else { &config.password },
            config.url.trim_start_matches("tcp://"),
            9000,
            if config.compression { "lz4" } else { "none" },
            config.connection_timeout.as_secs(),
            config.query_timeout.as_secs()
        );

        let pool = Pool::new(dsn);

        // æµ‹è¯•è¿æ¥
        let client = pool.get_handle().await?;
        let _ = timeout(config.connection_timeout, client.ping()).await
            .context("Failed to ping ClickHouse server")?;

        info!("Connected to ClickHouse: {}", config.url);

        Ok(Self { pool, config })
    }

    pub async fn get_handle(&self) -> Result<clickhouse_rs::ClientHandle> {
        self.pool.get_handle().await
            .context("Failed to get ClickHouse connection from pool")
    }

    /// æ‰§è¡Œæ‰¹é‡æ’å…¥
    pub async fn insert_batch<T>(&self, table: &str, data: Vec<T>) -> Result<()>
    where
        T: Send + Sync + Serialize,
    {
        if data.is_empty() {
            return Ok(());
        }

        let start_time = std::time::Instant::now();
        let chunk_size = self.calculate_optimal_chunk_size::<T>();

        let mut total_inserted = 0;
        let mut errors = Vec::new();

        // åˆ†æ‰¹å¤„ç†æ•°æ®
        for chunk in data.chunks(chunk_size) {
            match self.insert_chunk(table, chunk).await {
                Ok(inserted) => {
                    total_inserted += inserted;
                    debug!("Inserted {} records into {}", inserted, table);
                }
                Err(e) => {
                    error!("Failed to insert chunk into {}: {}", table, e);
                    errors.push(e);
                }
            }
        }

        let duration = start_time.elapsed();
        let throughput = total_inserted as f64 / duration.as_secs_f64();

        info!(
            "Batch insert completed: {} records in {:.2}s ({:.0} records/s)",
            total_inserted, duration.as_secs_f64(), throughput
        );

        if !errors.is_empty() && total_inserted == 0 {
            return Err(anyhow::anyhow!("All batch insert attempts failed"));
        }

        Ok(())
    }

    async fn insert_chunk<T>(&self, table: &str, chunk: &[T]) -> Result<usize>
    where
        T: Send + Sync + Serialize,
    {
        let mut client = self.get_handle().await?;

        // ä½¿ç”¨äº‹åŠ¡ç¡®ä¿åŸå­æ€§
        let mut insert = client.insert(table)?;

        for item in chunk {
            insert.write(item)?;
        }

        insert.end().await?;

        Ok(chunk.len())
    }

    fn calculate_optimal_chunk_size<T>(&self) -> usize {
        // åŸºäºæ•°æ®ç±»å‹å¤§å°å’Œæ€§èƒ½è¦æ±‚åŠ¨æ€è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        const TARGET_BATCH_SIZE_BYTES: usize = 1024 * 1024; // 1MB
        const MIN_CHUNK_SIZE: usize = 1000;
        const MAX_CHUNK_SIZE: usize = 100000;

        let estimated_size = std::mem::size_of::<T>();
        let calculated_size = TARGET_BATCH_SIZE_BYTES / estimated_size.max(1);

        calculated_size.clamp(MIN_CHUNK_SIZE, MAX_CHUNK_SIZE)
    }

    /// åˆ›å»ºæ•°æ®åº“è¡¨
    pub async fn create_table(&self, schema: &TableSchema) -> Result<()> {
        let sql = schema.to_create_table_sql();

        info!("Creating table: {}", schema.name);
        debug!("Create table SQL:\n{}", sql);

        let mut client = self.get_handle().await?;
        client.execute(&sql).await?;

        info!("Table created successfully: {}", schema.name);
        Ok(())
    }

    /// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    pub async fn table_exists(&self, table_name: &str) -> Result<bool> {
        let sql = format!(
            "SELECT count() FROM system.tables WHERE database = '{}' AND name = '{}'",
            self.config.database, table_name
        );

        let mut client = self.get_handle().await?;
        let mut row_set = client.query(&sql)?.fetch_all()?;

        if let Some(row) = row_set.next() {
            let count: u64 = row.get(0)?;
            Ok(count > 0)
        } else {
            Ok(false)
        }
    }

    /// è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_table_stats(&self, table_name: &str) -> Result<TableStats> {
        let sql = format!(
            "SELECT
                count() as total_rows,
                min(date) as min_date,
                max(date) as max_date,
                uniq(symbol) as unique_symbols,
                sum(size_bytes) / 1024 / 1024 as size_mb
            FROM {}",
            table_name
        );

        let mut client = self.get_handle().await?;
        let mut row_set = client.query(&sql)?.fetch_all()?;

        if let Some(row) = row_set.next() {
            Ok(TableStats {
                total_rows: row.get(0)?,
                min_date: row.get(1)?,
                max_date: row.get(2)?,
                unique_symbols: row.get(3)?,
                size_mb: row.get(4)?,
            })
        } else {
            Err(anyhow::anyhow!("No stats returned for table: {}", table_name))
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TableStats {
    pub total_rows: u64,
    pub min_date: Option<String>,
    pub max_date: Option<String>,
    pub unique_symbols: u64,
    pub size_mb: f64,
}
```

### 2. é«˜æ€§èƒ½æ‰¹é‡å†™å…¥å™¨

```rust
// rust/src/storage/batch_writer.rs
use anyhow::{Context, Result};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, Semaphore};
use tracing::{debug, error, info, warn};

use crate::parsers::tdx_day::TDXDayRecord;
use crate::storage::clickhouse::ClickHouseClient;
use crate::processors::DataCleaner;

#[derive(Debug, Clone)]
pub struct BatchConfig {
    pub max_batch_size: usize,
    pub max_wait_time: Duration,
    pub max_buffer_size: usize,
    pub compression_enabled: bool,
    pub retry_attempts: u32,
    pub retry_delay: Duration,
    pub parallel_writers: usize,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 50000,
            max_wait_time: Duration::from_secs(5),
            max_buffer_size: 500000,
            compression_enabled: true,
            retry_attempts: 3,
            retry_delay: Duration::from_secs(1),
            parallel_writers: 4,
        }
    }
}

#[derive(Debug)]
pub enum WriteMode {
    Async,    // å¼‚æ­¥æ‰¹é‡å†™å…¥
    Sync,     // åŒæ­¥å†™å…¥
    Streaming, // æµå¼å†™å…¥
}

pub struct BatchWriter {
    client: Arc<ClickHouseClient>,
    config: BatchConfig,
    data_cleaner: Arc<DataCleaner>,

    // å¼‚æ­¥å†™å…¥ç›¸å…³
    buffer: Arc<tokio::sync::Mutex<Vec<StockDataRecord>>>,
    buffer_sender: mpsc::UnboundedSender<StockDataRecord>,
    buffer_receiver: Arc<tokio::sync::Mutex<mpsc::UnboundedReceiver<StockDataRecord>>>,

    // å¹¶å‘æ§åˆ¶
    write_semaphore: Arc<Semaphore>,
    active_writes: Arc<tokio::sync::AtomicU64>,

    // æ€§èƒ½ç»Ÿè®¡
    stats: Arc<tokio::sync::Mutex<WriterStats>>,
}

#[derive(Debug, Clone, Serialize)]
pub struct StockDataRecord {
    pub symbol: String,
    pub date: String,
    pub open: f64,
    pub high: f64,
    pub low: f64,
    pub close: f64,
    pub volume: f64,
    pub amount: f64,
    pub change_rate: Option<f64>,
    pub amplitude: Option<f64>,
    pub turnover_rate: Option<f64>,
    pub data_source: String,
}

impl From<TDXDayRecord> for StockDataRecord {
    fn from(tdx_record: TDXDayRecord) -> Self {
        let change_rate = if tdx_record.open > 0.0 {
            Some((tdx_record.close - tdx_record.open) / tdx_record.open * 100.0)
        } else {
            None
        };

        let amplitude = if tdx_record.open > 0.0 {
            Some((tdx_record.high - tdx_record.low) / tdx_record.open * 100.0)
        } else {
            None
        };

        Self {
            symbol: tdx_record.symbol,
            date: tdx_record.date,
            open: tdx_record.open,
            high: tdx_record.high,
            low: tdx_record.low,
            close: tdx_record.close,
            volume: tdx_record.volume,
            amount: tdx_record.amount,
            change_rate,
            amplitude,
            turnover_rate: None, // éœ€è¦é¢å¤–è®¡ç®—
            data_source: "TDX".to_string(),
        }
    }
}

#[derive(Debug, Default)]
pub struct WriterStats {
    pub total_records_written: u64,
    pub total_batches_written: u64,
    pub failed_writes: u64,
    pub total_bytes_written: u64,
    pub avg_write_time: Duration,
    pub max_write_time: Duration,
    pub buffer_overflows: u64,
    pub last_write_time: Option<Instant>,
}

impl BatchWriter {
    pub fn new(
        client: Arc<ClickHouseClient>,
        config: BatchConfig,
    ) -> Result<Self> {
        let data_cleaner = Arc::new(DataCleaner::new_default());

        let (buffer_sender, buffer_receiver) = mpsc::unbounded_channel();
        let buffer = Arc::new(tokio::sync::Mutex::new(Vec::with_capacity(config.max_buffer_size)));

        Ok(Self {
            client,
            config,
            data_cleaner,
            buffer,
            buffer_sender,
            buffer_receiver: Arc::new(tokio::sync::Mutex::new(buffer_receiver)),
            write_semaphore: Arc::new(Semaphore::new(config.parallel_writers)),
            active_writes: Arc::new(tokio::sync::AtomicU64::new(0)),
            stats: Arc::new(tokio::sync::Mutex::new(WriterStats::default())),
        })
    }

    /// å¼‚æ­¥å†™å…¥æ•°æ®
    pub async fn write_async(&self, records: Vec<StockDataRecord>) -> Result<()> {
        let start_time = Instant::now();

        // æ•°æ®æ¸…æ´—
        let cleaned_records = self.data_cleaner.clean_batch(records)
            .context("Failed to clean data before writing")?;

        // å‘é€åˆ°ç¼“å†²åŒº
        for record in cleaned_records {
            if let Err(_) = self.buffer_sender.send(record) {
                // ç¼“å†²åŒºå·²æ»¡ï¼Œè§¦å‘ç«‹å³å†™å…¥
                warn!("Buffer overflow, triggering immediate write");
                self.flush_buffer().await?;

                // é‡è¯•å‘é€
                if let Err(e) = self.buffer_sender.send(record) {
                    error!("Failed to send record to buffer: {}", e);
                    return Err(anyhow::anyhow!("Buffer overflow and retry failed"));
                }
            }
        }

        let duration = start_time.elapsed();
        debug!("Queued {} records for async write in {:?}", cleaned_records.len(), duration);

        Ok(())
    }

    /// åŒæ­¥å†™å…¥æ•°æ®
    pub async fn write_sync(&self, records: Vec<StockDataRecord>) -> Result<()> {
        let start_time = Instant::now();

        // æ•°æ®æ¸…æ´—
        let cleaned_records = self.data_cleaner.clean_batch(records)
            .context("Failed to clean data before writing")?;

        // åˆ†æ‰¹å†™å…¥
        let chunk_size = self.config.max_batch_size;
        let mut total_written = 0;

        for chunk in cleaned_records.chunks(chunk_size) {
            self.write_chunk_with_retry(chunk.to_vec()).await?;
            total_written += chunk.len();
        }

        let duration = start_time.elapsed();
        let throughput = total_written as f64 / duration.as_secs_f64();

        info!(
            "Synchronous write completed: {} records in {:.2}s ({:.0} records/s)",
            total_written, duration.as_secs_f64(), throughput
        );

        Ok(())
    }

    /// æµå¼å†™å…¥ï¼ˆè¿ç»­æ•°æ®æµï¼‰
    pub async fn start_streaming_writer(&self) -> Result<mpsc::UnboundedSender<StockDataRecord>> {
        let writer_config = self.config.clone();
        let client = Arc::clone(&self.client);
        let data_cleaner = Arc::clone(&self.data_cleaner);
        let stats = Arc::clone(&self.stats);

        let (stream_sender, mut stream_receiver) = mpsc::unbounded_channel::<StockDataRecord>();

        // å¯åŠ¨æµå¼å†™å…¥ä»»åŠ¡
        tokio::spawn(async move {
            let mut buffer = Vec::with_capacity(writer_config.max_batch_size);
            let mut last_flush = Instant::now();

            while let Some(record) = stream_receiver.recv().await {
                buffer.push(record);

                // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
                let should_flush = buffer.len() >= writer_config.max_batch_size ||
                    last_flush.elapsed() >= writer_config.max_wait_time;

                if should_flush {
                    if !buffer.is_empty() {
                        // æ•°æ®æ¸…æ´—
                        if let Ok(cleaned_buffer) = data_cleaner.clean_batch(buffer.clone()) {
                            // å†™å…¥æ•°æ®
                            if let Err(e) = Self::write_to_clickhouse(
                                &client,
                                &cleaned_buffer,
                                &writer_config,
                                &stats
                            ).await {
                                error!("Streaming write failed: {}", e);
                            }
                        }

                        buffer.clear();
                        last_flush = Instant::now();
                    }
                }
            }

            // å†™å…¥å‰©ä½™æ•°æ®
            if !buffer.is_empty() {
                if let Ok(cleaned_buffer) = data_cleaner.clean_batch(buffer.clone()) {
                    let _ = Self::write_to_clickhouse(
                        &client,
                        &cleaned_buffer,
                        &writer_config,
                        &stats
                    ).await;
                }
            }
        });

        Ok(stream_sender)
    }

    async fn write_chunk_with_retry(&self, chunk: Vec<StockDataRecord>) -> Result<()> {
        let mut attempts = 0;
        let mut last_error = None;

        while attempts < self.config.retry_attempts {
            match self.write_chunk(chunk.clone()).await {
                Ok(_) => break,
                Err(e) => {
                    attempts += 1;
                    last_error = Some(e.clone());

                    if attempts < self.config.retry_attempts {
                        warn!("Write attempt {} failed, retrying in {:?}: {}", attempts, self.config.retry_delay, e);
                        tokio::time::sleep(self.config.retry_delay).await;
                    }
                }
            }
        }

        if let Some(error) = last_error {
            if attempts >= self.config.retry_attempts {
                error!("All write attempts failed: {}", error);
                return Err(error);
            }
        }

        Ok(())
    }

    async fn write_chunk(&self, chunk: Vec<StockDataRecord>) -> Result<()> {
        let start_time = Instant::now();

        let permit = self.write_semaphore.acquire().await?;
        self.active_writes.fetch_add(1, std::sync::atomic::Ordering::Relaxed);

        let result = self.client.insert_batch("stock_daily", chunk.clone()).await;

        drop(permit);
        self.active_writes.fetch_sub(1, std::sync::atomic::Ordering::Relaxed);

        let duration = start_time.elapsed();

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        {
            let mut stats = self.stats.lock().await;
            stats.total_batches_written += 1;
            stats.total_records_written += chunk.len() as u64;
            stats.avg_write_time = (stats.avg_write_time + duration) / 2;
            stats.max_write_time = stats.max_write_time.max(duration);
            stats.last_write_time = Some(Instant::now());
        }

        match result {
            Ok(_) => {
                debug!("Successfully wrote {} records in {:?}", chunk.len(), duration);
                Ok(())
            }
            Err(e) => {
                error!("Failed to write {} records: {}", chunk.len(), e);
                Err(e)
            }
        }
    }

    async fn write_to_clickhouse(
        client: &ClickHouseClient,
        records: &[StockDataRecord],
        config: &BatchConfig,
        stats: &Arc<tokio::sync::Mutex<WriterStats>>,
    ) -> Result<()> {
        let start_time = Instant::now();

        let result = client.insert_batch("stock_daily", records.to_vec()).await;
        let duration = start_time.elapsed();

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        {
            let mut s = stats.lock().await;
            s.total_batches_written += 1;
            s.total_records_written += records.len() as u64;
            s.avg_write_time = (s.avg_write_time + duration) / 2;
            s.max_write_time = s.max_write_time.max(duration);
        }

        result
    }

    /// ç«‹å³åˆ·æ–°ç¼“å†²åŒº
    pub async fn flush_buffer(&self) -> Result<()> {
        let mut buffer = self.buffer.lock().await;

        if !buffer.is_empty() {
            let chunk = buffer.drain(..).collect::<Vec<_>>();
            drop(buffer); // é‡Šæ”¾é”

            self.write_chunk_with_retry(chunk).await?;
        }

        Ok(())
    }

    /// è·å–å†™å…¥å™¨ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_stats(&self) -> WriterStats {
        self.stats.lock().await.clone()
    }

    /// åœæ­¢å†™å…¥å™¨å¹¶æ¸…ç†èµ„æº
    pub async fn shutdown(&self) -> Result<()> {
        info!("Shutting down batch writer...");

        // åˆ·æ–°å‰©ä½™æ•°æ®
        self.flush_buffer().await?;

        // ç­‰å¾…æ‰€æœ‰å†™å…¥æ“ä½œå®Œæˆ
        while self.active_writes.load(std::sync::atomic::Ordering::Relaxed) > 0 {
            tokio::time::sleep(Duration::from_millis(100)).await;
        }

        info!("Batch writer shutdown complete");
        Ok(())
    }
}

// å®ç°Drop traitç¡®ä¿èµ„æºæ¸…ç†
impl Drop for BatchWriter {
    fn drop(&mut self) {
        // æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨asyncï¼Œéœ€è¦ä½¿ç”¨é˜»å¡æ–¹å¼
        warn!("BatchWriter dropped without explicit shutdown");
    }
}
```

### 3. è¡¨ç®¡ç†å™¨

```rust
// rust/src/storage/table_manager.rs
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

use crate::storage::clickhouse::ClickHouseClient;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TableSchema {
    pub name: String,
    pub columns: Vec<ColumnDefinition>,
    pub engine: String,
    pub partition_by: Option<String>,
    pub order_by: Vec<String>,
    pub primary_key: Option<Vec<String>>,
    pub settings: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ColumnDefinition {
    pub name: String,
    pub data_type: String,
    pub default: Option<String>,
    pub comment: Option<String>,
    pub codec: Option<String>,
}

impl TableSchema {
    pub fn to_create_table_sql(&self) -> String {
        let mut sql = format!("CREATE TABLE IF NOT EXISTS {} (\n", self.name);

        // æ·»åŠ åˆ—å®šä¹‰
        let columns_sql: Vec<String> = self.columns
            .iter()
            .map(|col| {
                let mut col_sql = format!("    {} {}", col.name, col.data_type);

                if let Some(default) = &col.default {
                    col_sql.push_str(&format!(" DEFAULT {}", default));
                }

                if let Some(comment) = &col.comment {
                    col_sql.push_str(&format!(" COMMENT '{}'", comment));
                }

                if let Some(codec) = &col.codec {
                    col_sql.push_str(&format!(" CODEC({})", codec));
                }

                col_sql
            })
            .collect();

        sql.push_str(&columns_sql.join(",\n"));
        sql.push_str("\n)");

        // æ·»åŠ å¼•æ“
        sql.push_str(&format!("\nENGINE = {}", self.engine));

        // æ·»åŠ åˆ†åŒºé”®
        if let Some(partition_by) = &self.partition_by {
            sql.push_str(&format!("\nPARTITION BY {}", partition_by));
        }

        // æ·»åŠ æ’åºé”®
        if !self.order_by.is_empty() {
            sql.push_str(&format!("\nORDER BY ({})", self.order_by.join(", ")));
        }

        // æ·»åŠ ä¸»é”®
        if let Some(primary_key) = &self.primary_key {
            sql.push_str(&format!("\nPRIMARY KEY ({})", primary_key.join(", ")));
        }

        // æ·»åŠ è®¾ç½®
        if !self.settings.is_empty() {
            let settings_sql: Vec<String> = self.settings
                .iter()
                .map(|(k, v)| format!("{} = {}", k, v))
                .collect();
            sql.push_str(&format!("\nSETTINGS {}", settings_sql.join(", ")));
        }

        sql
    }
}

pub struct TableManager {
    client: ClickHouseClient,
}

impl TableManager {
    pub fn new(client: ClickHouseClient) -> Self {
        Self { client }
    }

    /// è·å–æ—¥çº¿æ•°æ®è¡¨ç»“æ„
    pub fn get_daily_table_schema() -> TableSchema {
        let mut settings = HashMap::new();
        settings.insert("index_granularity".to_string(), "8192".to_string());

        TableSchema {
            name: "stock_daily".to_string(),
            columns: vec![
                ColumnDefinition {
                    name: "symbol".to_string(),
                    data_type: "String".to_string(),
                    default: None,
                    comment: Some("è‚¡ç¥¨ä»£ç ".to_string()),
                    codec: Some("ZSTD".to_string()),
                },
                ColumnDefinition {
                    name: "date".to_string(),
                    data_type: "Date".to_string(),
                    default: None,
                    comment: Some("äº¤æ˜“æ—¥æœŸ".to_string()),
                    codec: Some("ZSTD".to_string()),
                },
                ColumnDefinition {
                    name: "open".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("å¼€ç›˜ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "high".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æœ€é«˜ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "low".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æœ€ä½ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "close".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æ”¶ç›˜ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "volume".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æˆäº¤é‡ï¼ˆæ‰‹ï¼‰".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "amount".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æˆäº¤é¢ï¼ˆå…ƒï¼‰".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "change_rate".to_string(),
                    data_type: "Float64".to_string(),
                    default: Some("NULL".to_string()),
                    comment: Some("æ¶¨è·Œå¹…".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "amplitude".to_string(),
                    data_type: "Float64".to_string(),
                    default: Some("NULL".to_string()),
                    comment: Some("æŒ¯å¹…".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "data_source".to_string(),
                    data_type: "String".to_string(),
                    default: Some("'TDX'".to_string()),
                    comment: Some("æ•°æ®æº".to_string()),
                    codec: Some("ZSTD".to_string()),
                },
                ColumnDefinition {
                    name: "updated_at".to_string(),
                    data_type: "DateTime64(3)".to_string(),
                    default: Some("now64()".to_string()),
                    comment: Some("æ›´æ–°æ—¶é—´".to_string()),
                    codec: None,
                },
                ColumnDefinition {
                    name: "year".to_string(),
                    data_type: "Month".to_string(),
                    default: None,
                    comment: Some("å¹´æœˆåˆ†åŒºå­—æ®µ".to_string()),
                    codec: None,
                },
                ColumnDefinition {
                    name: "market".to_string(),
                    data_type: "Enum8('SH' = 1, 'SZ' = 2)".to_string(),
                    default: None,
                    comment: Some("å¸‚åœºæ ‡è¯†".to_string()),
                    codec: None,
                },
            ],
            engine: "MergeTree".to_string(),
            partition_by: Some("year".to_string()),
            order_by: vec!["symbol".to_string(), "date".to_string()],
            primary_key: Some(vec!["symbol".to_string(), "date".to_string()]),
            settings,
        }
    }

    /// åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¡¨
    pub async fn create_all_tables(&self) -> Result<()> {
        let schemas = vec![
            Self::get_daily_table_schema(),
            Self::get_minute_table_schema(),
        ];

        for schema in schemas {
            if !self.client.table_exists(&schema.name).await? {
                self.client.create_table(&schema).await
                    .with_context(|| format!("Failed to create table: {}", schema.name))?;

                info!("Created table: {}", schema.name);
            } else {
                info!("Table already exists: {}", schema.name);
            }
        }

        Ok(())
    }

    /// è·å–åˆ†é’Ÿæ•°æ®è¡¨ç»“æ„
    pub fn get_minute_table_schema() -> TableSchema {
        let mut settings = HashMap::new();
        settings.insert("index_granularity".to_string(), "16384".to_string());

        TableSchema {
            name: "stock_minute".to_string(),
            columns: vec![
                ColumnDefinition {
                    name: "symbol".to_string(),
                    data_type: "String".to_string(),
                    default: None,
                    comment: Some("è‚¡ç¥¨ä»£ç ".to_string()),
                    codec: Some("ZSTD".to_string()),
                },
                ColumnDefinition {
                    name: "datetime".to_string(),
                    data_type: "DateTime64(3)".to_string(),
                    default: None,
                    comment: Some("æ—¶é—´æˆ³".to_string()),
                    codec: Some("ZSTD".to_string()),
                },
                ColumnDefinition {
                    name: "open".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("å¼€ç›˜ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "high".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æœ€é«˜ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "low".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æœ€ä½ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "close".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æ”¶ç›˜ä»·".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "volume".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æˆäº¤é‡".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "amount".to_string(),
                    data_type: "Float64".to_string(),
                    default: None,
                    comment: Some("æˆäº¤é¢".to_string()),
                    codec: Some("ZSTD(1)".to_string()),
                },
                ColumnDefinition {
                    name: "data_source".to_string(),
                    data_type: "String".to_string(),
                    default: Some("'TDX'".to_string()),
                    comment: Some("æ•°æ®æº".to_string()),
                    codec: Some("ZSTD".to_string()),
                },
            ],
            engine: "MergeTree".to_string(),
            partition_by: Some("toYYYYMM(datetime)".to_string()),
            order_by: vec!["symbol".to_string(), "datetime".to_string()],
            primary_key: Some(vec!["symbol".to_string(), "datetime".to_string()),
            settings,
        }
    }

    /// ä¼˜åŒ–è¡¨æ€§èƒ½
    pub async fn optimize_table(&self, table_name: &str) -> Result<()> {
        let sql = format!("OPTIMIZE TABLE {} FINAL", table_name);

        let mut client = self.client.get_handle().await?;
        client.execute(&sql).await?;

        info!("Optimized table: {}", table_name);
        Ok(())
    }

    /// æ£€æŸ¥è¡¨åˆ†åŒº
    pub async fn check_partitions(&self, table_name: &str) -> Result<Vec<PartitionInfo>> {
        let sql = format!(
            "SELECT
                partition,
                count() as rows,
                sum(data_bytes) / 1024 / 1024 as size_mb,
                min(date) as min_date,
                max(date) as max_date
            FROM system.parts
            WHERE table = '{}' AND active = 1
            GROUP BY partition
            ORDER BY partition",
            table_name
        );

        let mut client = self.client.get_handle().await?;
        let mut row_set = client.query(&sql)?.fetch_all()?;

        let mut partitions = Vec::new();

        while let Some(row) = row_set.next() {
            partitions.push(PartitionInfo {
                name: row.get(0)?,
                rows: row.get(1)?,
                size_mb: row.get(2)?,
                min_date: row.get(3)?,
                max_date: row.get(4)?,
            });
        }

        Ok(partitions)
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PartitionInfo {
    pub name: String,
    pub rows: u64,
    pub size_mb: f64,
    pub min_date: Option<String>,
    pub max_date: Option<String>,
}
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### åŸºå‡†æµ‹è¯•å¥—ä»¶

```rust
// rust/src/benches/clickhouse_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use pulse_trader_rust::{
    storage::{ClickHouseClient, BatchWriter, BatchConfig},
    parsers::tdx_day::TDXDayParser,
};
use std::time::Duration;
use tempfile::TempDir;

async fn setup_test_database() -> (ClickHouseClient, TempDir) {
    // è®¾ç½®æµ‹è¯•æ•°æ®åº“ï¼ˆä½¿ç”¨å†…å­˜æ¨¡å¼æˆ–ä¸´æ—¶ç›®å½•ï¼‰
    let config = ClickHouseConfig {
        url: "tcp://localhost:9000".to_string(),
        database: "pulse_trader_test".to_string(),
        ..Default::default()
    };

    let client = ClickHouseClient::new(config).await.unwrap();

    // åˆ›å»ºæµ‹è¯•è¡¨
    // ...ï¼ˆè¡¨åˆ›å»ºä»£ç ï¼‰

    (client, TempDir::new().unwrap())
}

fn bench_batch_writes(c: &mut Criterion) {
    let rt = tokio::runtime::Runtime::new().unwrap();

    c.bench_function("batch_write_1k", |b| {
        b.to_async(&rt).iter(|| async {
            let (client, _temp_dir) = setup_test_database().await;
            let config = BatchConfig {
                max_batch_size: 1000,
                ..Default::default()
            };
            let writer = BatchWriter::new(std::sync::Arc::new(client), config).unwrap();

            // ç”Ÿæˆæµ‹è¯•æ•°æ®
            let test_data = generate_test_records(1000);

            let start = std::time::Instant::now();
            writer.write_async(test_data).await.unwrap();
            start.elapsed()
        });
    });

    c.bench_function("batch_write_10k", |b| {
        b.to_async(&rt).iter(|| async {
            let (client, _temp_dir) = setup_test_database().await;
            let config = BatchConfig {
                max_batch_size: 10000,
                ..Default::default()
            };
            let writer = BatchWriter::new(std::sync::Arc::new(client), config).unwrap();

            // ç”Ÿæˆæµ‹è¯•æ•°æ®
            let test_data = generate_test_records(10000);

            let start = std::time::Instant::now();
            writer.write_async(test_data).await.unwrap();
            start.elapsed()
        });
    });
}

fn generate_test_records(count: usize) -> Vec<StockDataRecord> {
    (0..count).map(|i| StockDataRecord {
        symbol: format!("600{:04}", i % 1000),
        date: "2024-01-01".to_string(),
        open: 10.0 + (i as f64 * 0.01),
        high: 10.5 + (i as f64 * 0.01),
        low: 9.8 + (i as f64 * 0.01),
        close: 10.2 + (i as f64 * 0.01),
        volume: 1000000.0 + (i as f64 * 100.0),
        amount: 10200000.0 + (i as f64 * 1000.0),
        change_rate: Some(2.0),
        amplitude: Some(7.0),
        turnover_rate: Some(5.0),
        data_source: "TEST".to_string(),
    }).collect()
}

criterion_group!(
    benches,
    bench_batch_writes,
);
criterion_main!(benches);
```

## ğŸ› Pythoné›†æˆæµ‹è¯•

### å®Œæ•´çš„é›†æˆæµ‹è¯•

```python
# tests/test_clickhouse_storage.py
import pytest
import asyncio
import pandas as pd
from datetime import datetime, date
from pulse_trader_rust import (
    ClickHouseConfig, ClickHouseClient,
    BatchConfig, BatchWriter,
    TableManager, TableSchema
)

@pytest.fixture
async def clickhouse_client():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„ClickHouseå®¢æˆ·ç«¯"""
    config = ClickHouseConfig(
        url="tcp://localhost:9000",
        database="pulse_trader_test",
        user="default",
        password=""
    )

    client = ClickHouseClient(config)
    await client.connect()

    # åˆ›å»ºæµ‹è¯•è¡¨
    table_manager = TableManager(client)
    await table_manager.create_all_tables()

    yield client

    # æ¸…ç†
    await client.execute("DROP DATABASE IF EXISTS pulse_trader_test")

@pytest.fixture
def batch_config():
    """æ‰¹é‡å†™å…¥é…ç½®"""
    return BatchConfig(
        max_batch_size=1000,
        max_wait_time=5.0,
        max_buffer_size=10000,
        compression_enabled=True,
        retry_attempts=3,
        parallel_writers=2
    )

class TestClickHouseStorage:
    """ClickHouseå­˜å‚¨æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_client_connection(self, clickhouse_client):
        """æµ‹è¯•å®¢æˆ·ç«¯è¿æ¥"""
        assert await clickhouse_client.ping()
        assert await clickhouse_client.is_connected()

    @pytest.mark.asyncio
    async def test_table_creation(self, clickhouse_client):
        """æµ‹è¯•è¡¨åˆ›å»º"""
        table_manager = TableManager(clickhouse_client)

        # è·å–æ—¥çº¿è¡¨ç»“æ„
        daily_schema = TableManager.get_daily_table_schema()

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        assert not await clickhouse_client.table_exists("test_daily")

        # åˆ›å»ºè¡¨
        await clickhouse_client.create_table(daily_schema)

        # éªŒè¯è¡¨å­˜åœ¨
        assert await clickhouse_client.table_exists("test_daily")

    @pytest.mark.asyncio
    async def test_batch_writer_basic(self, clickhouse_client, batch_config):
        """æµ‹è¯•åŸºç¡€æ‰¹é‡å†™å…¥"""
        writer = BatchWriter(clickhouse_client, batch_config)

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            {
                'symbol': '600001',
                'date': '2024-01-01',
                'open': 10.0,
                'high': 10.5,
                'low': 9.8,
                'close': 10.2,
                'volume': 1000000.0,
                'amount': 10200000.0,
                'change_rate': 2.0,
                'amplitude': 7.0,
                'data_source': 'TEST'
            }
        ]

        # å†™å…¥æ•°æ®
        await writer.write_sync(test_data)

        # éªŒè¯æ•°æ®
        result = await clickhouse_client.query(
            "SELECT count() FROM stock_daily WHERE symbol = '600001'"
        )
        assert result[0]['count()'] == 1

    @pytest.mark.asyncio
    async def test_high_volume_insert(self, clickhouse_client, batch_config):
        """æµ‹è¯•å¤§é‡æ•°æ®æ’å…¥"""
        writer = BatchWriter(clickhouse_client, batch_config)

        # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
        symbols = ['600{:04}'.format(i) for i in range(1, 101)]
        dates = pd.date_range('2024-01-01', periods=250)

        test_data = []
        for symbol in symbols:
            for date in dates:
                test_data.append({
                    'symbol': symbol,
                    'date': date.strftime('%Y-%m-%d'),
                    'open': 10.0 + hash(symbol + date.strftime('%Y-%m-%d')) % 100 / 10,
                    'high': 11.0 + hash(symbol + date.strftime('%Y-%m-%d')) % 100 / 10,
                    'low': 9.0 + hash(symbol + date.strftime('%Y-%m-%d')) % 100 / 10,
                    'close': 10.5 + hash(symbol + date.strftime('%Y-%m-%d')) % 100 / 10,
                    'volume': 1000000.0 + hash(symbol + date.strftime('%Y-%m-%d')) % 1000000,
                    'amount': 10000000.0 + hash(symbol + date.strftime('%Y-%m-%d')) % 10000000,
                    'change_rate': (hash(symbol + date.strftime('%Y-%m-%d')) % 200 - 100) / 10,
                    'amplitude': (hash(symbol + date.strftime('%Y-%m-%d')) % 100) / 10,
                    'data_source': 'TEST'
                })

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()

        # å†™å…¥æ•°æ®
        await writer.write_async(test_data)

        # è®¡ç®—æ€§èƒ½
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        throughput = len(test_data) / duration

        print(f"Inserted {len(test_data)} records in {duration:.2f}s ({throughput:.0f} records/s)")

        # éªŒè¯æ•°æ®
        result = await clickhouse_client.query("SELECT count() FROM stock_daily")
        assert result[0]['count()'] == len(test_data)

        # éªŒè¯æ€§èƒ½åŸºå‡†
        assert throughput > 10000, f"Throughput too low: {throughput:.0f} records/s"

    @pytest.mark.asyncio
    async def test_streaming_writer(self, clickhouse_client, batch_config):
        """æµ‹è¯•æµå¼å†™å…¥"""
        writer = BatchWriter(clickhouse_client, batch_config)

        # å¯åŠ¨æµå¼å†™å…¥
        stream_sender = await writer.start_streaming_writer()

        # æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµ
        import time
        start_time = time.time()

        for i in range(10000):
            await stream_sender.send({
                'symbol': f'600{i:04}',
                'date': '2024-01-01',
                'open': 10.0 + i * 0.01,
                'high': 10.5 + i * 0.01,
                'low': 9.8 + i * 0.01,
                'close': 10.2 + i * 0.01,
                'volume': 1000000.0 + i * 100.0,
                'amount': 10200000.0 + i * 1000.0,
                'data_source': 'STREAM'
            })

            if i % 1000 == 0:
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå®æ—¶é—´éš”

        # å…³é—­æµå¼å†™å…¥
        stream_sender.close()

        end_time = time.time()
        duration = end_time - start_time

        print(f"Streaming write completed in {duration:.2f}s")

        # éªŒè¯æ•°æ®
        result = await clickhouse_client.query(
            "SELECT count() FROM stock_daily WHERE data_source = 'STREAM'"
        )
        assert result[0]['count()'] == 10000

    @pytest.mark.asyncio
    async def test_error_handling(self, clickhouse_client, batch_config):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        writer = BatchWriter(clickhouse_client, batch_config)

        # æµ‹è¯•æ— æ•ˆæ•°æ®
        invalid_data = [
            {
                'symbol': '',  # ç©ºä»£ç 
                'date': 'invalid-date',  # æ— æ•ˆæ—¥æœŸ
                'open': 'not-a-number',  # æ— æ•ˆä»·æ ¼
                'high': 10.0,
                'low': 9.0,
                'close': 10.0,
                'volume': -1000,  # è´Ÿæ•°æˆäº¤é‡
                'amount': 10000,
                'data_source': 'INVALID'
            }
        ]

        # åº”è¯¥èƒ½å¤Ÿå¤„ç†æ— æ•ˆæ•°æ®è€Œä¸å´©æºƒ
        try:
            await writer.write_sync(invalid_data)
        except Exception as e:
            print(f"Expected error caught: {e}")

        # éªŒè¯æ•°æ®åº“çŠ¶æ€
        result = await clickhouse_client.query("SELECT count() FROM stock_daily")
        # ä¸åº”è¯¥æœ‰ä»»ä½•æ— æ•ˆæ•°æ®è¢«å†™å…¥
        # å…·ä½“å–å†³äºæ•°æ®æ¸…æ´—é€»è¾‘

@pytest.mark.asyncio
async def test_performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    config = ClickHouseConfig(
        url="tcp://localhost:9000",
        database="pulse_trader_bench",
        user="default"
    )

    client = ClickHouseClient(config)
    await client.connect()

    # åˆ›å»ºæµ‹è¯•è¡¨
    table_manager = TableManager(client)
    await table_manager.create_all_tables()

    # ä¸åŒæ‰¹é‡å¤§å°çš„æ€§èƒ½æµ‹è¯•
    batch_sizes = [1000, 5000, 10000, 50000]

    for batch_size in batch_sizes:
        print(f"\nTesting batch size: {batch_size}")

        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = []
        for i in range(batch_size):
            test_data.append({
                'symbol': f'600{(i % 1000):04}',
                'date': '2024-01-01',
                'open': 10.0 + i * 0.001,
                'high': 10.5 + i * 0.001,
                'low': 9.8 + i * 0.001,
                'close': 10.2 + i * 0.001,
                'volume': 1000000.0 + i * 100.0,
                'amount': 10200000.0 + i * 1000.0,
                'data_source': 'BENCH'
            })

        # æ€§èƒ½æµ‹è¯•
        batch_config = BatchConfig(max_batch_size=batch_size)
        writer = BatchWriter(client, batch_config)

        start_time = datetime.now()
        await writer.write_sync(test_data)
        end_time = datetime.now()

        duration = (end_time - start_time).total_seconds()
        throughput = batch_size / duration

        print(f"  Duration: {duration:.3f}s")
        print(f"  Throughput: {throughput:.0f} records/s")

        # æ¸…ç†æ•°æ®
        await client.execute("TRUNCATE TABLE stock_daily")

    # æ¸…ç†
    await client.execute("DROP DATABASE IF EXISTS pulse_trader_bench")

if __name__ == "__main__":
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    asyncio.run(test_performance_benchmark())
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æŒ‡å—

### 1. å‚æ•°è°ƒä¼˜

```rust
// rust/src/storage/optimization.rs
pub struct PerformanceTuner {
    client: ClickHouseClient,
}

impl PerformanceTuner {
    /// è‡ªåŠ¨ä¼˜åŒ–ClickHouseé…ç½®
    pub async fn auto_tune(&self) -> Result<()> {
        // è·å–å½“å‰é…ç½®
        let current_config = self.get_current_config().await?;

        // åŸºäºç³»ç»Ÿèµ„æºè‡ªåŠ¨è°ƒä¼˜
        let cpu_count = num_cpus::get();
        let memory_gb = self.get_total_memory_gb() as u64;

        let mut optimized_config = current_config.clone();

        // å¹¶è¡Œå†™å…¥ä¼˜åŒ–
        optimized_config.max_threads = (cpu_count * 2).min(32);

        // å†…å­˜ä¼˜åŒ–
        optimized_config.max_memory_usage = memory_gb * 1024 * 1024 * 1024 / 2; // ä½¿ç”¨50%å†…å­˜
        optimized_config.max_bytes_before_external_group_by =
            (memory_gb / 4) * 1024 * 1024 * 1024; // 25%å†…å­˜ç”¨äºGROUP BY

        // ç¼“å†²åŒºä¼˜åŒ–
        optimized_config.max_insert_block_size = match memory_gb {
            0..=8 => 100000,
            9..=16 => 500000,
            17..=32 => 1000000,
            _ => 2000000,
        };

        // å‹ç¼©ä¼˜åŒ–
        optimized_config.enable_compression = true;
        optimized_config.compression_level = 3; // å¹³è¡¡é€Ÿåº¦å’Œå‹ç¼©ç‡

        self.apply_config(&optimized_config).await?;

        info!("Performance tuning completed");
        Ok(())
    }

    /// åˆ›å»ºä¼˜åŒ–çš„è¡¨ç´¢å¼•
    pub async fn create_optimized_indexes(&self, table_name: &str) -> Result<()> {
        let queries = vec![
            format!("ALTER TABLE {} ADD INDEX IF NOT EXISTS idx_symbol_date (symbol, date) TYPE minmax GRANULARITY 1", table_name),
            format!("ALTER TABLE {} ADD INDEX IF NOT EXISTS idx_date_symbol (date, symbol) TYPE minmax GRANULARITY 1", table_name),
        ];

        for query in queries {
            self.client.execute(&query).await?;
        }

        Ok(())
    }
}
```

### 2. ç›‘æ§å’Œå‘Šè­¦

```rust
// rust/src/storage/monitoring.rs
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct StorageMonitor {
    client: Arc<ClickHouseClient>,
    metrics: Arc<RwLock<StorageMetrics>>,
}

#[derive(Debug, Default)]
pub struct StorageMetrics {
    pub total_records: u64,
    pub total_size_mb: f64,
    pub write_throughput: f64,
    pub read_throughput: f64,
    pub error_rate: f64,
    pub avg_response_time: Duration,
}

impl StorageMonitor {
    pub async fn start_monitoring(&self) -> Result<()> {
        let client = Arc::clone(&self.client);
        let metrics = Arc::clone(&self.metrics);

        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(60));

            loop {
                interval.tick().await;

                if let Ok(current_metrics) = Self::collect_metrics(&client).await {
                    *metrics.write().await = current_metrics;
                }
            }
        });

        Ok(())
    }

    async fn collect_metrics(client: &ClickHouseClient) -> Result<StorageMetrics> {
        let mut metrics = StorageMetrics::default();

        // æŸ¥è¯¢ç³»ç»ŸæŒ‡æ ‡
        let query = r#"
            SELECT
                sum(rows) as total_records,
                sum(data_bytes) / 1024 / 1024 as total_size_mb,
                avg(write_duration_ms) as avg_write_time_ms
            FROM system.parts
            WHERE active = 1 AND database = 'pulse_trader'
        "#;

        let mut row_set = client.query(query)?.fetch_all()?;
        if let Some(row) = row_set.next() {
            metrics.total_records = row.get(0)?;
            metrics.total_size_mb = row.get(1)?;
            metrics.avg_response_time = Duration::from_millis(row.get::<u64, _>(2)? as u64);
        }

        Ok(metrics)
    }
}
```

## âœ… æµ‹è¯•éªŒè¯

### è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶

```bash
# 1. å•å…ƒæµ‹è¯•
cd rust && cargo test --lib

# 2. é›†æˆæµ‹è¯•
cd rust && cargo test --test integration_tests

# 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
cd rust && cargo bench

# 4. Pythonç»‘å®šæµ‹è¯•
cd tests && python -m pytest test_clickhouse_storage.py -v

# 5. ç«¯åˆ°ç«¯æµ‹è¯•
python scripts/test_complete_pipeline.py
```

### æ€§èƒ½åŸºå‡†æŒ‡æ ‡

**é¢„æœŸæ€§èƒ½æŒ‡æ ‡ï¼š**
- **æ‰¹é‡å†™å…¥ååé‡**ï¼š> 50,000 records/s
- **å†…å­˜ä½¿ç”¨æ•ˆç‡**ï¼šé›¶æ‹·è´æ“ä½œ
- **å‹ç¼©æ¯”**ï¼šæ•°æ®å‹ç¼© > 70%
- **æŸ¥è¯¢å»¶è¿Ÿ**ï¼šP99 < 100ms
- **å­˜å‚¨æ•ˆç‡**ï¼šMergeTreeåˆ†åŒºä¼˜åŒ–

## ğŸ“‹ ä»»åŠ¡å®Œæˆæ¸…å•

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å·²ç»å®Œæˆäº†ï¼š

- [x] **ClickHouseé«˜æ€§èƒ½å­˜å‚¨æ¶æ„è®¾è®¡**
- [x] **Rustæ‰¹é‡å†™å…¥å™¨å®ç°**
- [x] **è¡¨ç»“æ„ä¼˜åŒ–å’Œç®¡ç†**
- [x] **æ€§èƒ½åŸºå‡†æµ‹è¯•**
- [x] **é”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶**
- [x] **ç›‘æ§å’Œä¼˜åŒ–å·¥å…·**

## ğŸ¯ å°ç»“

æœ¬èŠ‚å®ç°äº†åŸºäºRustçš„ClickHouseé«˜æ€§èƒ½æ‰¹é‡å­˜å‚¨ç³»ç»Ÿï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **é«˜æ€§èƒ½æ¶æ„**ï¼šå¹¶å‘å†™å…¥ã€å†…å­˜ä¼˜åŒ–ã€é›¶æ‹·è´æ“ä½œ
2. **å¯é æ€§ä¿è¯**ï¼šé‡è¯•æœºåˆ¶ã€äº‹åŠ¡æ”¯æŒã€æ•°æ®å®Œæ•´æ€§
3. **å¯æ‰©å±•æ€§**ï¼šè¡¨ç®¡ç†å™¨ã€æ€§èƒ½è°ƒä¼˜ã€ç›‘æ§å‘Šè­¦
4. **æ˜“ç”¨æ€§**ï¼šPythonç»‘å®šã€å®Œæ•´æµ‹è¯•ã€è¯¦ç»†æ–‡æ¡£

è¿™ä¸ªå­˜å‚¨ç³»ç»Ÿèƒ½å¤Ÿæ»¡è¶³å¤§è§„æ¨¡é‡åŒ–æ•°æ®çš„é«˜æ€§èƒ½å­˜å‚¨éœ€æ±‚ï¼Œä¸ºåç»­çš„å›æµ‹å’Œå®ç›˜äº¤æ˜“æä¾›åšå®çš„æ•°æ®åŸºç¡€ã€‚

---

*ä¸‹ä¸€èŠ‚å°†å®ç°æ•°æ®æ›´æ–°å’Œç»´æŠ¤ç³»ç»Ÿï¼Œç¡®ä¿æ•°æ®çš„æŒç»­æ€§å’Œä¸€è‡´æ€§ã€‚*
