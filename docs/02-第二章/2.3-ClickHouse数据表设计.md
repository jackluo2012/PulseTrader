# 2.3 ClickHouseæ•°æ®è¡¨è®¾è®¡

## ğŸ“– æœ¬èŠ‚æ¦‚è¿°

æœ¬èŠ‚å°†è®¾è®¡é«˜æ€§èƒ½çš„ClickHouseæ—¶åºæ•°æ®è¡¨ç»“æ„ï¼ŒåŒ…æ‹¬è‚¡ç¥¨è¡Œæƒ…æ•°æ®ã€åŸºç¡€ä¿¡æ¯æ•°æ®ã€æŠ€æœ¯æŒ‡æ ‡æ•°æ®ç­‰çš„å­˜å‚¨æ–¹æ¡ˆã€‚ClickHouseä½œä¸ºåˆ—å¼æ•°æ®åº“ï¼Œç‰¹åˆ«é€‚åˆé‡åŒ–äº¤æ˜“ä¸­æµ·é‡æ—¶åºæ•°æ®çš„å­˜å‚¨å’ŒæŸ¥è¯¢ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- âœ… è®¾è®¡é«˜æ•ˆçš„æ—¶åºæ•°æ®è¡¨ç»“æ„
- âœ… é…ç½®åˆ†åŒºå’Œæ’åºé”®ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
- âœ… å®æ–½æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†ç­–ç•¥
- âœ… ä¼˜åŒ–å­˜å‚¨å‹ç¼©å’ŒæŸ¥è¯¢æ€§èƒ½
- âœ… å»ºç«‹æ•°æ®å¤‡ä»½å’Œå®¹ç¾æœºåˆ¶

## â±ï¸ é¢„è®¡æ—¶é—´ï¼š40-50åˆ†é’Ÿ

---

## ğŸ—ï¸ ClickHouseè¡¨ç»“æ„è®¾è®¡

### 1. è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è¡¨

åˆ›å»ºè¡¨ç»“æ„SQLæ–‡ä»¶ `config/clickhouse/tables/stock_info.sql`ï¼š

```sql
-- è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è¡¨
CREATE TABLE IF NOT EXISTS pulse_trader.stock_info (
    -- ä¸»é”®å’ŒåŸºç¡€ä¿¡æ¯
    symbol String,                    -- è‚¡ç¥¨ä»£ç  (6ä½æ•°å­—)
    stock_name String,               -- è‚¡ç¥¨åç§°
    short_name Nullable(String),      -- è‚¡ç¥¨ç®€ç§°
    market String,                   -- å¸‚åœºä»£ç  (SH/SZ)
    exchange String,                 -- äº¤æ˜“æ‰€åç§°
    listing_date Date,               -- ä¸Šå¸‚æ—¥æœŸ
    delisting_date Nullable(Date),    -- é€€å¸‚æ—¥æœŸ

    -- åˆ†ç±»ä¿¡æ¯
    industry String,                 -- æ‰€å±è¡Œä¸š
    sector String,                   -- æ‰€å±æ¿å—
    concept_tags Array(String),      -- æ¦‚å¿µæ ‡ç­¾
    is_st UInt8 DEFAULT 0,           -- æ˜¯å¦STè‚¡
    is_star UInt8 DEFAULT 0,         -- æ˜¯å¦ç§‘åˆ›æ¿

    -- çŠ¶æ€ä¿¡æ¯
    is_active UInt8 DEFAULT 1,       -- æ˜¯å¦æ´»è·ƒäº¤æ˜“
    is_suspended UInt8 DEFAULT 0,    -- æ˜¯å¦åœç‰Œ
    trade_status String DEFAULT 'æ­£å¸¸', -- äº¤æ˜“çŠ¶æ€

    -- å…¬å¸ä¿¡æ¯
    total_shares Nullable(Decimal64(2)),     -- æ€»è‚¡æœ¬
    float_shares Nullable(Decimal64(2)),     -- æµé€šè‚¡æœ¬
    market_cap Nullable(Decimal64(2)),       -- æ€»å¸‚å€¼
    pe_ratio Nullable(Decimal32(4)),         -- å¸‚ç›ˆç‡
    pb_ratio Nullable(Decimal32(4)),         -- å¸‚å‡€ç‡

    -- å…ƒæ•°æ®
    data_source String DEFAULT 'akshare',   -- æ•°æ®æº
    created_at DateTime DEFAULT now(),       -- åˆ›å»ºæ—¶é—´
    updated_at DateTime DEFAULT now(),       -- æ›´æ–°æ—¶é—´
    version UInt16 DEFAULT 1                 -- ç‰ˆæœ¬å·

) ENGINE = ReplacingMergeTree(updated_at, version)
PARTITION BY toYYYYMM(updated_at)
ORDER BY (symbol, updated_at)
TTL updated_at + toIntervalDay(365) DELETE;  -- 1å¹´ååˆ é™¤æ—§ç‰ˆæœ¬
```

### 2. æ—¥çº¿è¡Œæƒ…æ•°æ®è¡¨

åˆ›å»º `config/clickhouse/tables/daily_quotes.sql`ï¼š

```sql
-- æ—¥çº¿è¡Œæƒ…æ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS pulse_trader.daily_quotes (
    -- ä¸»é”®å’Œæ ‡è¯†
    symbol String,                    -- è‚¡ç¥¨ä»£ç 
    date Date,                       -- äº¤æ˜“æ—¥æœŸ

    -- OHLCVä»·æ ¼æ•°æ®
    open_price Decimal32(4),         -- å¼€ç›˜ä»· (ä¿ç•™4ä½å°æ•°)
    high_price Decimal32(4),         -- æœ€é«˜ä»·
    low_price Decimal32(4),          -- æœ€ä½ä»·
    close_price Decimal32(4),        -- æ”¶ç›˜ä»·
    volume UInt64,                   -- æˆäº¤é‡ (è‚¡)
    amount Decimal64(2),             -- æˆäº¤é¢ (å…ƒ)

    -- ä»·æ ¼å˜åŠ¨ä¿¡æ¯
    change_amount Decimal32(4),      -- æ¶¨è·Œé¢
    change_percent Decimal32(4),     -- æ¶¨è·Œå¹…
    amplitude Decimal32(4),          -- æŒ¯å¹…
    turnover_rate Decimal32(4),      -- æ¢æ‰‹ç‡

    -- æŠ€æœ¯æŒ‡æ ‡å­—æ®µ
    ma5 Decimal32(4),               -- 5æ—¥å‡çº¿
    ma10 Decimal32(4),              -- 10æ—¥å‡çº¿
    ma20 Decimal32(4),              -- 20æ—¥å‡çº¿
    ma60 Decimal32(4),              -- 60æ—¥å‡çº¿
    ema12 Decimal32(4),             -- 12æ—¥æŒ‡æ•°å‡çº¿
    ema26 Decimal32(4),             -- 26æ—¥æŒ‡æ•°å‡çº¿
    rsi Decimal32(4),               -- RSIæŒ‡æ ‡

    -- å¸ƒæ—å¸¦æŒ‡æ ‡
    bb_upper Decimal32(4),          -- å¸ƒæ—å¸¦ä¸Šè½¨
    bb_middle Decimal32(4),         -- å¸ƒæ—å¸¦ä¸­è½¨
    bb_lower Decimal32(4),          -- å¸ƒæ—å¸¦ä¸‹è½¨
    bb_width Decimal32(4),          -- å¸ƒæ—å¸¦å®½åº¦
    bb_position Decimal32(4),       -- ä»·æ ¼åœ¨å¸ƒæ—å¸¦ä¸­çš„ä½ç½®

    -- MACDæŒ‡æ ‡
    macd Decimal32(6),              -- MACD DIFçº¿
    macd_signal Decimal32(6),       -- MACDä¿¡å·çº¿
    macd_histogram Decimal32(6),    -- MACDæŸ±çŠ¶å›¾

    -- æˆäº¤é‡æŒ‡æ ‡
    volume_ma5 UInt64,              -- 5æ—¥æˆäº¤é‡å‡çº¿
    volume_ratio Decimal32(4),      -- é‡æ¯”

    -- å…¶ä»–æŠ€æœ¯æŒ‡æ ‡
    kdj_k Decimal32(4),             -- KDJ Kå€¼
    kdj_d Decimal32(4),             -- KDJ Då€¼
    kdj_j Decimal32(4),             -- KDJ Jå€¼

    -- åŸºæœ¬é¢æ•°æ®
    pe_ratio Decimal32(4),          -- å¸‚ç›ˆç‡
    pb_ratio Decimal32(4),          -- å¸‚å‡€ç‡

    -- å…ƒæ•°æ®
    data_source String DEFAULT 'akshare',   -- æ•°æ®æº
    collected_at DateTime DEFAULT now(),   -- æ•°æ®é‡‡é›†æ—¶é—´
    updated_at DateTime DEFAULT now(),     -- æ•°æ®æ›´æ–°æ—¶é—´
    batch_id String,                        -- æ‰¹æ¬¡ID

    -- æ•°æ®è´¨é‡æ ‡è®°
    data_quality_score UInt8 DEFAULT 100,  -- æ•°æ®è´¨é‡è¯„åˆ†(0-100)
    is_holiday UInt8 DEFAULT 0,            -- æ˜¯å¦ä¸ºèŠ‚å‡æ—¥
    is_trading_day UInt8 DEFAULT 1         -- æ˜¯å¦ä¸ºäº¤æ˜“æ—¥

) ENGINE = ReplacingMergeTree(updated_at)
PARTITION BY (toYYYY(date), market)       -- æŒ‰å¹´ä»½å’Œå¸‚åœºåˆ†åŒº
ORDER BY (symbol, date)                    -- æŒ‰è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸæ’åº
SETTINGS index_granularity = 8192;        -- ç´¢å¼•ç²’åº¦
```

### 3. åˆ†é’Ÿçº§è¡Œæƒ…æ•°æ®è¡¨

åˆ›å»º `config/clickhouse/tables/intraday_quotes.sql`ï¼š

```sql
-- åˆ†é’Ÿçº§è¡Œæƒ…æ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS pulse_trader.intraday_quotes (
    -- ä¸»é”®å’Œæ—¶é—´æ ‡è¯†
    symbol String,                    -- è‚¡ç¥¨ä»£ç 
    datetime DateTime,                -- æ—¶é—´æˆ³ï¼ˆç²¾ç¡®åˆ°åˆ†é’Ÿï¼‰
    date Date,                        -- æ—¥æœŸï¼ˆä»datetimeæå–ï¼‰
    time UInt32,                      -- æ—¶é—´ï¼ˆHHMMæ ¼å¼ï¼‰

    -- OHLCVä»·æ ¼æ•°æ®
    open_price Decimal32(4),         -- å¼€ç›˜ä»·
    high_price Decimal32(4),         -- æœ€é«˜ä»·
    low_price Decimal32(4),          -- æœ€ä½ä»·
    close_price Decimal32(4),        -- æ”¶ç›˜ä»·
    volume UInt64,                   -- æˆäº¤é‡
    amount Decimal64(2),             -- æˆäº¤é¢

    -- å¸‚åœºå¾®ç»“æ„æ•°æ®
    bid_price Nullable(Decimal32(4)),   -- ä¹°ä¸€ä»·
    ask_price Nullable(Decimal32(4)),   -- å–ä¸€ä»·
    bid_volume Nullable(UInt64),        -- ä¹°ä¸€é‡
    ask_volume Nullable(UInt64),        -- å–ä¸€é‡

    -- å®æ—¶æŒ‡æ ‡
    vwap Decimal32(4),               -- æˆäº¤é‡åŠ æƒå¹³å‡ä»·
    price_position Decimal32(4),     -- ä»·æ ¼ä½ç½®ï¼ˆé«˜ä½ä»·ç›¸å¯¹ä½ç½®ï¼‰
    body_size Decimal32(4),          -- Kçº¿å®ä½“å¤§å°

    -- æ•°æ®æ¥æºå’Œè´¨é‡
    data_source String DEFAULT 'realtime',  -- æ•°æ®æºç±»å‹
    data_delay UInt16 DEFAULT 0,            -- æ•°æ®å»¶è¿Ÿ(æ¯«ç§’)
    collected_at DateTime DEFAULT now(),    -- æ•°æ®é‡‡é›†æ—¶é—´

    -- æ•°æ®å®Œæ•´æ€§æ ‡è®°
    is_complete UInt8 DEFAULT 1,            -- æ•°æ®æ˜¯å¦å®Œæ•´
    quality_score UInt8 DEFAULT 100         -- æ•°æ®è´¨é‡è¯„åˆ†

) ENGINE = MergeTree()
PARTITION BY (toYYYYMMDD(datetime), toHour(datetime))  -- æŒ‰æ—¥æœŸå’Œå°æ—¶åˆ†åŒº
ORDER BY (symbol, datetime)                               -- æŒ‰è‚¡ç¥¨å’Œæ—¶é—´æ’åº
TTL datetime + toIntervalDay(90) DELETE;                  -- 3ä¸ªæœˆåè‡ªåŠ¨åˆ é™¤

-- åˆ›å»ºç‰©åŒ–è§†å›¾ç”¨äºå®æ—¶ç»Ÿè®¡
CREATE MATERIALIZED VIEW IF NOT EXISTS pulse_trader.intraday_stats
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMMDD(datetime)
ORDER BY (symbol, toStartOfHour(datetime))
AS SELECT
    symbol,
    toStartOfHour(datetime) as hour_time,
    sum(volume) as total_volume,
    sum(amount) as total_amount,
    avg(close_price) as avg_price,
    min(low_price) as min_price,
    max(high_price) as max_price
FROM pulse_trader.intraday_quotes
GROUP BY symbol, hour_time;
```

### 4. å®æ—¶è¡Œæƒ…å¿«ç…§è¡¨

åˆ›å»º `config/clickhouse/tables/realtime_snapshot.sql`ï¼š

```sql
-- å®æ—¶è¡Œæƒ…å¿«ç…§è¡¨
CREATE TABLE IF NOT EXISTS pulse_trader.realtime_snapshot (
    -- ä¸»é”®
    symbol String,                    -- è‚¡ç¥¨ä»£ç 

    -- æœ€æ–°ä»·æ ¼ä¿¡æ¯
    current_price Decimal32(4),      -- æœ€æ–°ä»·
    open_price Decimal32(4),         -- ä»Šå¼€
    high_price Decimal32(4),         -- ä»Šé«˜
    low_price Decimal32(4),          -- ä»Šä½
    prev_close Decimal32(4),         -- æ˜¨æ”¶

    -- æ¶¨è·Œä¿¡æ¯
    change_amount Decimal32(4),      -- æ¶¨è·Œé¢
    change_percent Decimal32(4),     -- æ¶¨è·Œå¹…
    amplitude Decimal32(4),          -- æŒ¯å¹…

    -- æˆäº¤ä¿¡æ¯
    volume UInt64,                   -- æˆäº¤é‡
    amount Decimal64(2),             -- æˆäº¤é¢
    turnover_rate Decimal32(4),      -- æ¢æ‰‹ç‡
    volume_ratio Decimal32(4),       -- é‡æ¯”

    -- å§”ä¹°å–æ•°æ®
    bid_price Array(Decimal32(4)),   -- ä¹°ç›˜ä»·æ ¼ï¼ˆä¹°ä¸€åˆ°ä¹°äº”ï¼‰
    bid_volume Array(UInt64),        -- ä¹°ç›˜æ•°é‡
    ask_price Array(Decimal32(4)),   -- å–ç›˜ä»·æ ¼
    ask_volume Array(UInt64),        -- å–ç›˜æ•°é‡

    -- å¸‚åœºæ·±åº¦
    total_bid_volume UInt64,         -- æ€»ä¹°ç›˜é‡
    total_ask_volume UInt64,         -- æ€»å–ç›˜é‡
    bid_ask_spread Decimal32(4),     -- ä¹°å–ä»·å·®

    -- åŸºæœ¬é¢æ•°æ®
    market_cap Decimal64(2),         -- æ€»å¸‚å€¼
    circulating_capital Decimal64(2), -- æµé€šå¸‚å€¼
    pe_ratio Decimal32(4),           -- å¸‚ç›ˆç‡

    -- æŠ€æœ¯æŒ‡æ ‡
    ma5 Decimal32(4),
    ma10 Decimal32(4),
    ma20 Decimal32(4),
    ma60 Decimal32(4),

    -- æ›´æ–°æ—¶é—´
    updated_at DateTime DEFAULT now(),  -- æœ€åæ›´æ–°æ—¶é—´
    data_source String DEFAULT 'realtime', -- æ•°æ®æº

    -- çŠ¶æ€æ ‡è®°
    is_trading UInt8 DEFAULT 1,         -- æ˜¯å¦äº¤æ˜“ä¸­
    is_suspended UInt8 DEFAULT 0,       -- æ˜¯å¦åœç‰Œ
    update_count UInt32 DEFAULT 1       -- æ›´æ–°æ¬¡æ•°

) ENGINE = ReplacingMergeTree(updated_at)
ORDER BY symbol
SETTINGS index_granularity = 1;
```

### 5. è´¢åŠ¡æ•°æ®è¡¨

åˆ›å»º `config/clickhouse/tables/financial_data.sql`ï¼š

```sql
-- è´¢åŠ¡æ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS pulse_trader.financial_data (
    -- ä¸»é”®æ ‡è¯†
    symbol String,                    -- è‚¡ç¥¨ä»£ç 
    report_date Date,                 -- æŠ¥å‘ŠæœŸ
    report_type String,               -- æŠ¥å‘Šç±»å‹ (å¹´æŠ¥/åŠå¹´æŠ¥/å­£æŠ¥)

    -- ç›ˆåˆ©èƒ½åŠ›æŒ‡æ ‡
    total_revenue Decimal64(2),       -- è¥ä¸šæ€»æ”¶å…¥
    net_profit Decimal64(2),          -- å‡€åˆ©æ¶¦
    gross_profit Decimal64(2),        -- æ¯›åˆ©æ¶¦
    operating_profit Decimal64(2),    -- è¥ä¸šåˆ©æ¶¦
    eps Decimal32(4),                 -- æ¯è‚¡æ”¶ç›Š
    roe Decimal32(4),                 -- å‡€èµ„äº§æ”¶ç›Šç‡
    roa Decimal32(4),                 -- æ€»èµ„äº§æ”¶ç›Šç‡

    -- å¿å€ºèƒ½åŠ›æŒ‡æ ‡
    total_assets Decimal64(2),        -- æ€»èµ„äº§
    total_liabilities Decimal64(2),   -- æ€»è´Ÿå€º
    current_assets Decimal64(2),      -- æµåŠ¨èµ„äº§
    current_liabilities Decimal64(2), -- æµåŠ¨è´Ÿå€º
    debt_ratio Decimal32(4),          -- èµ„äº§è´Ÿå€ºç‡

    -- è¿è¥èƒ½åŠ›æŒ‡æ ‡
    inventory_turnover Decimal32(4),  -- å­˜è´§å‘¨è½¬ç‡
    receivable_turnover Decimal32(4), -- åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡
    total_asset_turnover Decimal32(4), -- æ€»èµ„äº§å‘¨è½¬ç‡

    -- æˆé•¿èƒ½åŠ›æŒ‡æ ‡
    revenue_growth Decimal32(4),      -- è¥æ”¶å¢é•¿ç‡
    profit_growth Decimal32(4),       -- åˆ©æ¶¦å¢é•¿ç‡

    -- ç°é‡‘æµé‡æŒ‡æ ‡
    operating_cash_flow Decimal64(2), -- ç»è¥ç°é‡‘æµ
    free_cash_flow Decimal64(2),      -- è‡ªç”±ç°é‡‘æµ

    -- å…¶ä»–è´¢åŠ¡æ•°æ®
    shares_outstanding Decimal64(2),  -- æ€»è‚¡æœ¬
    book_value_per_share Decimal32(4), -- æ¯è‚¡å‡€èµ„äº§

    -- å…ƒæ•°æ®
    data_source String DEFAULT 'company_report', -- æ•°æ®æº
    published_at Date,               -- å‘å¸ƒæ—¥æœŸ
    created_at DateTime DEFAULT now(),          -- åˆ›å»ºæ—¶é—´
    updated_at DateTime DEFAULT now()           -- æ›´æ–°æ—¶é—´

) ENGINE = ReplacingMergeTree(updated_at)
PARTITION BY (toYYYY(report_date), report_type)
ORDER BY (symbol, report_date, report_type);
```

### 6. äº¤æ˜“æ—¥å†è¡¨

åˆ›å»º `config/clickhouse/tables/trading_calendar.sql`ï¼š

```sql
-- äº¤æ˜“æ—¥å†è¡¨
CREATE TABLE IF NOT EXISTS pulse_trader.trading_calendar (
    -- ä¸»é”®
    date Date,                       -- æ—¥æœŸ

    -- äº¤æ˜“ä¿¡æ¯
    is_trading_day UInt8 DEFAULT 1,  -- æ˜¯å¦äº¤æ˜“æ—¥
    market String,                   -- å¸‚åœºä»£ç 

    -- èŠ‚å‡æ—¥ä¿¡æ¯
    is_holiday UInt8 DEFAULT 0,      -- æ˜¯å¦èŠ‚å‡æ—¥
    holiday_name Nullable(String),   -- èŠ‚å‡æ—¥åç§°

    -- ç‰¹æ®Šäº¤æ˜“æ—¥
    is_special_trading_day UInt8 DEFAULT 0, -- æ˜¯å¦ç‰¹æ®Šäº¤æ˜“æ—¥
    special_day_type Nullable(String),      -- ç‰¹æ®Šäº¤æ˜“æ—¥ç±»å‹

    -- äº¤æ˜“æ—¶é—´
    trading_start_time String DEFAULT '09:30',  -- å¼€å§‹äº¤æ˜“æ—¶é—´
    trading_end_time String DEFAULT '15:00',    -- ç»“æŸäº¤æ˜“æ—¶é—´
    lunch_start_time String DEFAULT '11:30',    -- åˆä¼‘å¼€å§‹
    lunch_end_time String DEFAULT '13:00',      -- åˆä¼‘ç»“æŸ

    -- å…ƒæ•°æ®
    data_source String DEFAULT 'exchange',  -- æ•°æ®æº
    created_at DateTime DEFAULT now(),      -- åˆ›å»ºæ—¶é—´
    updated_at DateTime DEFAULT now()       -- æ›´æ–°æ—¶é—´

) ENGINE = ReplacingMergeTree(date)
ORDER BY date
PRIMARY KEY date;

-- åˆ›å»ºäº¤æ˜“æ—¥å†å‡½æ•°è§†å›¾
CREATE VIEW IF NOT EXISTS pulse_trader.v_trading_days AS
SELECT date FROM pulse_trader.trading_calendar WHERE is_trading_day = 1 ORDER BY date;

-- è·å–æœ€è¿‘Nä¸ªäº¤æ˜“æ—¥çš„å‡½æ•°
CREATE FUNCTION IF NOT EXISTS get_recent_trading_days AS (start_date Date, days_count UInt32) -> Array(Date)
RETURN arraySlice(
    arrayFilter(x -> x <= start_date,
        groupUniqArray(date)
    ), -days_count
);
```

---

## ğŸ¯ è¡¨è®¾è®¡ä¼˜åŒ–ç­–ç•¥

### 1. åˆ†åŒºç­–ç•¥ä¼˜åŒ–

åˆ›å»ºåˆ†åŒºç®¡ç†è„šæœ¬ `scripts/manage_partitions.py`ï¼š

```python
#!/usr/bin/env python3
"""
ClickHouseåˆ†åŒºç®¡ç†è„šæœ¬
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import pandas as pd
from datetime import datetime, timedelta
from src.data.database import db_manager
import logging

logger = logging.getLogger(__name__)

class PartitionManager:
    """åˆ†åŒºç®¡ç†å™¨"""

    def __init__(self, db_client=None):
        self.db_client = db_client or db_manager

    def create_partition_function(self):
        """åˆ›å»ºåˆ†åŒºç®¡ç†å‡½æ•°"""
        sql = """
        -- è‡ªåŠ¨åˆ›å»ºæœªæ¥åˆ†åŒºçš„å­˜å‚¨è¿‡ç¨‹
        CREATE PROCEDURE IF NOT EXISTS pulse_trader.create_future_partitions() AS
        BEGIN
            DECLARE future_days DEFAULT 30;
            DECLARE i DEFAULT 1;
            DECLARE current_date DEFAULT today();
            DECLARE partition_date DATE;
            DECLARE partition_name STRING;

            -- ä¸ºæ—¥çº¿æ•°æ®åˆ›å»ºæœªæ¥åˆ†åŒº
            WHILE i <= future_days DO
                partition_date = current_date + INTERVAL i DAY;
                partition_name = toString(toYYYYMM(partition_date));

                -- æ‰§è¡Œåˆ†åŒºåˆ›å»ºï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å…·ä½“è¡¨ç»“æ„è°ƒæ•´ï¼‰
                -- ALTER TABLE pulse_trader.daily_quotes ADD PARTITION partition_name;

                i = i + 1;
            END WHILE;
        END;
        """
        self.db_client.execute_query(sql)
        logger.info("åˆ†åŒºç®¡ç†å‡½æ•°åˆ›å»ºå®Œæˆ")

    def analyze_partition_usage(self):
        """åˆ†æåˆ†åŒºä½¿ç”¨æƒ…å†µ"""
        sql = """
        SELECT
            table,
            partition,
            count() as row_count,
            sum(bytes_on_disk) as size_bytes,
            round(sum(bytes_on_disk) / 1024 / 1024, 2) as size_mb,
            round(avg(row_count), 0) as avg_rows_per_part
        FROM system.parts
        WHERE database = 'pulse_trader'
          AND active = 1
          AND table IN ('daily_quotes', 'intraday_quotes')
        GROUP BY table, partition
        ORDER BY table, partition
        """

        try:
            result = self.db_client.execute_query(sql)
            if result:
                df = pd.DataFrame(result,
                    columns=['table', 'partition', 'row_count', 'size_bytes', 'size_mb', 'avg_rows_per_part'])
                logger.info("åˆ†åŒºä½¿ç”¨æƒ…å†µåˆ†æå®Œæˆ")
                return df
        except Exception as e:
            logger.error(f"åˆ†åŒºä½¿ç”¨æƒ…å†µåˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()

    def optimize_partitions(self, table_name: str, partition_filter: str = None):
        """ä¼˜åŒ–æŒ‡å®šåˆ†åŒº"""
        base_sql = f"OPTIMIZE TABLE {table_name}"

        if partition_filter:
            sql = f"{base_sql} PARTITION {partition_filter} FINAL"
        else:
            sql = f"{base_sql} FINAL"

        try:
            self.db_client.execute_query(sql)
            logger.info(f"åˆ†åŒºä¼˜åŒ–å®Œæˆ: {table_name} {partition_filter or 'ALL'}")
        except Exception as e:
            logger.error(f"åˆ†åŒºä¼˜åŒ–å¤±è´¥ {table_name}: {e}")

    def drop_old_partitions(self, table_name: str, retention_days: int):
        """åˆ é™¤è¿‡æœŸåˆ†åŒº"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        cutoff_partition = cutoff_date.strftime('%Y%m')

        sql = f"""
        ALTER TABLE {table_name}
        DROP PARTITION WHERE partition < '{cutoff_partition}'
        """

        try:
            self.db_client.execute_query(sql)
            logger.info(f"åˆ é™¤è¿‡æœŸåˆ†åŒºå®Œæˆ: {table_name}, ä¿ç•™æœ€è¿‘{retention_days}å¤©")
        except Exception as e:
            logger.error(f"åˆ é™¤è¿‡æœŸåˆ†åŒºå¤±è´¥ {table_name}: {e}")

    def merge_small_partitions(self, table_name: str, min_size_mb: float = 10):
        """åˆå¹¶å°åˆ†åŒº"""
        sql = f"""
        SELECT
            partition,
            sum(bytes_on_disk) / 1024 / 1024 as size_mb
        FROM system.parts
        WHERE database = 'pulse_trader'
          AND table = '{table_name}'
          AND active = 1
        GROUP BY partition
        HAVING size_mb < {min_size_mb}
        ORDER BY partition
        """

        try:
            small_partitions = self.db_client.execute_query(sql)

            if small_partitions:
                logger.info(f"å‘ç° {len(small_partitions)} ä¸ªå°åˆ†åŒºéœ€è¦åˆå¹¶")

                for partition_info in small_partitions:
                    partition_name = partition_info[0]
                    size_mb = partition_info[1]
                    logger.info(f"åˆ†åŒº {partition_name}: {size_mb:.2f}MB")

                # æ‰§è¡Œåˆ†åŒºåˆå¹¶
                merge_sql = f"OPTIMIZE TABLE {table_name} FINAL"
                self.db_client.execute_query(merge_sql)
                logger.info(f"å°åˆ†åŒºåˆå¹¶å®Œæˆ: {table_name}")

        except Exception as e:
            logger.error(f"åˆå¹¶å°åˆ†åŒºå¤±è´¥ {table_name}: {e}")


def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(level=logging.INFO)

    manager = PartitionManager()

    print("=== ClickHouseåˆ†åŒºç®¡ç† ===")

    # 1. åˆ†æåˆ†åŒºä½¿ç”¨æƒ…å†µ
    print("\n1. åˆ†æåˆ†åŒºä½¿ç”¨æƒ…å†µ...")
    usage_df = manager.analyze_partition_usage()
    if not usage_df.empty:
        print(usage_df)

    # 2. ä¼˜åŒ–åˆ†åŒº
    print("\n2. ä¼˜åŒ–åˆ†åŒº...")
    tables_to_optimize = ['daily_quotes', 'intraday_quotes']
    for table in tables_to_optimize:
        manager.optimize_partitions(table)

    # 3. åˆå¹¶å°åˆ†åŒº
    print("\n3. åˆå¹¶å°åˆ†åŒº...")
    for table in tables_to_optimize:
        manager.merge_small_partitions(table, min_size_mb=5)

    print("\nåˆ†åŒºç®¡ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
```

### 2. ç´¢å¼•ä¼˜åŒ–é…ç½®

åˆ›å»ºç´¢å¼•ä¼˜åŒ–é…ç½® `config/clickhouse/indexes.sql`ï¼š

```sql
-- åˆ›å»ºè·³æ•°ç´¢å¼•ï¼ˆSkipping Indexï¼‰ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½

-- ä¸ºæ—¥çº¿è¡Œæƒ…æ•°æ®åˆ›å»ºç´¢å¼•
ALTER TABLE pulse_trader.daily_quotes
ADD INDEX idx_symbol_volume (symbol, volume) TYPE minmax GRANULARITY 1;

ALTER TABLE pulse_trader.daily_quotes
ADD INDEX idx_price_change (change_percent) TYPE minmax GRANULARITY 10;

ALTER TABLE pulse_trader.daily_quotes
ADD INDEX idx_date_symbol (date, symbol) TYPE bloom_filter GRANULARITY 1;

-- ä¸ºåˆ†é’Ÿçº§è¡Œæƒ…æ•°æ®åˆ›å»ºç´¢å¼•
ALTER TABLE pulse_trader.intraday_quotes
ADD INDEX idx_datetime_symbol (datetime, symbol) TYPE minmax GRANULARITY 1;

ALTER TABLE pulse_trader.intraday_quotes
ADD INDEX idx_volume (volume) TYPE minmax GRANULARITY 100;

-- ä¸ºå®æ—¶å¿«ç…§è¡¨åˆ›å»ºç´¢å¼•
ALTER TABLE pulse_trader.realtime_snapshot
ADD INDEX idx_market_cap (market_cap) TYPE minmax GRANULARITY 1;

ALTER TABLE pulse_trader.realtime_snapshot
ADD INDEX idx_change_percent (change_percent) TYPE minmax GRANULARITY 1;

-- ä¸ºè´¢åŠ¡æ•°æ®åˆ›å»ºç´¢å¼•
ALTER TABLE pulse_trader.financial_data
ADD INDEX idx_report_date (report_date) TYPE minmax GRANULARITY 1;

ALTER TABLE pulse_trader.financial_data
ADD INDEX idx_symbol_report_type (symbol, report_type) TYPE bloom_filter GRANULARITY 1;

-- åˆ›å»ºç‰©åŒ–è§†å›¾ç”¨äºå¸¸ç”¨ç»Ÿè®¡æŸ¥è¯¢
CREATE MATERIALIZED VIEW IF NOT EXISTS pulse_trader.daily_market_stats
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(date)
ORDER BY (date, market)
AS SELECT
    date,
    market,
    count() as stock_count,
    sum(volume) as total_volume,
    sum(amount) as total_amount,
    avg(close_price) as avg_price,
    max(change_percent) as max_change,
    min(change_percent) as min_change,
    countIf(change_percent > 0) as rising_stocks,
    countIf(change_percent < 0) as falling_stocks,
    countIf(abs(change_percent) >= 0.09) as limit_up_stocks,
    countIf(change_percent <= -0.09) as limit_down_stocks
FROM (
    SELECT
        date,
        substring(symbol, 1, 1) as market,  -- SH=ä¸Šæµ·, SZ=æ·±åœ³
        volume,
        amount,
        close_price,
        change_percent
    FROM pulse_trader.daily_quotes
    WHERE date >= today() - INTERVAL 30 DAY
)
GROUP BY date, market;

-- åˆ›å»ºçƒ­é—¨è‚¡ç¥¨ç»Ÿè®¡è§†å›¾
CREATE MATERIALIZED VIEW IF NOT EXISTS pulse_trader.hot_stocks_ranking
ENGINE = ReplacingMergeTree(updated_at)
PARTITION BY toYYYYMM(date)
ORDER BY (date, ranking_type, rank)
AS SELECT
    date,
    'volume' as ranking_type,
    row_number() OVER (ORDER BY volume DESC) as rank,
    symbol,
    stock_name,
    volume,
    amount,
    change_percent,
    updated_at
FROM (
    SELECT
        date,
        symbol,
        first_value(stock_name) OVER (PARTITION BY symbol ORDER BY date DESC) as stock_name,
        volume,
        amount,
        change_percent,
        updated_at
    FROM pulse_trader.daily_quotes
    WHERE date = today() - INTERVAL 1 DAY
    AND volume > 0
);
```

---

## ğŸ“Š æ•°æ®æ’å…¥ä¼˜åŒ–

### 1. æ‰¹é‡æ’å…¥å·¥å…·

åˆ›å»º `src/data/storage/batch_insert.py`ï¼š

```python
"""
ClickHouseæ‰¹é‡æ•°æ®æ’å…¥å·¥å…·
"""

import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
import time

logger = logging.getLogger(__name__)

class BatchInserter:
    """æ‰¹é‡æ•°æ®æ’å…¥å™¨"""

    def __init__(self, db_client, batch_size: int = 10000, max_workers: int = 4):
        """
        åˆå§‹åŒ–æ‰¹é‡æ’å…¥å™¨

        Args:
            db_client: ClickHouseå®¢æˆ·ç«¯
            batch_size: æ‰¹é‡å¤§å°
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        """
        self.db_client = db_client
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.insert_queue = Queue()
        self.error_count = 0
        self.success_count = 0
        self.lock = threading.Lock()

    def insert_dataframe(self, df: pd.DataFrame, table_name: str,
                        columns_mapping: Dict[str, str] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡æ’å…¥DataFrameæ•°æ®

        Args:
            df: è¦æ’å…¥çš„æ•°æ®
            table_name: ç›®æ ‡è¡¨å
            columns_mapping: åˆ—åæ˜ å°„

        Returns:
            æ’å…¥ç»“æœç»Ÿè®¡
        """
        if df.empty:
            logger.warning("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ’å…¥")
            return {'inserted_rows': 0, 'success': True, 'message': 'No data to insert'}

        start_time = time.time()

        # åº”ç”¨åˆ—åæ˜ å°„
        if columns_mapping:
            df = df.rename(columns=columns_mapping)

        # æ·»åŠ å…ƒæ•°æ®
        if 'created_at' not in df.columns:
            df['created_at'] = datetime.now()

        # åˆ†æ‰¹æ’å…¥
        total_rows = len(df)
        inserted_rows = 0

        try:
            for i in range(0, total_rows, self.batch_size):
                batch_df = df.iloc[i:i + self.batch_size]

                try:
                    # è½¬æ¢ä¸ºClickHouseæ ¼å¼
                    batch_data = self._prepare_data_for_clickhouse(batch_df)

                    # æ„å»ºæ’å…¥SQL
                    columns_str = ', '.join(batch_df.columns)
                    placeholders = ', '.join(['%s'] * len(batch_df.columns))

                    sql = f"INSERT INTO {table_name} ({columns_str}) VALUES"

                    # æ‰§è¡Œæ’å…¥
                    self.db_client.execute_query(sql, batch_data)

                    inserted_rows += len(batch_df)
                    logger.info(f"æ’å…¥è¿›åº¦: {inserted_rows}/{total_rows}")

                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡æ’å…¥å¤±è´¥ {table_name}: {e}")
                    with self.lock:
                        self.error_count += 1

            duration = time.time() - start_time
            success_rate = inserted_rows / total_rows

            result = {
                'inserted_rows': inserted_rows,
                'total_rows': total_rows,
                'success_rate': success_rate,
                'duration': duration,
                'rows_per_second': inserted_rows / duration if duration > 0 else 0,
                'success': success_rate > 0.9  # 90%ä»¥ä¸ŠæˆåŠŸç‡è§†ä¸ºæˆåŠŸ
            }

            logger.info(f"æ’å…¥å®Œæˆ {table_name}: {inserted_rows}/{total_rows} è¡Œ, "
                       f"æˆåŠŸç‡: {success_rate:.2%}, è€—æ—¶: {duration:.2f}s")

            return result

        except Exception as e:
            logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥ {table_name}: {e}")
            return {
                'inserted_rows': inserted_rows,
                'total_rows': total_rows,
                'error': str(e),
                'success': False
            }

    def _prepare_data_for_clickhouse(self, df: pd.DataFrame) -> List[tuple]:
        """
        ä¸ºClickHouseå‡†å¤‡æ•°æ®

        Args:
            df: è¾“å…¥DataFrame

        Returns:
            ClickHouseæ ¼å¼çš„æ•°æ®åˆ—è¡¨
        """
        # å¤„ç†ç©ºå€¼
        df = df.fillna('')

        # è½¬æ¢æ—¥æœŸæ—¶é—´æ ¼å¼
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]':
                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
            elif df[col].dtype == 'object':
                # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ç©ºå€¼
                df[col] = df[col].astype(str)
                df[col] = df[col].replace('nan', '')

        # è½¬æ¢ä¸ºå…ƒç»„åˆ—è¡¨
        return [tuple(row) for row in df.values]

    def insert_with_retry(self, data, table_name: str, max_retries: int = 3) -> bool:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®æ’å…¥

        Args:
            data: è¦æ’å…¥çš„æ•°æ®
            table_name: ç›®æ ‡è¡¨å
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            æ˜¯å¦æ’å…¥æˆåŠŸ
        """
        for attempt in range(max_retries):
            try:
                self.db_client.execute_query(f"INSERT INTO {table_name} VALUES", data)
                return True

            except Exception as e:
                logger.warning(f"æ’å…¥é‡è¯• {attempt + 1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"æ’å…¥æœ€ç»ˆå¤±è´¥ {table_name}: {e}")
                    return False

    def parallel_insert(self, data_batches: List[tuple], table_name: str) -> Dict[str, Any]:
        """
        å¹¶è¡Œæ’å…¥æ•°æ®

        Args:
            data_batches: æ•°æ®æ‰¹æ¬¡åˆ—è¡¨
            table_name: ç›®æ ‡è¡¨å

        Returns:
            æ’å…¥ç»“æœç»Ÿè®¡
        """
        start_time = time.time()
        success_count = 0
        error_count = 0

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰æ’å…¥ä»»åŠ¡
            future_to_batch = {
                executor.submit(self.insert_with_retry, batch, table_name): batch
                for batch in data_batches
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    if future.result():
                        success_count += len(batch)
                    else:
                        error_count += len(batch)
                except Exception as e:
                    logger.error(f"å¹¶è¡Œæ’å…¥å¼‚å¸¸: {e}")
                    error_count += len(batch)

        duration = time.time() - start_time
        total_rows = sum(len(batch) for batch in data_batches)

        result = {
            'success_rows': success_count,
            'error_rows': error_count,
            'total_rows': total_rows,
            'success_rate': success_count / total_rows if total_rows > 0 else 0,
            'duration': duration,
            'parallel_workers': self.max_workers
        }

        logger.info(f"å¹¶è¡Œæ’å…¥å®Œæˆ {table_name}: {success_count}/{total_rows} è¡Œ, "
                   f"æˆåŠŸç‡: {result['success_rate']:.2%}, è€—æ—¶: {duration:.2f}s")

        return result

    def insert_daily_quotes(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        æ’å…¥æ—¥çº¿è¡Œæƒ…æ•°æ®ï¼ˆä¸“ç”¨æ–¹æ³•ï¼‰

        Args:
            df: æ—¥çº¿æ•°æ®DataFrame

        Returns:
            æ’å…¥ç»“æœ
        """
        # åˆ—åæ˜ å°„
        columns_mapping = {
            'symbol': 'symbol',
            'date': 'date',
            'open': 'open_price',
            'high': 'high_price',
            'low': 'low_price',
            'close': 'close_price',
            'volume': 'volume',
            'amount': 'amount',
            'change': 'change_amount',
            'pct_chg': 'change_percent',
            'amplitude': 'amplitude',
            'turnover': 'turnover_rate'
        }

        # æ•°æ®éªŒè¯
        required_columns = ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']
        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")

        # ä¸“ç”¨æ’å…¥é€»è¾‘
        return self.insert_dataframe(df, 'daily_quotes', columns_mapping)

    def insert_realtime_snapshot(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        æ’å…¥å®æ—¶å¿«ç…§æ•°æ®

        Args:
            df: å®æ—¶æ•°æ®DataFrame

        Returns:
            æ’å…¥ç»“æœ
        """
        # æ·»åŠ æ›´æ–°æ—¶é—´
        if 'updated_at' not in df.columns:
            df['updated_at'] = datetime.now()

        # å¢åŠ æ›´æ–°è®¡æ•°ï¼ˆç®€å•å®ç°ï¼‰
        df['update_count'] = 1

        return self.insert_dataframe(df, 'realtime_snapshot')


def test_batch_inserter():
    """æµ‹è¯•æ‰¹é‡æ’å…¥å™¨"""
    from src.data.database import db_manager

    inserter = BatchInserter(db_manager, batch_size=1000)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'symbol': ['000001'] * 100,
        'date': pd.date_range('2024-01-01', periods=100),
        'open': [10.0] * 100,
        'high': [10.5] * 100,
        'low': [9.5] * 100,
        'close': [10.2] * 100,
        'volume': [1000000] * 100,
        'amount': [10200000] * 100
    })

    print("=== æµ‹è¯•æ‰¹é‡æ’å…¥ ===")
    result = inserter.insert_daily_quotes(test_data)
    print(f"æ’å…¥ç»“æœ: {result}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_batch_inserter()
```

---

## ğŸ§ª è¡¨ç»“æ„æµ‹è¯•

### 1. åˆ›å»ºè¡¨ç»“æ„åˆå§‹åŒ–è„šæœ¬

åˆ›å»º `scripts/init_clickhouse_tables.py`ï¼š

```python
#!/usr/bin/env python3
"""
ClickHouseè¡¨ç»“æ„åˆå§‹åŒ–è„šæœ¬
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.data.database import db_manager
import logging

logger = logging.getLogger(__name__)

def create_tables():
    """åˆ›å»ºæ‰€æœ‰è¡¨ç»“æ„"""

    table_scripts = [
        'config/clickhouse/tables/stock_info.sql',
        'config/clickhouse/tables/daily_quotes.sql',
        'config/clickhouse/tables/intraday_quotes.sql',
        'config/clickhouse/tables/realtime_snapshot.sql',
        'config/clickhouse/tables/financial_data.sql',
        'config/clickhouse/tables/trading_calendar.sql'
    ]

    for script_path in table_scripts:
        try:
            if os.path.exists(script_path):
                with open(script_path, 'r', encoding='utf-8') as f:
                    sql_content = f.read()

                # åˆ†å‰²å¤šä¸ªSQLè¯­å¥
                sql_statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]

                for sql in sql_statements:
                    if sql:
                        db_manager.execute_query(sql)
                        logger.info(f"æ‰§è¡ŒSQL: {sql[:50]}...")

                logger.info(f"è¡¨ç»“æ„åˆ›å»ºå®Œæˆ: {script_path}")
            else:
                logger.warning(f"è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {script_path}")

        except Exception as e:
            logger.error(f"åˆ›å»ºè¡¨ç»“æ„å¤±è´¥ {script_path}: {e}")

def create_indexes():
    """åˆ›å»ºç´¢å¼•"""
    index_script = 'config/clickhouse/indexes.sql'

    try:
        if os.path.exists(index_script):
            with open(index_script, 'r', encoding='utf-8') as f:
                sql_content = f.read()

            sql_statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]

            for sql in sql_statements:
                if sql:
                    db_manager.execute_query(sql)
                    logger.info(f"åˆ›å»ºç´¢å¼•: {sql[:50]}...")

            logger.info("ç´¢å¼•åˆ›å»ºå®Œæˆ")
        else:
            logger.warning(f"ç´¢å¼•è„šæœ¬ä¸å­˜åœ¨: {index_script}")

    except Exception as e:
        logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")

def verify_tables():
    """éªŒè¯è¡¨ç»“æ„"""
    sql = """
    SELECT
        table,
        count() as column_count,
        sum(bytes_on_disk) as size_bytes,
        round(sum(bytes_on_disk) / 1024, 2) as size_kb
    FROM system.columns
    WHERE database = 'pulse_trader'
    GROUP BY table
    ORDER BY table
    """

    try:
        result = db_manager.execute_query(sql)
        if result:
            print("\n=== è¡¨ç»“æ„éªŒè¯ ===")
            for row in result:
                print(f"è¡¨: {row[0]}, åˆ—æ•°: {row[1]}, å¤§å°: {row[3]} KB")
    except Exception as e:
        logger.error(f"éªŒè¯è¡¨ç»“æ„å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(level=logging.INFO)

    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–ClickHouseè¡¨ç»“æ„...")

    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not db_manager.test_connection():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return

    print("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")

    # åˆ›å»ºè¡¨ç»“æ„
    print("\nğŸ“ åˆ›å»ºè¡¨ç»“æ„...")
    create_tables()

    # åˆ›å»ºç´¢å¼•
    print("\nğŸ” åˆ›å»ºç´¢å¼•...")
    create_indexes()

    # éªŒè¯è¡¨ç»“æ„
    print("\nâœ”ï¸ éªŒè¯è¡¨ç»“æ„...")
    verify_tables()

    print("\nğŸ‰ ClickHouseè¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆï¼")


if __name__ == "__main__":
    main()
```

### 2. è¿è¡Œåˆå§‹åŒ–

```bash
# ç¡®ä¿ClickHouseæœåŠ¡è¿è¡Œ
docker-compose up -d

# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒæ¿€æ´»
source .venv/bin/activate

# è¿è¡Œè¡¨ç»“æ„åˆå§‹åŒ–
python scripts/init_clickhouse_tables.py
```

---

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### 1. åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬

åˆ›å»º `scripts/monitor_clickhouse_performance.py`ï¼š

```python
#!/usr/bin/env python3
"""
ClickHouseæ€§èƒ½ç›‘æ§è„šæœ¬
"""

import time
import pandas as pd
from datetime import datetime, timedelta
from src.data.database import db_manager
import logging

logger = logging.getLogger(__name__)

class ClickHouseMonitor:
    """ClickHouseæ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, db_client=None):
        self.db_client = db_client or db_manager

    def get_table_stats(self) -> pd.DataFrame:
        """è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯"""
        sql = """
        SELECT
            table,
            count() as row_count,
            sum(bytes_on_disk) as size_bytes,
            round(sum(bytes_on_disk) / 1024 / 1024, 2) as size_mb,
            round(avg uncompressed_size) / 1024 / 1024, 2) as avg_uncompressed_size_mb,
            round(sum(compressed_size) / sum(uncompressed_size) * 100, 2) as compression_ratio
        FROM system.parts
        WHERE database = 'pulse_trader'
          AND active = 1
        GROUP BY table
        ORDER BY size_bytes DESC
        """

        try:
            result = self.db_client.execute_query(sql)
            if result:
                return pd.DataFrame(result, columns=[
                    'table', 'row_count', 'size_bytes', 'size_mb',
                    'avg_uncompressed_size_mb', 'compression_ratio'
                ])
        except Exception as e:
            logger.error(f"è·å–è¡¨ç»Ÿè®¡å¤±è´¥: {e}")

        return pd.DataFrame()

    def get_query_performance(self) -> pd.DataFrame:
        """è·å–æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡"""
        sql = """
        SELECT
            query,
            count() as execution_count,
            avg(query_duration_ms) / 1000 as avg_duration_sec,
            max(query_duration_ms) / 1000 as max_duration_sec,
            sum(read_rows) as total_read_rows,
            sum(result_rows) as total_result_rows,
            sum(memory_usage) / 1024 / 1024 as total_memory_usage_mb
        FROM system.query_log
        WHERE event_time >= now() - INTERVAL 1 HOUR
          AND type = 'QueryFinish'
          AND database = 'pulse_trader'
        GROUP BY query
        HAVING execution_count > 1
        ORDER BY avg_duration_sec DESC
        LIMIT 20
        """

        try:
            result = self.db_client.execute_query(sql)
            if result:
                return pd.DataFrame(result, columns=[
                    'query', 'execution_count', 'avg_duration_sec', 'max_duration_sec',
                    'total_read_rows', 'total_result_rows', 'total_memory_usage_mb'
                ])
        except Exception as e:
            logger.error(f"è·å–æŸ¥è¯¢æ€§èƒ½å¤±è´¥: {e}")

        return pd.DataFrame()

    def get_system_metrics(self) -> dict:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        metrics = {}

        try:
            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            sql = """
            SELECT
                formatReadableSize(value) as memory_readable,
                value as memory_bytes
            FROM system.metrics
            WHERE metric LIKE '%Memory%'
            """

            result = self.db_client.execute_query(sql)
            metrics['memory'] = result if result else []

            # ç£ç›˜ä½¿ç”¨æƒ…å†µ
            sql = "SELECT formatReadableSize(value) as disk_readable, value as disk_bytes FROM system.metrics WHERE metric LIKE '%DiskSize%'"
            result = self.db_client.execute_query(sql)
            metrics['disk'] = result if result else []

            # è¿æ¥æ•°
            sql = "SELECT value FROM system.metrics WHERE metric = 'NumberOfConnections'"
            result = self.db_client.execute_query(sql)
            metrics['connections'] = result[0][0] if result else 0

        except Exception as e:
            logger.error(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")

        return metrics

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("ğŸ“Š ClickHouseæ€§èƒ½ç›‘æ§æŠ¥å‘Š")
        print("=" * 50)

        # è¡¨ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“‹ è¡¨ç»Ÿè®¡ä¿¡æ¯:")
        table_stats = self.get_table_stats()
        if not table_stats.empty:
            print(table_stats.to_string(index=False))
        else:
            print("æ— æ•°æ®")

        # æŸ¥è¯¢æ€§èƒ½
        print("\nâš¡ æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡ (æœ€è¿‘1å°æ—¶):")
        query_stats = self.get_query_performance()
        if not query_stats.empty:
            # åªæ˜¾ç¤ºå‰5ä¸ªæœ€è€—æ—¶çš„æŸ¥è¯¢
            print(query_stats.head().to_string(index=False))
        else:
            print("æ— æŸ¥è¯¢æ•°æ®")

        # ç³»ç»ŸæŒ‡æ ‡
        print("\nğŸ’» ç³»ç»ŸæŒ‡æ ‡:")
        system_metrics = self.get_system_metrics()
        print(f"  å½“å‰è¿æ¥æ•°: {system_metrics.get('connections', 0)}")

        if system_metrics.get('memory'):
            print("  å†…å­˜ä½¿ç”¨:")
            for metric in system_metrics['memory'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"    {metric[0]}")

        if system_metrics.get('disk'):
            print("  ç£ç›˜ä½¿ç”¨:")
            for metric in system_metrics['disk'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"    {metric[0]}")

        print(f"\nğŸ• æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(level=logging.INFO)

    monitor = ClickHouseMonitor()
    monitor.generate_performance_report()


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ æœ¬èŠ‚å°ç»“

### âœ… å®Œæˆå†…å®¹

1. **è¡¨ç»“æ„è®¾è®¡**: åˆ›å»ºäº†è‚¡ç¥¨ä¿¡æ¯ã€æ—¥çº¿è¡Œæƒ…ã€åˆ†é’Ÿè¡Œæƒ…ã€å®æ—¶å¿«ç…§ã€è´¢åŠ¡æ•°æ®ã€äº¤æ˜“æ—¥å†ç­‰è¡¨ç»“æ„
2. **åˆ†åŒºç­–ç•¥**: å®ç°äº†æŒ‰æ—¶é—´ã€å¸‚åœºç­‰å¤šç»´åº¦åˆ†åŒºï¼Œä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
3. **ç´¢å¼•ä¼˜åŒ–**: é…ç½®äº†è·³æ•°ç´¢å¼•ã€ç‰©åŒ–è§†å›¾ç­‰æå‡æŸ¥è¯¢æ•ˆç‡
4. **æ‰¹é‡æ’å…¥**: å®ç°äº†é«˜æ€§èƒ½çš„æ‰¹é‡æ•°æ®æ’å…¥å·¥å…·
5. **æ€§èƒ½ç›‘æ§**: å»ºç«‹äº†å®Œå–„çš„æ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Šä½“ç³»

### ğŸ“Š è¡¨ç»“æ„æ¦‚è§ˆ

| è¡¨å | ä¸»è¦ç”¨é€” | åˆ†åŒºç­–ç•¥ | å­˜å‚¨å¼•æ“ |
|------|----------|----------|----------|
| `stock_info` | è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ | æŒ‰æ›´æ–°æœˆä»½ | ReplacingMergeTree |
| `daily_quotes` | æ—¥çº¿è¡Œæƒ…æ•°æ® | æŒ‰å¹´ä»½å’Œå¸‚åœº | ReplacingMergeTree |
| `intraday_quotes` | åˆ†é’Ÿçº§è¡Œæƒ… | æŒ‰æ—¥æœŸå’Œå°æ—¶ | MergeTree |
| `realtime_snapshot` | å®æ—¶å¿«ç…§ | æ— åˆ†åŒº | ReplacingMergeTree |
| `financial_data` | è´¢åŠ¡æ•°æ® | æŒ‰æŠ¥å‘ŠæœŸç±»å‹ | ReplacingMergeTree |
| `trading_calendar` | äº¤æ˜“æ—¥å† | æ— åˆ†åŒº | ReplacingMergeTree |

### ğŸ”§ æ ¸å¿ƒç‰¹æ€§

1. **æ—¶åºä¼˜åŒ–**: é’ˆå¯¹æ—¶åºæ•°æ®ç‰¹ç‚¹è®¾è®¡çš„åˆ†åŒºå’Œæ’åºç­–ç•¥
2. **å‹ç¼©å­˜å‚¨**: åˆç†çš„æ•°æ®ç±»å‹é€‰æ‹©å’Œå‹ç¼©é…ç½®
3. **æŸ¥è¯¢åŠ é€Ÿ**: å¤šçº§ç´¢å¼•å’Œç‰©åŒ–è§†å›¾åŠ é€Ÿå¸¸ç”¨æŸ¥è¯¢
4. **æ•°æ®ç”Ÿå‘½å‘¨æœŸ**: TTLé…ç½®è‡ªåŠ¨ç®¡ç†æ•°æ®è¿‡æœŸ
5. **æ‰¹é‡å¤„ç†**: é«˜æ•ˆçš„æ‰¹é‡æ’å…¥å’Œå¹¶è¡Œå¤„ç†æœºåˆ¶

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°æ®ä¸€è‡´æ€§**: ä½¿ç”¨ReplacingMergeTreeç¡®ä¿æ•°æ®ç‰ˆæœ¬ç®¡ç†
2. **å­˜å‚¨è§„åˆ’**: åˆç†é…ç½®TTLé¿å…æ•°æ®æ— é™å¢é•¿
3. **æŸ¥è¯¢ä¼˜åŒ–**: æ ¹æ®å®é™…æŸ¥è¯¢æ¨¡å¼è°ƒæ•´åˆ†åŒºå’Œæ’åºé”®
4. **å†…å­˜ç®¡ç†**: ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œé˜²æ­¢OOMé—®é¢˜

### ğŸ”„ ä¸‹ä¸€æ­¥

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ **æ•°æ®æ’å…¥ä¸æŸ¥è¯¢ä¼˜åŒ–**ï¼ŒæŒæ¡é«˜æ€§èƒ½çš„æ•°æ®æ“ä½œæŠ€å·§ã€‚

**[â†’ å‰å¾€ 2.4 æ•°æ®æ’å…¥ä¸æŸ¥è¯¢ä¼˜åŒ–](2.4-æ•°æ®æ’å…¥ä¸æŸ¥è¯¢ä¼˜åŒ–.md)**

---

## ğŸ†˜ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: ClickHouseè¡¨åˆ›å»ºå¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç¡®ä¿ClickHouseæœåŠ¡æ­£å¸¸è¿è¡Œ
2. æ£€æŸ¥SQLè¯­æ³•æ˜¯å¦æ­£ç¡®
3. ç¡®è®¤æ•°æ®åº“å’Œç”¨æˆ·æƒé™è®¾ç½®
4. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³

### Q2: æŸ¥è¯¢æ€§èƒ½ä¸ç†æƒ³å¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**A**: ä¼˜åŒ–å»ºè®®ï¼š
1. æ£€æŸ¥åˆ†åŒºé”®æ˜¯å¦åˆç†
2. æ·»åŠ åˆé€‚çš„è·³æ•°ç´¢å¼•
3. ä¼˜åŒ–ORDER BYå’ŒWHEREå­å¥
4. è€ƒè™‘ä½¿ç”¨ç‰©åŒ–è§†å›¾é¢„è®¡ç®—
5. è°ƒæ•´å†…å­˜å’Œç¼“å­˜é…ç½®

### Q3: æ‰¹é‡æ’å…¥é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: æ€§èƒ½ä¼˜åŒ–ï¼š
1. å¢åŠ æ‰¹é‡å¤§å°ï¼ˆbatch_sizeï¼‰
2. ä½¿ç”¨å¹¶è¡Œæ’å…¥
3. è°ƒæ•´æ•°æ®åº“æ’å…¥ç¼“å†²åŒº
4. ç¦ç”¨ç´¢å¼•åˆ›å»ºï¼ˆæ‰¹é‡æ’å…¥åå†å»ºï¼‰
5. ä½¿ç”¨å‹ç¼©åè®®è¿æ¥

### Q4: æ•°æ®å‹ç¼©ç‡å¦‚ä½•æå‡ï¼Ÿ

**A**: å‹ç¼©ä¼˜åŒ–ï¼š
1. é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹
2. ä½¿ç”¨LowCardinalityç±»å‹
3. å¯ç”¨å­—å…¸å‹ç¼©
4. åˆç†è®¾ç½®ç¼–è§£ç å™¨
5. åˆ†åŒºå­˜å‚¨å‡å°‘æ‰«ææ•°æ®é‡

---

**æœ¬èŠ‚å»ºç«‹äº†é«˜æ€§èƒ½çš„ClickHouseæ•°æ®å­˜å‚¨æ¶æ„ï¼Œä¸ºé‡åŒ–äº¤æ˜“ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„æ•°æ®å­˜å‚¨å’ŒæŸ¥è¯¢èƒ½åŠ›ï¼** ğŸš€âœ¨
