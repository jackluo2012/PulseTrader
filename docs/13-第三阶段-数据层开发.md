# ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®å±‚å¼€å‘

> **ç›®æ ‡**: å®ç°å¤šæ•°æ®æºç»Ÿä¸€æ¥å…¥å’Œç®¡ç†
> **é¢„è®¡æ—¶é—´**: 4-5å¤©
> **æäº¤æ ‡è®°**: `data-layer-complete`
> **å‰ç½®æ¡ä»¶**: å·²å®Œæˆ[ç¬¬äºŒé˜¶æ®µåŸºç¡€æ¡†æ¶](./12-ç¬¬äºŒé˜¶æ®µ-åŸºç¡€æ¡†æ¶.md)

## ğŸ“‹ æ­¥éª¤æ¦‚è§ˆ

1. [âœ…] Tushare æ•°æ®æºå®ç°
2. [ ] akshare æ•°æ®æºé›†æˆ
3. [ ] pytdx å®æ—¶è¡Œæƒ…æ¥å£
4. [ ] æ•°æ®ç¼“å­˜ç³»ç»Ÿ
5. [ ] æ•°æ®è´¨é‡éªŒè¯
6. [ ] ç»Ÿä¸€æ•°æ®æ¥å£

---

## æ­¥éª¤ 1: Tushare æ•°æ®æºå®ç°

### 1.1 é…ç½® Tushare

é¦–å…ˆï¼Œæ‚¨éœ€è¦è·å– Tushare Pro Tokenï¼š

1. æ³¨å†Œ [Tushare Pro](https://tushare.pro/) è´¦æˆ·
2. å®Œæˆå®åè®¤è¯
3. è·å– API Token

é…ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
# ç¼–è¾‘ config/.env æ–‡ä»¶
cp config/.env.example config/.env

# æ·»åŠ æ‚¨çš„ Tushare Token
echo "TUSHARE_TOKEN=your_actual_token_here" >> config/.env
```

### 1.2 å®ç° Tushare æ•°æ®æº

åˆ›å»º `engine/src/data/tushare.rs`:

```rust
// engine/src/data/tushare.rs

use super::{DataSource, DataSourceConfig};
use crate::{KlineData, TickData, DataType, Result, PulseTraderError};
use async_trait::async_trait;
use chrono::{DateTime, Utc, TimeZone, NaiveDateTime};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Tushare Pro æ•°æ®æº
pub struct TushareDataSource {
    client: Client,
    token: String,
    base_url: String,
    rate_limiter: std::sync::Arc<tokio::sync::Semaphore>,
}

/// Tushare API å“åº”ç»“æ„
#[derive(Debug, Deserialize)]
struct TushareResponse {
    request_id: String,
    code: i32,
    msg: Option<String>,
    data: TushareData,
}

#[derive(Debug, Deserialize)]
struct TushareData {
    fields: Vec<String>,
    items: Vec<Vec<serde_json::Value>>,
}

/// Kçº¿æ•°æ®é¡¹
#[derive(Debug, Deserialize)]
struct TushareKlineItem {
    ts_code: String,
    trade_date: String,
    open: f64,
    high: f64,
    low: f64,
    close: f64,
    pre_close: f64,
    change: f64,
    pct_chg: f64,
    vol: f64,
    amount: f64,
}

impl TushareDataSource {
    pub fn new(config: DataSourceConfig) -> Self {
        let token = config.params
            .get("token")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .to_string();

        let base_url = config.params
            .get("base_url")
            .and_then(|v| v.as_str())
            .unwrap_or("https://api.tushare.pro")
            .to_string();

        let rate_limit = config.rate_limit.unwrap_or(200); // é»˜è®¤æ¯åˆ†é’Ÿ200æ¬¡
        let rate_limiter = std::sync::Arc::new(tokio::sync::Semaphore::new(
            (rate_limit / 60).max(1) as usize
        ));

        Self {
            client: Client::new(),
            token,
            base_url,
            rate_limiter,
        }
    }

    /// æ‰§è¡Œ Tushare API è¯·æ±‚
    async fn request_api(&self, api_name: &str, params: HashMap<String, serde_json::Value>) -> Result<TushareResponse> {
        // é€Ÿç‡é™åˆ¶
        let _permit = self.rate_limiter.acquire().await.unwrap();

        let mut request_params = params;
        request_params.insert("token".to_string(), serde_json::Value::String(self.token.clone()));
        request_params.insert("api_name".to_string(), serde_json::Value::String(api_name.to_string()));

        let response = self.client
            .post(&format!("{}/v1/", self.base_url))
            .json(&request_params)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(PulseTraderError::NetworkError {
                source: reqwest::Error::from(response.error_for_status().unwrap_err()),
            });
        }

        let tushare_response: TushareResponse = response.json().await?;

        if tushare_response.code != 0 {
            return Err(PulseTraderError::DataError {
                message: tushare_response.msg.unwrap_or_else(|| "Unknown Tushare error".to_string()),
            });
        }

        Ok(tushare_response)
    }

    /// è½¬æ¢ Tushare æ—¥æœŸæ ¼å¼
    fn parse_tushare_date(&self, date_str: &str) -> Result<DateTime<Utc>> {
        let naive_date = NaiveDateTime::parse_from_str(date_str, "%Y%m%d %H:%M:%S")
            .or_else(|_| NaiveDateTime::parse_from_str(&format!("{} 09:00:00", date_str), "%Y%m%d %H:%M:%S"))?;

        Ok(Utc.from_utc_datetime(&naive_date))
    }

    /// è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
    async fn get_stock_basic(&self) -> Result<Vec<String>> {
        let mut params = HashMap::new();
        params.insert("exchange".to_string(), serde_json::Value::String("SSE,SZSE".to_string()));
        params.insert("list_status".to_string(), serde_json::Value::String("L".to_string()));

        let response = self.request_api("stock_basic", params).await?;
        let mut symbols = Vec::new();

        for item in response.data.items {
            if let (Some(ts_code), Some(list_status)) = (item.get(0), item.get(5)) {
                if let (Some(ts_code_str), Some("L")) = (ts_code.as_str(), list_status.as_str()) {
                    symbols.push(ts_code_str.to_string());
                }
            }
        }

        Ok(symbols)
    }
}

#[async_trait]
impl DataSource for TushareDataSource {
    async fn get_kline_data(
        &self,
        symbol: &str,
        data_type: DataType,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> Result<Vec<KlineData>> {
        let api_name = match data_type {
            DataType::Daily => "daily",
            DataType::Minute1 => "daily",  // Tushare å…è´¹ç‰ˆä¸»è¦æ”¯æŒæ—¥çº¿
            _ => return Err(PulseTraderError::DataError {
                message: format!("ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {:?}", data_type),
            }),
        };

        let mut params = HashMap::new();
        params.insert("ts_code".to_string(), serde_json::Value::String(symbol.to_string()));
        params.insert("start_date".to_string(), serde_json::Value::String(
            start.format("%Y%m%d").to_string()
        ));
        params.insert("end_date".to_string(), serde_json::Value::String(
            end.format("%Y%m%d").to_string()
        ));

        let response = self.request_api(api_name, params).await?;
        let mut kline_data = Vec::new();

        // è§£æå­—æ®µå
        let fields: HashMap<String, usize> = response.data.fields
            .into_iter()
            .enumerate()
            .map(|(i, field)| (field, i))
            .collect();

        for item in response.data.items {
            // æå–å„ä¸ªå­—æ®µçš„å€¼
            let get_value = |field: &str| -> Option<f64> {
                fields.get(field)
                    .and_then(|&idx| item.get(idx))
                    .and_then(|v| v.as_str())
                    .and_then(|s| s.parse().ok())
            };

            let get_string = |field: &str| -> Option<String> {
                fields.get(field)
                    .and_then(|&idx| item.get(idx))
                    .and_then(|v| v.as_str())
                    .map(|s| s.to_string())
            };

            if let (Some(ts_code), Some(trade_date), Some(open), Some(high), Some(low), Some(close), Some(vol), Some(amount)) = (
                get_string("ts_code"),
                get_string("trade_date"),
                get_value("open"),
                get_value("high"),
                get_value("low"),
                get_value("close"),
                get_value("vol"),
                get_value("amount"),
            ) {
                let timestamp = self.parse_tushare_date(&trade_date)?;

                let kline = KlineData::new(
                    ts_code,
                    timestamp,
                    open,
                    high,
                    low,
                    close,
                    vol * 100.0,  // Tushare è¿”å›çš„æ˜¯æ‰‹æ•°
                    amount * 1000.0, // Tushare è¿”å›çš„æ˜¯åƒå…ƒ
                );

                if kline.is_valid() {
                    kline_data.push(kline);
                }
            }
        }

        // æŒ‰æ—¶é—´æ’åº
        kline_data.sort_by_key(|k| k.timestamp);
        Ok(kline_data)
    }

    async fn get_tick_data(&self, _symbol: &str) -> Result<Vec<TickData>> {
        // Tushare å…è´¹ç‰ˆä¸æ”¯æŒå®æ—¶tickæ•°æ®
        Err(PulseTraderError::DataError {
            message: "Tushare å…è´¹ç‰ˆä¸æ”¯æŒå®æ—¶tickæ•°æ®".to_string(),
        })
    }

    async fn get_stock_list(&self) -> Result<Vec<String>> {
        self.get_stock_basic().await
    }

    async fn health_check(&self) -> Result<bool> {
        // å°è¯•è·å–ä¸€åªè‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯
        let response = self.request_api("stock_basic", HashMap::new()).await;
        Ok(response.is_ok())
    }
}
```

### 1.3 æ›´æ–°æ•°æ®æºæ¨¡å—

ç¼–è¾‘ `engine/src/data/mod.rs`:

```rust
// engine/src/data/mod.rs

pub mod source;
pub mod storage;
pub mod tushare;

pub use source::{DataSource, DataSourceConfig};
pub use storage::DataStorage;
pub use tushare::TushareDataSource;

use crate::{KlineData, TickData, DataType, Result};
use async_trait::async_trait;
use std::collections::HashMap;

// ... å…¶ä½™ä»£ç ä¿æŒä¸å˜ ...
```

### 1.4 æ·»åŠ  Cargo ä¾èµ–

ç¼–è¾‘ `engine/Cargo.toml`:

```toml
# åœ¨ [dependencies] éƒ¨åˆ†æ·»åŠ 
async-trait = "0.1"
csv = "1.3"
rand = "0.8"
serde_json = "1.0"
```

---

## æ­¥éª¤ 2: akshare æ•°æ®æºé›†æˆ

### 2.1 åˆ›å»º Python æ•°æ®æºç®¡ç†å™¨

åˆ›å»º `python/pulse_trader/data_sources/__init__.py`:

```python
# python/pulse_trader/data_sources/__init__.py

"""
æ•°æ®æºæ¨¡å—
"""

from .base import BaseDataSource
from .tushare_source import TushareSource
from .akshare_source import AkshareSource
from .pytdx_source import PytdxSource
from .manager import DataSourceManager

__all__ = [
    "BaseDataSource",
    "TushareSource",
    "AkshareSource",
    "PytdxSource",
    "DataSourceManager",
]
```

åˆ›å»º `python/pulse_trader/data_sources/base.py`:

```python
# python/pulse_trader/data_sources/base.py

"""
æ•°æ®æºåŸºç±»
"""

from abc import ABC, abstractmethod
from typing import List, Optional, Union, Dict, Any
from datetime import datetime, date
import pandas as pd
from ..exceptions import DataError, NetworkError


class BaseDataSource(ABC):
    """æ•°æ®æºåŸºç±»"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.name = self.__class__.__name__
        self._initialized = False

    @abstractmethod
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ•°æ®æº"""
        pass

    @abstractmethod
    async def get_kline_data(
        self,
        symbol: str,
        period: str = "daily",
        start_date: Union[str, date, datetime] = None,
        end_date: Union[str, date, datetime] = None,
        adjust: str = "qfq"
    ) -> pd.DataFrame:
        """
        è·å–Kçº¿æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            period: å‘¨æœŸ (daily, weekly, monthly, 1min, 5min, 15min, 30min, 60min)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹ (qfq-å‰å¤æƒ, hfq-åå¤æƒ, none-ä¸å¤æƒ)

        Returns:
            Kçº¿æ•°æ® DataFrame
        """
        pass

    @abstractmethod
    async def get_realtime_quote(self, symbols: List[str]) -> pd.DataFrame:
        """è·å–å®æ—¶è¡Œæƒ…"""
        pass

    @abstractmethod
    async def get_stock_list(self, market: str = "A") -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨

        Args:
            market: å¸‚åœºç±»å‹ (A-Aè‚¡, HK-æ¸¯è‚¡, US-ç¾è‚¡)

        Returns:
            è‚¡ç¥¨åˆ—è¡¨ DataFrame
        """
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        pass

    def _validate_symbol(self, symbol: str) -> str:
        """éªŒè¯å¹¶æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç """
        if not symbol:
            raise DataError("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")

        # æ ‡å‡†åŒ–æ ¼å¼
        symbol = symbol.upper().strip()

        # å¦‚æœæ²¡æœ‰äº¤æ˜“æ‰€åç¼€ï¼Œè‡ªåŠ¨æ·»åŠ 
        if '.' not in symbol:
            if symbol.startswith('0') or symbol.startswith('3'):
                symbol += '.SZ'
            elif symbol.startswith('6'):
                symbol += '.SH'
            elif symbol.startswith('8') or symbol.startswith('4'):
                symbol += '.BJ'

        return symbol

    def _validate_date_range(self, start_date: Union[str, date, datetime],
                           end_date: Union[str, date, datetime]) -> tuple:
        """éªŒè¯æ—¥æœŸèŒƒå›´"""
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date).date()
        elif isinstance(start_date, datetime):
            start_date = start_date.date()

        if isinstance(end_date, str):
            end_date = pd.to_datetime(end_date).date()
        elif isinstance(end_date, datetime):
            end_date = end_date.date()

        if start_date >= end_date:
            raise DataError("å¼€å§‹æ—¥æœŸå¿…é¡»æ—©äºç»“æŸæ—¥æœŸ")

        return start_date, end_date

    def _standardize_kline_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–Kçº¿æ•°æ®åˆ—å"""
        column_mapping = {
            'trade_date': 'timestamp',
            'date': 'timestamp',
            'time': 'timestamp',
            'open': 'open',
            'high': 'high',
            'low': 'low',
            'close': 'close',
            'volume': 'volume',
            'vol': 'volume',
            'amount': 'amount',
            'turnover': 'amount',
            'pct_chg': 'change_pct',
            'pct_change': 'change_pct',
        }

        # é‡å‘½ååˆ—
        df = df.rename(columns=column_mapping)

        # ç¡®ä¿å¿…éœ€çš„åˆ—å­˜åœ¨
        required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            raise DataError(f"æ•°æ®ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")

        # æŒ‰æ—¶é—´æ’åº
        if 'timestamp' in df.columns:
            df = df.sort_values('timestamp').reset_index(drop=True)

        return df

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        if not self._initialized:
            await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        # æ¸…ç†èµ„æº
        pass
```

åˆ›å»º `python/pulse_trader/data_sources/akshare_source.py`:

```python
# python/pulse_trader/data_sources/akshare_source.py

"""
akshare æ•°æ®æºå®ç°
"""

import asyncio
from typing import List, Union, Dict, Any
from datetime import datetime, date
import pandas as pd
import akshare as ak
from .base import BaseDataSource
from ..exceptions import DataError, NetworkError
from ..utils import get_logger

logger = get_logger(__name__)


class AkshareSource(BaseDataSource):
    """akshare æ•°æ®æº"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.cache_enabled = config.get('cache_enabled', True)
        self.timeout = config.get('timeout', 30)

    async def initialize(self) -> bool:
        """åˆå§‹åŒ– akshare"""
        try:
            # æµ‹è¯• akshare æ˜¯å¦å¯ç”¨
            test_df = ak.tool_trade_date_hist_sina()
            if not test_df.empty:
                self._initialized = True
                logger.info("akshare æ•°æ®æºåˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                logger.error("akshare åˆå§‹åŒ–æµ‹è¯•å¤±è´¥")
                return False
        except Exception as e:
            logger.error(f"akshare åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def get_kline_data(
        self,
        symbol: str,
        period: str = "daily",
        start_date: Union[str, date, datetime] = None,
        end_date: Union[str, date, datetime] = None,
        adjust: str = "qfq"
    ) -> pd.DataFrame:
        """è·å–Kçº¿æ•°æ®"""
        try:
            # éªŒè¯å’Œæ ‡å‡†åŒ–å‚æ•°
            symbol = self._validate_symbol(symbol)

            if start_date is None:
                start_date = (datetime.now() - pd.Timedelta(days=365)).date()
            if end_date is None:
                end_date = datetime.now().date()

            start_date, end_date = self._validate_date_range(start_date, end_date)

            # è½¬æ¢ akshare æ ¼å¼çš„è‚¡ç¥¨ä»£ç 
            ak_symbol = self._convert_symbol_for_akshare(symbol)

            # é€‰æ‹©å¯¹åº”çš„ akshare æ¥å£
            if period == "daily":
                df = await self._get_daily_data(ak_symbol, start_date, end_date, adjust)
            elif period in ["1min", "5min", "15min", "30min", "60min"]:
                df = await self._get_minute_data(ak_symbol, period, start_date, end_date)
            else:
                raise DataError(f"ä¸æ”¯æŒçš„æ—¶é—´å‘¨æœŸ: {period}")

            if df.empty:
                logger.warning(f"æœªè·å–åˆ°è‚¡ç¥¨ {symbol} çš„æ•°æ®")
                return pd.DataFrame()

            # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
            df['symbol'] = symbol

            # æ ‡å‡†åŒ–åˆ—å
            df = self._standardize_kline_columns(df)

            logger.info(f"æˆåŠŸè·å– {symbol} Kçº¿æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
            raise DataError(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")

    async def _get_daily_data(self, symbol: str, start_date: date, end_date: date, adjust: str) -> pd.DataFrame:
        """è·å–æ—¥çº¿æ•°æ®"""
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ akshare è°ƒç”¨
            loop = asyncio.get_event_loop()

            if adjust == "qfq":
                # å‰å¤æƒæ•°æ®
                df = await loop.run_in_executor(
                    None,
                    lambda: ak.stock_zh_a_hist(
                        symbol=symbol,
                        period="daily",
                        start_date=start_date.strftime("%Y%m%d"),
                        end_date=end_date.strftime("%Y%m%d"),
                        adjust="qfq"
                    )
                )
            else:
                # ä¸å¤æƒæ•°æ®
                df = await loop.run_in_executor(
                    None,
                    lambda: ak.stock_zh_a_hist(
                        symbol=symbol,
                        period="daily",
                        start_date=start_date.strftime("%Y%m%d"),
                        end_date=end_date.strftime("%Y%m%d"),
                        adjust=""
                    )
                )

            return df

        except Exception as e:
            logger.error(f"è·å–æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
            raise

    async def _get_minute_data(self, symbol: str, period: str, start_date: date, end_date: date) -> pd.DataFrame:
        """è·å–åˆ†é’Ÿçº¿æ•°æ®"""
        try:
            loop = asyncio.get_event_loop()

            # akshare çš„åˆ†é’Ÿçº¿æ•°æ®æ¥å£
            df = await loop.run_in_executor(
                None,
                lambda: ak.stock_zh_a_hist_min_em(
                    symbol=symbol,
                    period=period,
                    start_date=start_date.strftime("%Y%m%d %H:%M:%S"),
                    end_date=end_date.strftime("%Y%m%d %H:%M:%S")
                )
            )

            return df

        except Exception as e:
            logger.error(f"è·å–åˆ†é’Ÿçº¿æ•°æ®å¤±è´¥: {e}")
            raise

    async def get_realtime_quote(self, symbols: List[str]) -> pd.DataFrame:
        """è·å–å®æ—¶è¡Œæƒ…"""
        try:
            if not symbols:
                return pd.DataFrame()

            loop = asyncio.get_event_loop()
            quotes = []

            for symbol in symbols:
                symbol = self._validate_symbol(symbol)
                ak_symbol = self._convert_symbol_for_akshare(symbol)

                try:
                    quote = await loop.run_in_executor(
                        None,
                        lambda: ak.stock_zh_a_spot_em()
                    )

                    # ç­›é€‰æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
                    stock_quote = quote[quote['ä»£ç '] == ak_symbol[:-3]]  # å»æ‰äº¤æ˜“æ‰€åç¼€
                    if not stock_quote.empty:
                        quotes.append(stock_quote.iloc[0].to_dict())

                except Exception as e:
                    logger.warning(f"è·å– {symbol} å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
                    continue

            if quotes:
                df = pd.DataFrame(quotes)
                # æ ‡å‡†åŒ–åˆ—å
                df = df.rename(columns={
                    'ä»£ç ': 'symbol',
                    'æœ€æ–°ä»·': 'price',
                    'æ¶¨è·Œå¹…': 'change_pct',
                    'æˆäº¤é‡': 'volume',
                    'æˆäº¤é¢': 'amount',
                    'å¼€ç›˜ä»·': 'open',
                    'æœ€é«˜ä»·': 'high',
                    'æœ€ä½ä»·': 'low',
                    'æ˜¨æ”¶': 'pre_close'
                })
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
            raise NetworkError(f"è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")

    async def get_stock_list(self, market: str = "A") -> pd.DataFrame:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        try:
            loop = asyncio.get_event_loop()

            if market == "A":
                # è·å–Aè‚¡åˆ—è¡¨
                df = await loop.run_in_executor(None, lambda: ak.stock_zh_a_spot_em())

                # æ ‡å‡†åŒ–æ ¼å¼
                df = df[['ä»£ç ', 'åç§°']].copy()
                df.columns = ['symbol', 'name']

                # æ·»åŠ äº¤æ˜“æ‰€åç¼€
                def add_exchange_suffix(code):
                    if code.startswith('0') or code.startswith('3'):
                        return f"{code}.SZ"
                    elif code.startswith('6'):
                        return f"{code}.SH"
                    elif code.startswith('8') or code.startswith('4'):
                        return f"{code}.BJ"
                    return code

                df['symbol'] = df['symbol'].apply(add_exchange_suffix)
                return df

            else:
                raise DataError(f"ä¸æ”¯æŒçš„å¸‚åœºç±»å‹: {market}")

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            raise DataError(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            loop = asyncio.get_event_loop()
            # å°è¯•è·å–äº¤æ˜“æ—¥å†
            df = await loop.run_in_executor(None, lambda: ak.tool_trade_date_hist_sina())
            return not df.empty
        except Exception as e:
            logger.error(f"akshare å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _convert_symbol_for_akshare(self, symbol: str) -> str:
        """è½¬æ¢ä¸º akshare æ ¼å¼çš„è‚¡ç¥¨ä»£ç """
        # akshare ä½¿ç”¨ä¸å¸¦äº¤æ˜“æ‰€åç¼€çš„6ä½ä»£ç 
        if '.' in symbol:
            return symbol.split('.')[0]
        return symbol

    def __str__(self) -> str:
        return f"AkshareSource(initialized={self._initialized})"


# akshare æ•°æ®æºæ³¨å†Œ
def get_akshare_source(config: Dict[str, Any]) -> AkshareSource:
    """è·å– akshare æ•°æ®æºå®ä¾‹"""
    return AkshareSource(config)
```

### 2.2 åˆ›å»º Tushare Python æ•°æ®æº

åˆ›å»º `python/pulse_trader/data_sources/tushare_source.py`:

```python
# python/pulse_trader/data_sources/tushare_source.py

"""
Tushare æ•°æ®æºå®ç°
"""

import asyncio
from typing import List, Union, Dict, Any
from datetime import datetime, date
import pandas as pd
import tushare as ts
from .base import BaseDataSource
from ..exceptions import DataError, NetworkError
from ..utils import get_logger

logger = get_logger(__name__)


class TushareSource(BaseDataSource):
    """Tushare Pro æ•°æ®æº"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.token = config.get('token', '')
        self.pro_api = config.get('pro_api', 'https://api.tushare.pro')
        self.rate_limit = config.get('rate_limit', 200)  # æ¯åˆ†é’Ÿè¯·æ±‚æ¬¡æ•°
        self._ts_api = None
        self._request_count = 0
        self._last_reset_time = datetime.now()

    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Tushare"""
        try:
            if not self.token:
                raise DataError("Tushare token æœªé…ç½®")

            # è®¾ç½® token
            ts.set_token(self.token)
            self._ts_api = ts.pro_api(self.pro_api)

            # æµ‹è¯•è¿æ¥
            test_result = await self._test_connection()
            if test_result:
                self._initialized = True
                logger.info("Tushare æ•°æ®æºåˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                logger.error("Tushare è¿æ¥æµ‹è¯•å¤±è´¥")
                return False

        except Exception as e:
            logger.error(f"Tushare åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def _test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            loop = asyncio.get_event_loop()
            # å°è¯•è·å–äº¤æ˜“æ—¥æœŸ
            df = await loop.run_in_executor(None, lambda: self._ts_api.trade_cal(exchange='SSE', start_date='20240101', end_date='20240105'))
            return not df.empty
        except Exception as e:
            logger.error(f"Tushare è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    async def _rate_limit_check(self):
        """é€Ÿç‡é™åˆ¶æ£€æŸ¥"""
        now = datetime.now()

        # æ¯åˆ†é’Ÿé‡ç½®è®¡æ•°å™¨
        if (now - self._last_reset_time).seconds >= 60:
            self._request_count = 0
            self._last_reset_time = now

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if self._request_count >= self.rate_limit:
            wait_time = 60 - (now - self._last_reset_time).seconds
            if wait_time > 0:
                logger.warning(f"è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’")
                await asyncio.sleep(wait_time)
                self._request_count = 0
                self._last_reset_time = datetime.now()

        self._request_count += 1

    async def get_kline_data(
        self,
        symbol: str,
        period: str = "daily",
        start_date: Union[str, date, datetime] = None,
        end_date: Union[str, date, datetime] = None,
        adjust: str = "qfq"
    ) -> pd.DataFrame:
        """è·å–Kçº¿æ•°æ®"""
        try:
            if not self._initialized:
                raise DataError("æ•°æ®æºæœªåˆå§‹åŒ–")

            # éªŒè¯å‚æ•°
            symbol = self._validate_symbol(symbol)

            if start_date is None:
                start_date = (datetime.now() - pd.Timedelta(days=365)).date()
            if end_date is None:
                end_date = datetime.now().date()

            start_date, end_date = self._validate_date_range(start_date, end_date)

            # é€Ÿç‡é™åˆ¶
            await self._rate_limit_check()

            # è·å–æ•°æ®
            loop = asyncio.get_event_loop()

            df = await loop.run_in_executor(
                None,
                lambda: self._ts_api.daily(
                    ts_code=symbol,
                    start_date=start_date.strftime("%Y%m%d"),
                    end_date=end_date.strftime("%Y%m%d")
                )
            )

            if df.empty:
                logger.warning(f"æœªè·å–åˆ°è‚¡ç¥¨ {symbol} çš„æ•°æ®")
                return pd.DataFrame()

            # å¤„ç†å¤æƒ (å¦‚æœéœ€è¦)
            if adjust == "qfq" and not df.empty:
                df = await self._adjust_factor(symbol, df, start_date, end_date)

            # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
            df['symbol'] = symbol

            # æ ‡å‡†åŒ–åˆ—å
            df = self._standardize_kline_columns(df)

            logger.info(f"æˆåŠŸè·å– {symbol} Kçº¿æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
            raise DataError(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")

    async def _adjust_factor(self, symbol: str, df: pd.DataFrame, start_date: date, end_date: date) -> pd.DataFrame:
        """è·å–å¤æƒå› å­å¹¶è®¡ç®—å‰å¤æƒä»·æ ¼"""
        try:
            await self._rate_limit_check()

            loop = asyncio.get_event_loop()

            # è·å–å¤æƒå› å­
            adj_df = await loop.run_in_executor(
                None,
                lambda: self._ts_api.adj_factor(
                    ts_code=symbol,
                    start_date=start_date.strftime("%Y%m%d"),
                    end_date=end_date.strftime("%Y%m%d")
                )
            )

            if adj_df.empty:
                return df

            # åˆå¹¶æ•°æ®å¹¶è®¡ç®—å¤æƒä»·æ ¼
            df = df.merge(adj_df[['trade_date', 'adj_factor']], on='trade_date', how='left')

            # å‰å‘å¡«å……å¤æƒå› å­
            df['adj_factor'] = df['adj_factor'].fillna(method='ffill')
            df['adj_factor'] = df['adj_factor'].fillna(1.0)

            # è®¡ç®—å¤æƒä»·æ ¼
            price_columns = ['open', 'high', 'low', 'close', 'pre_close']
            for col in price_columns:
                if col in df.columns:
                    df[col] = df[col] * df['adj_factor']

            return df.drop('adj_factor', axis=1)

        except Exception as e:
            logger.warning(f"è·å–å¤æƒå› å­å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
            return df

    async def get_realtime_quote(self, symbols: List[str]) -> pd.DataFrame:
        """è·å–å®æ—¶è¡Œæƒ… (Tushare å…è´¹ç‰ˆä¸æ”¯æŒ)"""
        logger.warning("Tushare å…è´¹ç‰ˆä¸æ”¯æŒå®æ—¶è¡Œæƒ…æ•°æ®")
        return pd.DataFrame()

    async def get_stock_list(self, market: str = "A") -> pd.DataFrame:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        try:
            if not self._initialized:
                raise DataError("æ•°æ®æºæœªåˆå§‹åŒ–")

            await self._rate_limit_check()

            loop = asyncio.get_event_loop()

            # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            df = await loop.run_in_executor(
                None,
                lambda: self._ts_api.stock_basic(
                    exchange='',
                    list_status='L',
                    fields='ts_code,symbol,name,area,industry,list_date'
                )
            )

            if df.empty:
                return pd.DataFrame()

            # é‡å‘½ååˆ—
            df = df.rename(columns={'ts_code': 'symbol', 'name': 'name'})

            logger.info(f"è·å–åˆ° {len(df)} åªè‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯")
            return df

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            raise DataError(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            if not self._initialized:
                return False

            await self._rate_limit_check()

            loop = asyncio.get_event_loop()
            # å°è¯•è·å–ä¸€æ¡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            df = await loop.run_in_executor(
                None,
                lambda: self._ts_api.stock_basic(exchange='', list_status='L', limit=1)
            )
            return not df.empty

        except Exception as e:
            logger.error(f"Tushare å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def __str__(self) -> str:
        return f"TushareSource(initialized={self._initialized}, rate_limit={self.rate_limit})"


def get_tushare_source(config: Dict[str, Any]) -> TushareSource:
    """è·å– Tushare æ•°æ®æºå®ä¾‹"""
    return TushareSource(config)
```

---

## æ­¥éª¤ 3: pytdx å®æ—¶è¡Œæƒ…æ¥å£

### 3.1 åˆ›å»º pytdx æ•°æ®æº

åˆ›å»º `python/pulse_trader/data_sources/pytdx_source.py`:

```python
# python/pulse_trader/data_sources/pytdx_source.py

"""
pytdx å®æ—¶è¡Œæƒ…æ•°æ®æº
"""

import asyncio
from typing import List, Union, Dict, Any, Optional
from datetime import datetime, date, time
import pandas as pd
from pytdx.hq import TdxHq_API
from pytdx.params import TDXParams
from .base import BaseDataSource
from ..exceptions import DataError, NetworkError
from ..utils import get_logger

logger = get_logger(__name__)


class PytdxSource(BaseDataSource):
    """pytdx å®æ—¶è¡Œæƒ…æ•°æ®æº"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.servers = config.get('servers', [
            {'host': '119.147.212.81', 'port': 7709, 'name': 'ä¸Šæµ·æœŸè´§'},
            {'host': '202.108.253.130', 'port': 7709, 'name': 'æ·±åœ³æœŸè´§'},
        ])
        self.timeout = config.get('timeout', 5)
        self.retry_count = config.get('retry_count', 3)
        self._api = None
        self._connected_server = None

    async def initialize(self) -> bool:
        """åˆå§‹åŒ– pytdx è¿æ¥"""
        try:
            self._api = TdxHq_API()

            # å°è¯•è¿æ¥åˆ°å¯ç”¨æœåŠ¡å™¨
            for server in self.servers:
                if await self._connect_to_server(server):
                    self._connected_server = server
                    self._initialized = True
                    logger.info(f"pytdx è¿æ¥æˆåŠŸ: {server['name']} ({server['host']}:{server['port']})")
                    return True

            logger.error("æ‰€æœ‰ pytdx æœåŠ¡å™¨è¿æ¥å¤±è´¥")
            return False

        except Exception as e:
            logger.error(f"pytdx åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def _connect_to_server(self, server: Dict[str, Any]) -> bool:
        """è¿æ¥åˆ°æŒ‡å®šæœåŠ¡å™¨"""
        try:
            loop = asyncio.get_event_loop()

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è¿æ¥
            connected = await loop.run_in_executor(
                None,
                lambda: self._api.connect(server['host'], server['port'])
            )

            if connected:
                # æµ‹è¯•è¿æ¥
                test_data = await loop.run_in_executor(
                    None,
                    lambda: self._api.get_security_quotes([(1, '000001')])
                )
                return test_data is not None and len(test_data) > 0

            return False

        except Exception as e:
            logger.warning(f"è¿æ¥åˆ°æœåŠ¡å™¨ {server['host']}:{server['port']} å¤±è´¥: {e}")
            return False

    async def get_kline_data(
        self,
        symbol: str,
        period: str = "daily",
        start_date: Union[str, date, datetime] = None,
        end_date: Union[str, date, datetime] = None,
        adjust: str = "qfq"
    ) -> pd.DataFrame:
        """è·å–Kçº¿æ•°æ®"""
        try:
            if not self._initialized:
                raise DataError("æ•°æ®æºæœªåˆå§‹åŒ–")

            # éªŒè¯å‚æ•°
            symbol = self._validate_symbol(symbol)

            if start_date is None:
                start_date = (datetime.now() - pd.Timedelta(days=365)).date()
            if end_date is None:
                end_date = datetime.now().date()

            start_date, end_date = self._validate_date_range(start_date, end_date)

            # è½¬æ¢ pytdx æ ¼å¼
            market_code, stock_code = self._convert_symbol_for_pytdx(symbol)
            period_code = self._convert_period_for_pytdx(period)

            # è·å–æ•°æ®
            loop = asyncio.get_event_loop()

            # pytdx ä¸€æ¬¡æœ€å¤šè·å–800æ¡æ•°æ®
            all_data = []
            current_start = start_date

            while current_start <= end_date:
                try:
                    data = await loop.run_in_executor(
                        None,
                        lambda: self._api.get_k_data(
                            code=stock_code,
                            start_date=current_start.strftime("%Y-%m-%d"),
                            end_date=end_date.strftime("%Y-%m-%d"),
                            market=market_code,
                            ktype=period_code,
                            autype=QfqType.Qfq if adjust == "qfq" else QfqType.None_
                        )
                    )

                    if data and not data.empty:
                        all_data.append(data)

                        # æ›´æ–°å¼€å§‹æ—¥æœŸä¸ºæœ€åä¸€æ¡æ•°æ®çš„ä¸‹ä¸€å¤©
                        last_date = pd.to_datetime(data.iloc[-1]['date']).date()
                        current_start = last_date + pd.Timedelta(days=1)
                    else:
                        break

                except Exception as e:
                    logger.warning(f"è·å– {symbol} éƒ¨åˆ†æ•°æ®å¤±è´¥: {e}")
                    break

            if not all_data:
                logger.warning(f"æœªè·å–åˆ°è‚¡ç¥¨ {symbol} çš„æ•°æ®")
                return pd.DataFrame()

            # åˆå¹¶æ‰€æœ‰æ•°æ®
            df = pd.concat(all_data, ignore_index=True)
            df = df.drop_duplicates(subset=['date']).sort_values('date')

            # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
            df['symbol'] = symbol

            # æ ‡å‡†åŒ–åˆ—å
            df = self._standardize_kline_columns(df)

            logger.info(f"æˆåŠŸè·å– {symbol} Kçº¿æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
            raise DataError(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")

    async def get_realtime_quote(self, symbols: List[str]) -> pd.DataFrame:
        """è·å–å®æ—¶è¡Œæƒ…"""
        try:
            if not self._initialized:
                raise DataError("æ•°æ®æºæœªåˆå§‹åŒ–")

            if not symbols:
                return pd.DataFrame()

            # è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼
            stock_list = []
            for symbol in symbols:
                symbol = self._validate_symbol(symbol)
                market_code, stock_code = self._convert_symbol_for_pytdx(symbol)
                stock_list.append((market_code, stock_code))

            # åˆ†æ‰¹è·å– (æ¯æ¬¡æœ€å¤š80åªè‚¡ç¥¨)
            batch_size = 80
            all_quotes = []

            loop = asyncio.get_event_loop()

            for i in range(0, len(stock_list), batch_size):
                batch = stock_list[i:i + batch_size]

                try:
                    quotes = await loop.run_in_executor(
                        None,
                        lambda: self._api.get_security_quotes(batch)
                    )

                    if quotes:
                        all_quotes.extend(quotes)

                except Exception as e:
                    logger.warning(f"è·å–ç¬¬ {i//batch_size + 1} æ‰¹å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
                    continue

            if not all_quotes:
                return pd.DataFrame()

            # è½¬æ¢ä¸º DataFrame
            df = pd.DataFrame(all_quotes)

            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'code': 'symbol',
                'price': 'price',
                'vol': 'volume',
                'cur_vol': 'volume',
                'amount': 'amount',
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'pre_close': 'pre_close',
                'last_close': 'pre_close',
            }

            df = df.rename(columns=column_mapping)

            # æ·»åŠ äº¤æ˜“æ‰€åç¼€
            def add_exchange_suffix(code):
                if len(code) == 6:
                    if code.startswith('0') or code.startswith('3'):
                        return f"{code}.SZ"
                    elif code.startswith('6'):
                        return f"{code}.SH"
                    elif code.startswith('8') or code.startswith('4'):
                        return f"{code}.BJ"
                return code

            if 'symbol' in df.columns:
                df['symbol'] = df['symbol'].apply(add_exchange_suffix)

            logger.info(f"æˆåŠŸè·å– {len(df)} åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…")
            return df

        except Exception as e:
            logger.error(f"è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
            raise NetworkError(f"è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")

    async def get_stock_list(self, market: str = "A") -> pd.DataFrame:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        try:
            if not self._initialized:
                raise DataError("æ•°æ®æºæœªåˆå§‹åŒ–")

            loop = asyncio.get_event_loop()

            # è·å–è‚¡ç¥¨åˆ—è¡¨
            stocks = await loop.run_in_executor(
                None,
                lambda: self._api.get_security_list(1, 0)  # 1: æ·±äº¤æ‰€, 0: æ²ªäº¤æ‰€
            )

            if not stocks:
                return pd.DataFrame()

            df = pd.DataFrame(stocks)

            # æ ‡å‡†åŒ–æ ¼å¼
            df = df[['code', 'name']].copy()
            df.columns = ['symbol', 'name']

            # æ·»åŠ äº¤æ˜“æ‰€åç¼€
            def add_exchange_suffix(code):
                if code.startswith('0') or code.startswith('3'):
                    return f"{code}.SZ"
                elif code.startswith('6'):
                    return f"{code}.SH"
                return code

            df['symbol'] = df['symbol'].apply(add_exchange_suffix)

            logger.info(f"è·å–åˆ° {len(df)} åªè‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯")
            return df

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            raise DataError(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            if not self._initialized or not self._api:
                return False

            loop = asyncio.get_event_loop()

            # å°è¯•è·å–ä¸€åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…
            quotes = await loop.run_in_executor(
                None,
                lambda: self._api.get_security_quotes([(1, '000001')])
            )

            return quotes is not None and len(quotes) > 0

        except Exception as e:
            logger.error(f"pytdx å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _convert_symbol_for_pytdx(self, symbol: str) -> tuple:
        """è½¬æ¢ä¸º pytdx æ ¼å¼çš„è‚¡ç¥¨ä»£ç """
        if '.' not in symbol:
            raise DataError(f"æ— æ•ˆçš„è‚¡ç¥¨ä»£ç æ ¼å¼: {symbol}")

        code_part, exchange = symbol.split('.')

        # pytdx å¸‚åœºä»£ç 
        if exchange == 'SZ':
            market = 0  # æ·±äº¤æ‰€
        elif exchange == 'SH':
            market = 1  # ä¸Šäº¤æ‰€
        elif exchange == 'BJ':
            market = 2  # åŒ—äº¤æ‰€
        else:
            raise DataError(f"ä¸æ”¯æŒçš„äº¤æ˜“æ‰€: {exchange}")

        return market, code_part

    def _convert_period_for_pytdx(self, period: str) -> int:
        """è½¬æ¢ä¸º pytdx æ ¼å¼çš„æ—¶é—´å‘¨æœŸ"""
        period_mapping = {
            'daily': 9,    # æ—¥K
            '60min': 60,   # 60åˆ†é’Ÿ
            '30min': 30,   # 30åˆ†é’Ÿ
            '15min': 15,   # 15åˆ†é’Ÿ
            '5min': 5,     # 5åˆ†é’Ÿ
            '1min': 1,     # 1åˆ†é’Ÿ
            '1w': 101,     # å‘¨K
            '1m': 102,     # æœˆK
        }

        return period_mapping.get(period, 9)  # é»˜è®¤æ—¥K

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """æ¸…ç†èµ„æº"""
        if self._api:
            try:
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, self._api.disconnect)
            except Exception as e:
                logger.warning(f"pytdx æ–­å¼€è¿æ¥æ—¶å‡ºé”™: {e}")

    def __str__(self) -> str:
        server_info = "æœªè¿æ¥"
        if self._connected_server:
            server_info = f"{self._connected_server['name']} ({self._connected_server['host']}:{self._connected_server['port']})"

        return f"PytdxSource(initialized={self._initialized}, server={server_info})"


def get_pytdx_source(config: Dict[str, Any]) -> PytdxSource:
    """è·å– pytdx æ•°æ®æºå®ä¾‹"""
    return PytdxSource(config)
```

---

## æ­¥éª¤ 4: æ•°æ®æºç®¡ç†å™¨

### 4.1 åˆ›å»ºç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨

åˆ›å»º `python/pulse_trader/data_sources/manager.py`:

```python
# python/pulse_trader/data_sources/manager.py

"""
æ•°æ®æºç®¡ç†å™¨
"""

import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, date
import pandas as pd
from .base import BaseDataSource
from .tushare_source import get_tushare_source
from .akshare_source import get_akshare_source
from .pytdx_source import get_pytdx_source
from ..exceptions import DataError, NetworkError
from ..utils import get_logger, get_config

logger = get_logger(__name__)


class DataSourceManager:
    """æ•°æ®æºç®¡ç†å™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        if config is None:
            app_config = get_config()
            config = app_config.get('data_sources', {})

        self.config = config
        self.sources: Dict[str, BaseDataSource] = {}
        self.primary_source = config.get('primary', 'akshare')
        self.fallback_source = config.get('fallback', 'tushare')
        self.realtime_source = config.get('real_time', 'pytdx')

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æ•°æ®æº"""
        try:
            # åˆå§‹åŒ–ä¸»è¦æ•°æ®æº
            if self.primary_source and self.primary_source in self.config:
                success = await self._initialize_source(self.primary_source)
                if not success:
                    logger.error(f"ä¸»è¦æ•°æ®æº {self.primary_source} åˆå§‹åŒ–å¤±è´¥")

            # åˆå§‹åŒ–å¤‡ç”¨æ•°æ®æº
            if self.fallback_source and self.fallback_source in self.config:
                success = await self._initialize_source(self.fallback_source)
                if not success:
                    logger.error(f"å¤‡ç”¨æ•°æ®æº {self.fallback_source} åˆå§‹åŒ–å¤±è´¥")

            # åˆå§‹åŒ–å®æ—¶æ•°æ®æº
            if self.realtime_source and self.realtime_source in self.config:
                success = await self._initialize_source(self.realtime_source)
                if not success:
                    logger.error(f"å®æ—¶æ•°æ®æº {self.realtime_source} åˆå§‹åŒ–å¤±è´¥")

            logger.info(f"æ•°æ®æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {len(self.sources)} ä¸ªæ•°æ®æº")
            return len(self.sources) > 0

        except Exception as e:
            logger.error(f"æ•°æ®æºç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def _initialize_source(self, source_name: str) -> bool:
        """åˆå§‹åŒ–å•ä¸ªæ•°æ®æº"""
        try:
            if source_name not in self.config:
                logger.warning(f"æœªæ‰¾åˆ°æ•°æ®æºé…ç½®: {source_name}")
                return False

            source_config = self.config[source_name]

            # åˆ›å»ºæ•°æ®æºå®ä¾‹
            source = self._create_source(source_name, source_config)

            # åˆå§‹åŒ–æ•°æ®æº
            success = await source.initialize()

            if success:
                self.sources[source_name] = source
                logger.info(f"æ•°æ®æº {source_name} åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.error(f"æ•°æ®æº {source_name} åˆå§‹åŒ–å¤±è´¥")

            return success

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ•°æ®æº {source_name} æ—¶å‡ºé”™: {e}")
            return False

    def _create_source(self, source_name: str, config: Dict[str, Any]) -> BaseDataSource:
        """åˆ›å»ºæ•°æ®æºå®ä¾‹"""
        source_creators = {
            'tushare': get_tushare_source,
            'akshare': get_akshare_source,
            'pytdx': get_pytdx_source,
        }

        if source_name not in source_creators:
            raise DataError(f"ä¸æ”¯æŒçš„æ•°æ®æº: {source_name}")

        return source_creators[source_name](config)

    async def get_kline_data(
        self,
        symbol: str,
        period: str = "daily",
        start_date: Union[str, date, datetime] = None,
        end_date: Union[str, date, datetime] = None,
        adjust: str = "qfq",
        source_preference: Optional[str] = None
    ) -> pd.DataFrame:
        """
        è·å–Kçº¿æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            period: æ—¶é—´å‘¨æœŸ
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹
            source_preference: æŒ‡å®šæ•°æ®æº

        Returns:
            Kçº¿æ•°æ® DataFrame
        """
        # ç¡®å®šæ•°æ®æºä¼˜å…ˆçº§
        sources_to_try = []

        if source_preference and source_preference in self.sources:
            sources_to_try.append(source_preference)

        if self.primary_source in self.sources:
            sources_to_try.append(self.primary_source)

        if self.fallback_source in self.sources and self.fallback_source not in sources_to_try:
            sources_to_try.append(self.fallback_source)

        # æ·»åŠ å…¶ä»–å¯ç”¨æ•°æ®æº
        for source_name in self.sources:
            if source_name not in sources_to_try:
                sources_to_try.append(source_name)

        last_error = None

        for source_name in sources_to_try:
            try:
                logger.info(f"å°è¯•ä»æ•°æ®æº {source_name} è·å– {symbol} Kçº¿æ•°æ®")

                source = self.sources[source_name]
                df = await source.get_kline_data(symbol, period, start_date, end_date, adjust)

                if not df.empty:
                    logger.info(f"æˆåŠŸä» {source_name} è·å– {len(df)} æ¡ {symbol} Kçº¿æ•°æ®")
                    return df
                else:
                    logger.warning(f"æ•°æ®æº {source_name} è¿”å›ç©ºæ•°æ®")

            except Exception as e:
                last_error = e
                logger.warning(f"ä»æ•°æ®æº {source_name} è·å–æ•°æ®å¤±è´¥: {e}")
                continue

        # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        if last_error:
            raise DataError(f"æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è·å–æ•°æ®ï¼Œæœ€åé”™è¯¯: {last_error}")
        else:
            raise DataError("æ‰€æœ‰æ•°æ®æºéƒ½è¿”å›ç©ºæ•°æ®")

    async def get_realtime_quote(self, symbols: List[str]) -> pd.DataFrame:
        """è·å–å®æ—¶è¡Œæƒ…"""
        if not symbols:
            return pd.DataFrame()

        # ä¼˜å…ˆä½¿ç”¨å®æ—¶æ•°æ®æº
        if self.realtime_source in self.sources:
            try:
                logger.info(f"ä½¿ç”¨å®æ—¶æ•°æ®æº {self.realtime_source} è·å–å®æ—¶è¡Œæƒ…")
                source = self.sources[self.realtime_source]
                df = await source.get_realtime_quote(symbols)

                if not df.empty:
                    logger.info(f"æˆåŠŸè·å– {len(df)} åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…")
                    return df

            except Exception as e:
                logger.warning(f"å®æ—¶æ•°æ®æºè·å–å¤±è´¥: {e}")

        # å°è¯•å…¶ä»–æ•°æ®æº
        for source_name, source in self.sources.items():
            if source_name == self.realtime_source:
                continue

            try:
                df = await source.get_realtime_quote(symbols)
                if not df.empty:
                    logger.info(f"ä»æ•°æ®æº {source_name} è·å–åˆ°å®æ—¶è¡Œæƒ…")
                    return df

            except Exception as e:
                logger.warning(f"æ•°æ®æº {source_name} è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
                continue

        logger.warning("æ— æ³•è·å–ä»»ä½•å®æ—¶è¡Œæƒ…æ•°æ®")
        return pd.DataFrame()

    async def get_stock_list(self, market: str = "A") -> pd.DataFrame:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ•°æ®æº
        source_priority = [self.primary_source, self.fallback_source]

        for source_name in source_priority:
            if source_name not in self.sources:
                continue

            try:
                logger.info(f"ä»æ•°æ®æº {source_name} è·å–è‚¡ç¥¨åˆ—è¡¨")
                source = self.sources[source_name]
                df = await source.get_stock_list(market)

                if not df.empty:
                    logger.info(f"æˆåŠŸä» {source_name} è·å– {len(df)} åªè‚¡ç¥¨ä¿¡æ¯")
                    return df

            except Exception as e:
                logger.warning(f"æ•°æ®æº {source_name} è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
                continue

        # å°è¯•å…¶ä»–æ•°æ®æº
        for source_name, source in self.sources.items():
            if source_name in source_priority:
                continue

            try:
                df = await source.get_stock_list(market)
                if not df.empty:
                    logger.info(f"ä»æ•°æ®æº {source_name} è·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
                    return df

            except Exception as e:
                logger.warning(f"æ•°æ®æº {source_name} è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
                continue

        raise DataError("æ— æ³•ä»ä»»ä½•æ•°æ®æºè·å–è‚¡ç¥¨åˆ—è¡¨")

    async def health_check(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ‰€æœ‰æ•°æ®æºçš„å¥åº·çŠ¶æ€"""
        health_status = {}

        for source_name, source in self.sources.items():
            try:
                is_healthy = await source.health_check()
                health_status[source_name] = is_healthy

                if is_healthy:
                    logger.info(f"æ•°æ®æº {source_name} å¥åº·")
                else:
                    logger.warning(f"æ•°æ®æº {source_name} ä¸å¥åº·")

            except Exception as e:
                health_status[source_name] = False
                logger.error(f"æ£€æŸ¥æ•°æ®æº {source_name} å¥åº·çŠ¶æ€æ—¶å‡ºé”™: {e}")

        return health_status

    def get_available_sources(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ•°æ®æºåˆ—è¡¨"""
        return list(self.sources.keys())

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        # æ¸…ç†èµ„æº
        for source in self.sources.values():
            try:
                await source.__aexit__(exc_type, exc_val, exc_tb)
            except Exception as e:
                logger.warning(f"æ¸…ç†æ•°æ®æºèµ„æºæ—¶å‡ºé”™: {e}")


# å…¨å±€æ•°æ®æºç®¡ç†å™¨å®ä¾‹
_manager_instance: Optional[DataSourceManager] = None


async def get_data_manager() -> DataSourceManager:
    """è·å–å…¨å±€æ•°æ®æºç®¡ç†å™¨å®ä¾‹"""
    global _manager_instance

    if _manager_instance is None:
        _manager_instance = DataSourceManager()
        await _manager_instance.initialize()

    return _manager_instance


def reset_data_manager():
    """é‡ç½®å…¨å±€æ•°æ®æºç®¡ç†å™¨"""
    global _manager_instance
    _manager_instance = None
```

---

## æ­¥éª¤ 5: æ•°æ®ç¼“å­˜ç³»ç»Ÿ

### 5.1 åˆ›å»ºæ•°æ®ç¼“å­˜æ¨¡å—

åˆ›å»º `python/pulse_trader/cache/__init__.py`:

```python
# python/pulse_trader/cache/__init__.py

"""
æ•°æ®ç¼“å­˜æ¨¡å—
"""

from .redis_cache import RedisCache
from .file_cache import FileCache
from .memory_cache import MemoryCache
from .manager import CacheManager

__all__ = [
    "RedisCache",
    "FileCache",
    "MemoryCache",
    "CacheManager",
]
```

åˆ›å»º `python/pulse_trader/cache/file_cache.py`:

```python
# python/pulse_trader/cache/file_cache.py

"""
æ–‡ä»¶ç¼“å­˜å®ç°
"""

import os
import pickle
import hashlib
from pathlib import Path
from typing import Any, Optional, Dict
from datetime import datetime, timedelta
import pandas as pd
from ..utils import get_logger

logger = get_logger(__name__)


class FileCache:
    """æ–‡ä»¶ç¼“å­˜å®ç°"""

    def __init__(self, cache_dir: str = "data/cache", default_ttl: int = 3600):
        self.cache_dir = Path(cache_dir)
        self.default_ttl = default_ttl
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_path(self, key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨ MD5 å“ˆå¸Œé¿å…æ–‡ä»¶åè¿‡é•¿æˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦
        hash_key = hashlib.md5(key.encode()).hexdigest()
        return self.cache_dir / f"{hash_key}.cache"

    def _get_metadata_path(self, key: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        hash_key = hashlib.md5(key.encode()).hexdigest()
        return self.cache_dir / f"{hash_key}.meta"

    def _is_expired(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        meta_path = self._get_metadata_path(key)

        if not meta_path.exists():
            return True

        try:
            with open(meta_path, 'rb') as f:
                metadata = pickle.load(f)

            created_time = metadata.get('created_time')
            ttl = metadata.get('ttl', self.default_ttl)

            if created_time:
                return datetime.now() - created_time > timedelta(seconds=ttl)

            return True

        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç¼“å­˜è¿‡æœŸçŠ¶æ€å¤±è´¥: {e}")
            return True

    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(key)
            meta_path = self._get_metadata_path(key)

            # ä¿å­˜æ•°æ®
            with open(cache_path, 'wb') as f:
                pickle.dump(value, f)

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'created_time': datetime.now(),
                'ttl': ttl or self.default_ttl,
                'key': key,
            }

            with open(meta_path, 'wb') as f:
                pickle.dump(metadata, f)

            logger.debug(f"ç¼“å­˜æ•°æ®æˆåŠŸ: {key}")
            return True

        except Exception as e:
            logger.error(f"è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
            return False

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        try:
            if self._is_expired(key):
                self.delete(key)
                return None

            cache_path = self._get_cache_path(key)

            if not cache_path.exists():
                return None

            with open(cache_path, 'rb') as f:
                value = pickle.load(f)

            logger.debug(f"ç¼“å­˜å‘½ä¸­: {key}")
            return value

        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜å¤±è´¥: {e}")
            return None

    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(key)
            meta_path = self._get_metadata_path(key)

            deleted = False

            if cache_path.exists():
                cache_path.unlink()
                deleted = True

            if meta_path.exists():
                meta_path.unlink()
                deleted = True

            if deleted:
                logger.debug(f"åˆ é™¤ç¼“å­˜æˆåŠŸ: {key}")

            return deleted

        except Exception as e:
            logger.error(f"åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False

    def clear(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        try:
            deleted_count = 0

            for file_path in self.cache_dir.glob("*.cache"):
                file_path.unlink()
                deleted_count += 1

            for file_path in self.cache_dir.glob("*.meta"):
                file_path.unlink()
                deleted_count += 1

            logger.info(f"æ¸…ç©ºç¼“å­˜å®Œæˆï¼Œåˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶")
            return True

        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
            return False

    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            expired_keys = []

            for meta_path in self.cache_dir.glob("*.meta"):
                key = meta_path.stem  # å»æ‰ .meta åç¼€

                if self._is_expired(key):
                    expired_keys.append(key)

            for key in expired_keys:
                self.delete(key)

            logger.info(f"æ¸…ç†è¿‡æœŸç¼“å­˜å®Œæˆï¼Œåˆ é™¤ {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜")
            return len(expired_keys)

        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {e}")
            return 0

    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        try:
            cache_files = list(self.cache_dir.glob("*.cache"))
            total_size = sum(f.stat().st_size for f in cache_files)

            expired_count = 0
            for meta_path in self.cache_dir.glob("*.meta"):
                key = meta_path.stem
                if self._is_expired(key):
                    expired_count += 1

            return {
                'cache_dir': str(self.cache_dir),
                'total_files': len(cache_files),
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / 1024 / 1024, 2),
                'expired_files': expired_count,
            }

        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def __contains__(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
        return not self._is_expired(key)

    def __len__(self) -> int:
        """è·å–ç¼“å­˜é¡¹æ•°é‡"""
        try:
            count = 0
            for meta_path in self.cache_dir.glob("*.meta"):
                key = meta_path.stem
                if not self._is_expired(key):
                    count += 1
            return count
        except Exception:
            return 0
```

åˆ›å»º `python/pulse_trader/cache/manager.py`:

```python
# python/pulse_trader/cache/manager.py

"""
ç¼“å­˜ç®¡ç†å™¨
"""

from typing import Any, Optional, Dict, Union
import pandas as pd
from .file_cache import FileCache
from .memory_cache import MemoryCache
from ..utils import get_logger, get_config

logger = get_logger(__name__)


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, config: Optional[Dict] = None):
        if config is None:
            app_config = get_config()
            config = app_config.get('cache', {})

        self.config = config
        self.cache_type = config.get('type', 'file')

        # åˆå§‹åŒ–ç¼“å­˜åç«¯
        self._init_cache_backend()

    def _init_cache_backend(self):
        """åˆå§‹åŒ–ç¼“å­˜åç«¯"""
        if self.cache_type == 'file':
            cache_dir = self.config.get('dir', 'data/cache')
            default_ttl = self.config.get('ttl', 3600)
            self.backend = FileCache(cache_dir=cache_dir, default_ttl=default_ttl)
        elif self.cache_type == 'memory':
            max_size = self.config.get('max_size', 1000)
            default_ttl = self.config.get('ttl', 3600)
            self.backend = MemoryCache(max_size=max_size, default_ttl=default_ttl)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„ç¼“å­˜ç±»å‹: {self.cache_type}ï¼Œä½¿ç”¨æ–‡ä»¶ç¼“å­˜")
            self.backend = FileCache()

    def _generate_cache_key(self, prefix: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # å¯¹å‚æ•°è¿›è¡Œæ’åºï¼Œç¡®ä¿ç›¸åŒçš„å‚æ•°ç”Ÿæˆç›¸åŒçš„é”®
        sorted_items = sorted(kwargs.items())
        key_parts = [prefix]

        for key, value in sorted_items:
            if value is not None:
                key_parts.append(f"{key}={value}")

        return ":".join(key_parts)

    async def get_kline_data(
        self,
        symbol: str,
        period: str = "daily",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        adjust: str = "qfq"
    ) -> Optional[pd.DataFrame]:
        """è·å–ç¼“å­˜çš„Kçº¿æ•°æ®"""
        cache_key = self._generate_cache_key(
            'kline',
            symbol=symbol,
            period=period,
            start_date=start_date,
            end_date=end_date,
            adjust=adjust
        )

        cached_data = self.backend.get(cache_key)

        if cached_data is not None:
            logger.debug(f"Kçº¿æ•°æ®ç¼“å­˜å‘½ä¸­: {symbol}")
            return cached_data
        else:
            logger.debug(f"Kçº¿æ•°æ®ç¼“å­˜æœªå‘½ä¸­: {symbol}")
            return None

    async def set_kline_data(
        self,
        symbol: str,
        data: pd.DataFrame,
        period: str = "daily",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        adjust: str = "qfq",
        ttl: Optional[int] = None
    ) -> bool:
        """ç¼“å­˜Kçº¿æ•°æ®"""
        if data.empty:
            return False

        cache_key = self._generate_cache_key(
            'kline',
            symbol=symbol,
            period=period,
            start_date=start_date,
            end_date=end_date,
            adjust=adjust
        )

        # æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®ä¸åŒçš„TTL
        if ttl is None:
            if period in ['1min', '5min']:
                ttl = 300  # 5åˆ†é’Ÿ
            elif period in ['15min', '30min']:
                ttl = 1800  # 30åˆ†é’Ÿ
            elif period == '60min':
                ttl = 3600  # 1å°æ—¶
            else:
                ttl = 86400  # 1å¤©

        success = self.backend.set(cache_key, data, ttl=ttl)

        if success:
            logger.debug(f"Kçº¿æ•°æ®ç¼“å­˜æˆåŠŸ: {symbol}, {len(data)} æ¡è®°å½•")

        return success

    async def get_stock_list(self, market: str = "A") -> Optional[pd.DataFrame]:
        """è·å–ç¼“å­˜çš„è‚¡ç¥¨åˆ—è¡¨"""
        cache_key = self._generate_cache_key('stock_list', market=market)
        return self.backend.get(cache_key)

    async def set_stock_list(
        self,
        data: pd.DataFrame,
        market: str = "A",
        ttl: Optional[int] = None
    ) -> bool:
        """ç¼“å­˜è‚¡ç¥¨åˆ—è¡¨"""
        if data.empty:
            return False

        cache_key = self._generate_cache_key('stock_list', market=market)
        ttl = ttl or 86400 * 7  # é»˜è®¤7å¤©

        success = self.backend.set(cache_key, data, ttl=ttl)

        if success:
            logger.debug(f"è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜æˆåŠŸ: {market}, {len(data)} æ¡è®°å½•")

        return success

    async def get_realtime_quote(self, symbols: list) -> Optional[pd.DataFrame]:
        """è·å–ç¼“å­˜çš„å®æ—¶è¡Œæƒ…"""
        # å®æ—¶è¡Œæƒ…ç¼“å­˜æ—¶é—´å¾ˆçŸ­
        symbols_str = ",".join(sorted(symbols))
        cache_key = self._generate_cache_key('realtime', symbols=symbols_str)
        return self.backend.get(cache_key)

    async def set_realtime_quote(
        self,
        data: pd.DataFrame,
        symbols: list,
        ttl: Optional[int] = None
    ) -> bool:
        """ç¼“å­˜å®æ—¶è¡Œæƒ…"""
        if data.empty:
            return False

        symbols_str = ",".join(sorted(symbols))
        cache_key = self._generate_cache_key('realtime', symbols=symbols_str)
        ttl = ttl or 10  # é»˜è®¤10ç§’

        return self.backend.set(cache_key, data, ttl=ttl)

    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        return self.backend.delete(key)

    def clear(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        return self.backend.clear()

    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        return self.backend.cleanup_expired()

    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        info = self.backend.get_cache_info()
        info['cache_type'] = self.cache_type
        return info

    def __contains__(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        return key in self.backend

    def __len__(self) -> int:
        """è·å–ç¼“å­˜é¡¹æ•°é‡"""
        return len(self.backend)


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_cache_manager: Optional[CacheManager] = None


def get_cache_manager() -> CacheManager:
    """è·å–å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _cache_manager

    if _cache_manager is None:
        _cache_manager = CacheManager()

    return _cache_manager


def reset_cache_manager():
    """é‡ç½®å…¨å±€ç¼“å­˜ç®¡ç†å™¨"""
    global _cache_manager
    _cache_manager = None
```

---

## æ­¥éª¤ 6: ç»Ÿä¸€æ•°æ®æ¥å£

### 6.1 åˆ›å»ºæ•°æ®æœåŠ¡å±‚

åˆ›å»º `python/pulse_trader/services/data_service.py`:

```python
# python/pulse_trader/services/data_service.py

"""
æ•°æ®æœåŠ¡å±‚ - ç»Ÿä¸€æ•°æ®æ¥å£
"""

import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, date
import pandas as pd
from ..data_sources.manager import get_data_manager
from ..cache.manager import get_cache_manager
from ..utils import get_logger
from ..exceptions import DataError, NetworkError

logger = get_logger(__name__)


class DataService:
    """æ•°æ®æœåŠ¡ç±» - æä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£"""

    def __init__(self):
        self._data_manager = None
        self._cache_manager = get_cache_manager()
        self._initialized = False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ•°æ®æœåŠ¡"""
        try:
            self._data_manager = await get_data_manager()
            self._initialized = True
            logger.info("æ•°æ®æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"æ•°æ®æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def get_kline_data(
        self,
        symbol: str,
        period: str = "daily",
        start_date: Union[str, date, datetime] = None,
        end_date: Union[str, date, datetime] = None,
        adjust: str = "qfq",
        use_cache: bool = True,
        force_refresh: bool = False
    ) -> pd.DataFrame:
        """
        è·å–Kçº¿æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            period: æ—¶é—´å‘¨æœŸ (daily, weekly, 1min, 5min, 15min, 30min, 60min)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹ (qfq-å‰å¤æƒ, hfq-åå¤æƒ, none-ä¸å¤æƒ)
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°

        Returns:
            Kçº¿æ•°æ® DataFrame
        """
        if not self._initialized:
            raise DataError("æ•°æ®æœåŠ¡æœªåˆå§‹åŒ–")

        # è½¬æ¢æ—¥æœŸæ ¼å¼
        if isinstance(start_date, datetime):
            start_date = start_date.strftime("%Y-%m-%d")
        elif isinstance(start_date, date):
            start_date = start_date.strftime("%Y-%m-%d")

        if isinstance(end_date, datetime):
            end_date = end_date.strftime("%Y-%m-%d")
        elif isinstance(end_date, date):
            end_date = end_date.strftime("%Y-%m-%d")

        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache and not force_refresh:
            cached_data = await self._cache_manager.get_kline_data(
                symbol, period, start_date, end_date, adjust
            )
            if cached_data is not None and not cached_data.empty:
                logger.info(f"ä»ç¼“å­˜è·å– {symbol} Kçº¿æ•°æ®ï¼Œå…± {len(cached_data)} æ¡")
                return cached_data

        # ä»æ•°æ®æºè·å–
        try:
            logger.info(f"ä»æ•°æ®æºè·å– {symbol} Kçº¿æ•°æ®")
            df = await self._data_manager.get_kline_data(
                symbol, period, start_date, end_date, adjust
            )

            if df.empty:
                logger.warning(f"æœªè·å–åˆ° {symbol} çš„Kçº¿æ•°æ®")
                return pd.DataFrame()

            # ç¼“å­˜æ•°æ®
            if use_cache:
                await self._cache_manager.set_kline_data(
                    symbol, df, period, start_date, end_date, adjust
                )

            logger.info(f"æˆåŠŸè·å– {symbol} Kçº¿æ•°æ®ï¼Œå…± {len(df)} æ¡")
            return df

        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
            raise DataError(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")

    async def get_realtime_quotes(
        self,
        symbols: List[str],
        use_cache: bool = True
    ) -> pd.DataFrame:
        """
        è·å–å®æ—¶è¡Œæƒ…

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            å®æ—¶è¡Œæƒ… DataFrame
        """
        if not self._initialized:
            raise DataError("æ•°æ®æœåŠ¡æœªåˆå§‹åŒ–")

        if not symbols:
            return pd.DataFrame()

        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_data = await self._cache_manager.get_realtime_quote(symbols)
            if cached_data is not None and not cached_data.empty:
                logger.info(f"ä»ç¼“å­˜è·å– {len(symbols)} åªè‚¡ç¥¨å®æ—¶è¡Œæƒ…")
                return cached_data

        # ä»æ•°æ®æºè·å–
        try:
            logger.info(f"ä»æ•°æ®æºè·å– {len(symbols)} åªè‚¡ç¥¨å®æ—¶è¡Œæƒ…")
            df = await self._data_manager.get_realtime_quote(symbols)

            if df.empty:
                logger.warning("æœªè·å–åˆ°ä»»ä½•å®æ—¶è¡Œæƒ…æ•°æ®")
                return pd.DataFrame()

            # ç¼“å­˜æ•°æ®ï¼ˆå®æ—¶è¡Œæƒ…ç¼“å­˜æ—¶é—´å¾ˆçŸ­ï¼‰
            if use_cache:
                await self._cache_manager.set_realtime_quote(df, symbols)

            logger.info(f"æˆåŠŸè·å– {len(df)} åªè‚¡ç¥¨å®æ—¶è¡Œæƒ…")
            return df

        except Exception as e:
            logger.error(f"è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›ç©ºDataFrame
            return pd.DataFrame()

    async def get_stock_list(
        self,
        market: str = "A",
        use_cache: bool = True,
        force_refresh: bool = False
    ) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨

        Args:
            market: å¸‚åœºç±»å‹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°

        Returns:
            è‚¡ç¥¨åˆ—è¡¨ DataFrame
        """
        if not self._initialized:
            raise DataError("æ•°æ®æœåŠ¡æœªåˆå§‹åŒ–")

        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache and not force_refresh:
            cached_data = await self._cache_manager.get_stock_list(market)
            if cached_data is not None and not cached_data.empty:
                logger.info(f"ä»ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå…± {len(cached_data)} åªè‚¡ç¥¨")
                return cached_data

        # ä»æ•°æ®æºè·å–
        try:
            logger.info(f"ä»æ•°æ®æºè·å–è‚¡ç¥¨åˆ—è¡¨")
            df = await self._data_manager.get_stock_list(market)

            if df.empty:
                logger.warning("æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
                return pd.DataFrame()

            # ç¼“å­˜æ•°æ®
            if use_cache:
                await self._cache_manager.set_stock_list(df, market)

            logger.info(f"æˆåŠŸè·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå…± {len(df)} åªè‚¡ç¥¨")
            return df

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            raise DataError(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

    async def batch_get_kline_data(
        self,
        symbols: List[str],
        period: str = "daily",
        start_date: Union[str, date, datetime] = None,
        end_date: Union[str, date, datetime] = None,
        adjust: str = "qfq",
        max_concurrent: int = 10
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–Kçº¿æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            period: æ—¶é—´å‘¨æœŸ
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            è‚¡ç¥¨ä»£ç åˆ°Kçº¿æ•°æ®çš„æ˜ å°„
        """
        if not symbols:
            return {}

        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {len(symbols)} åªè‚¡ç¥¨çš„Kçº¿æ•°æ®")

        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)

        async def get_single_kline(symbol: str) -> tuple[str, pd.DataFrame]:
            async with semaphore:
                try:
                    df = await self.get_kline_data(
                        symbol, period, start_date, end_date, adjust
                    )
                    return symbol, df
                except Exception as e:
                    logger.warning(f"è·å– {symbol} Kçº¿æ•°æ®å¤±è´¥: {e}")
                    return symbol, pd.DataFrame()

        # å¹¶å‘è·å–æ‰€æœ‰æ•°æ®
        tasks = [get_single_kline(symbol) for symbol in symbols]
        results = await asyncio.gather(*tasks)

        # æ•´ç†ç»“æœ
        data_dict = {}
        success_count = 0

        for symbol, df in results:
            if not df.empty:
                data_dict[symbol] = df
                success_count += 1
            else:
                logger.warning(f"è‚¡ç¥¨ {symbol} æ— æ•°æ®")

        logger.info(f"æ‰¹é‡è·å–å®Œæˆï¼ŒæˆåŠŸ {success_count}/{len(symbols)} åªè‚¡ç¥¨")
        return data_dict

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        if not self._initialized:
            return {
                'data_service': False,
                'data_sources': {},
                'cache': {},
            }

        # æ£€æŸ¥æ•°æ®æºå¥åº·çŠ¶æ€
        data_sources_health = await self._data_manager.health_check()

        # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
        cache_info = self._cache_manager.get_cache_info()

        return {
            'data_service': True,
            'data_sources': data_sources_health,
            'cache': cache_info,
            'available_sources': self._data_manager.get_available_sources(),
        }

    def clear_cache(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        return self._cache_manager.clear()

    def cleanup_cache(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        return self._cache_manager.cleanup_expired()


# å…¨å±€æ•°æ®æœåŠ¡å®ä¾‹
_data_service: Optional[DataService] = None


async def get_data_service() -> DataService:
    """è·å–å…¨å±€æ•°æ®æœåŠ¡å®ä¾‹"""
    global _data_service

    if _data_service is None:
        _data_service = DataService()
        await _data_service.initialize()

    return _data_service


def reset_data_service():
    """é‡ç½®å…¨å±€æ•°æ®æœåŠ¡å®ä¾‹"""
    global _data_service
    _data_service = None
```

### 6.2 æ›´æ–°ä¸»æ¨¡å—

æ›´æ–° `python/pulse_trader/__init__.py`:

```python
# python/pulse_trader/__init__.py

"""
PulseTrader - Aè‚¡é‡åŒ–äº¤æ˜“ç³»ç»Ÿ

é«˜æ€§èƒ½Rust+Pythonæ··åˆæ¶æ„çš„é‡åŒ–äº¤æ˜“å¹³å°
"""

__version__ = "0.3.0"
__author__ = "Your Name"
__email__ = "your.email@example.com"

# æ ¸å¿ƒæ¨¡å—
from .engine import get_engine
from .services.data_service import get_data_service, DataService

# æ•°æ®æº
from .data_sources.manager import get_data_manager, DataSourceManager

# å¼‚å¸¸
from .exceptions import (
    PulseTraderError,
    DataError,
    NetworkError,
    CalculationError,
    ConfigError,
    ValidationError,
)

# å·¥å…·
from .utils import get_logger, setup_logging, load_config, get_config

__all__ = [
    # ç‰ˆæœ¬ä¿¡æ¯
    "__version__",
    "__author__",
    "__email__",

    # æ ¸å¿ƒåŠŸèƒ½
    "get_engine",
    "get_data_service",
    "DataService",

    # æ•°æ®æº
    "get_data_manager",
    "DataSourceManager",

    # å¼‚å¸¸
    "PulseTraderError",
    "DataError",
    "NetworkError",
    "CalculationError",
    "ConfigError",
    "ValidationError",

    # å·¥å…·
    "get_logger",
    "setup_logging",
    "load_config",
    "get_config",
]
```

---

## æ­¥éª¤ 7: åˆ›å»ºæµ‹è¯•å’Œç¤ºä¾‹

### 7.1 åˆ›å»ºæ•°æ®å±‚æµ‹è¯•

åˆ›å»º `python/tests/test_data_service.py`:

```python
# python/tests/test_data_service.py

"""
æ•°æ®æœåŠ¡æµ‹è¯•
"""

import pytest
import asyncio
from datetime import datetime, timedelta
import pandas as pd
from pulse_trader.services.data_service import DataService
from pulse_trader.exceptions import DataError


class TestDataService:
    """æ•°æ®æœåŠ¡æµ‹è¯•ç±»"""

    @pytest.fixture
    async def data_service(self):
        """æ•°æ®æœåŠ¡æµ‹è¯•å¤¹å…·"""
        service = DataService()
        await service.initialize()
        yield service

    @pytest.mark.asyncio
    async def test_get_kline_data(self, data_service):
        """æµ‹è¯•è·å–Kçº¿æ•°æ®"""
        df = await data_service.get_kline_data(
            "000001.SZ",
            start_date="2023-01-01",
            end_date="2023-01-10"
        )

        assert isinstance(df, pd.DataFrame)
        if not df.empty:
            required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
            for col in required_columns:
                assert col in df.columns

    @pytest.mark.asyncio
    async def test_get_realtime_quotes(self, data_service):
        """æµ‹è¯•è·å–å®æ—¶è¡Œæƒ…"""
        symbols = ["000001.SZ", "600000.SH"]
        df = await data_service.get_realtime_quotes(symbols)

        assert isinstance(df, pd.DataFrame)
        # æ³¨æ„ï¼šå®æ—¶è¡Œæƒ…å¯èƒ½å› ä¸ºç½‘ç»œé—®é¢˜è·å–å¤±è´¥ï¼Œæ‰€ä»¥ä¸å¼ºåˆ¶è¦æ±‚éç©º

    @pytest.mark.asyncio
    async def test_get_stock_list(self, data_service):
        """æµ‹è¯•è·å–è‚¡ç¥¨åˆ—è¡¨"""
        df = await data_service.get_stock_list()

        assert isinstance(df, pd.DataFrame)
        if not df.empty:
            assert 'symbol' in df.columns
            assert 'name' in df.columns

    @pytest.mark.asyncio
    async def test_batch_get_kline_data(self, data_service):
        """æµ‹è¯•æ‰¹é‡è·å–Kçº¿æ•°æ®"""
        symbols = ["000001.SZ", "600000.SH"]

        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=30)

        data_dict = await data_service.batch_get_kline_data(
            symbols,
            start_date=start_date,
            end_date=end_date,
            max_concurrent=2
        )

        assert isinstance(data_dict, dict)
        for symbol in symbols:
            if symbol in data_dict:
                assert isinstance(data_dict[symbol], pd.DataFrame)

    @pytest.mark.asyncio
    async def test_health_check(self, data_service):
        """æµ‹è¯•å¥åº·æ£€æŸ¥"""
        health_info = await data_service.health_check()

        assert isinstance(health_info, dict)
        assert 'data_service' in health_info
        assert 'data_sources' in health_info
        assert 'cache' in health_info

    @pytest.mark.asyncio
    async def test_cache_functionality(self, data_service):
        """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""
        symbol = "000001.SZ"
        start_date = "2023-01-01"
        end_date = "2023-01-05"

        # æ¸…é™¤å¯èƒ½çš„ç¼“å­˜
        data_service.clear_cache()

        # ç¬¬ä¸€æ¬¡è·å–ï¼ˆåº”è¯¥ä»æ•°æ®æºè·å–ï¼‰
        df1 = await data_service.get_kline_data(
            symbol, start_date=start_date, end_date=end_date
        )

        # ç¬¬äºŒæ¬¡è·å–ï¼ˆåº”è¯¥ä»ç¼“å­˜è·å–ï¼‰
        df2 = await data_service.get_kline_data(
            symbol, start_date=start_date, end_date=end_date
        )

        # éªŒè¯ä¸¤æ¬¡è·å–çš„æ•°æ®ç›¸åŒ
        if not df1.empty and not df2.empty:
            pd.testing.assert_frame_equal(df1, df2)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

### 7.2 åˆ›å»ºä½¿ç”¨ç¤ºä¾‹

åˆ›å»º `examples/data_service_demo.py`:

```python
#!/usr/bin/env python3
"""
æ•°æ®æœåŠ¡ä½¿ç”¨ç¤ºä¾‹
"""

import asyncio
import sys
import os
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'python'))

from pulse_trader import (
    get_data_service,
    setup_logging,
    get_logger,
    DataError
)


async def demo_basic_usage():
    """åŸºæœ¬ä½¿ç”¨æ¼”ç¤º"""
    print("=== æ•°æ®æœåŠ¡åŸºæœ¬ä½¿ç”¨æ¼”ç¤º ===\n")

    # è®¾ç½®æ—¥å¿—
    setup_logging(level="INFO")
    logger = get_logger(__name__)

    try:
        # è·å–æ•°æ®æœåŠ¡
        logger.info("åˆå§‹åŒ–æ•°æ®æœåŠ¡...")
        data_service = await get_data_service()

        # 1. è·å–å•åªè‚¡ç¥¨çš„Kçº¿æ•°æ®
        print("1. è·å–å¹³å®‰é“¶è¡Œ(000001.SZ)æœ€è¿‘30å¤©Kçº¿æ•°æ®")
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=30)

        df = await data_service.get_kline_data(
            symbol="000001.SZ",
            start_date=start_date,
            end_date=end_date
        )

        if not df.empty:
            print(f"   è·å–åˆ° {len(df)} æ¡Kçº¿æ•°æ®")
            print(f"   æ•°æ®åˆ—: {list(df.columns)}")
            print(f"   æœ€æ–°æ•°æ®:\n{df.tail(3)}")
        else:
            print("   æœªè·å–åˆ°æ•°æ®")

        print()

        # 2. è·å–å®æ—¶è¡Œæƒ…
        print("2. è·å–å®æ—¶è¡Œæƒ…")
        symbols = ["000001.SZ", "600000.SH", "600036.SH"]
        quotes = await data_service.get_realtime_quotes(symbols)

        if not quotes.empty:
            print(f"   è·å–åˆ° {len(quotes)} åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…")
            print(f"   è¡Œæƒ…æ•°æ®:\n{quotes[['symbol', 'price', 'change_pct']]}")
        else:
            print("   æœªè·å–åˆ°å®æ—¶è¡Œæƒ…æ•°æ®")

        print()

        # 3. æ‰¹é‡è·å–Kçº¿æ•°æ®
        print("3. æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨Kçº¿æ•°æ®")
        batch_symbols = ["000001.SZ", "600000.SH", "600036.SH"]

        batch_data = await data_service.batch_get_kline_data(
            symbols=batch_symbols,
            start_date=start_date,
            end_date=end_date,
            max_concurrent=3
        )

        print(f"   æˆåŠŸè·å– {len(batch_data)} åªè‚¡ç¥¨çš„æ•°æ®")
        for symbol, data in batch_data.items():
            print(f"   {symbol}: {len(data)} æ¡è®°å½•")

        print()

        # 4. å¥åº·æ£€æŸ¥
        print("4. æ•°æ®æœåŠ¡å¥åº·æ£€æŸ¥")
        health_info = await data_service.health_check()

        print("   æ•°æ®æºçŠ¶æ€:")
        for source, status in health_info['data_sources'].items():
            status_text = "æ­£å¸¸" if status else "å¼‚å¸¸"
            print(f"   {source}: {status_text}")

        print(f"   ç¼“å­˜ä¿¡æ¯: {health_info['cache']}")

        print("\n=== æ¼”ç¤ºå®Œæˆ ===")

    except DataError as e:
        logger.error(f"æ•°æ®é”™è¯¯: {e}")
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {e}")


async def demo_cache_usage():
    """ç¼“å­˜ä½¿ç”¨æ¼”ç¤º"""
    print("\n=== ç¼“å­˜åŠŸèƒ½æ¼”ç¤º ===\n")

    logger = get_logger(__name__)

    try:
        data_service = await get_data_service()

        symbol = "000001.SZ"
        start_date = "2023-01-01"
        end_date = "2023-01-10"

        # æ¸…é™¤ç¼“å­˜
        print("1. æ¸…é™¤ç¼“å­˜")
        cleared = data_service.clear_cache()
        print(f"   æ¸…é™¤ç»“æœ: {'æˆåŠŸ' if cleared else 'å¤±è´¥'}")

        # ç¬¬ä¸€æ¬¡è·å–ï¼ˆä»æ•°æ®æºï¼‰
        print("\n2. ç¬¬ä¸€æ¬¡è·å–æ•°æ®ï¼ˆä»æ•°æ®æºï¼‰")
        import time
        start_time = time.time()
        df1 = await data_service.get_kline_data(symbol, start_date=start_date, end_date=end_date)
        first_duration = time.time() - start_time
        print(f"   è€—æ—¶: {first_duration:.3f} ç§’")

        # ç¬¬äºŒæ¬¡è·å–ï¼ˆä»ç¼“å­˜ï¼‰
        print("\n3. ç¬¬äºŒæ¬¡è·å–æ•°æ®ï¼ˆä»ç¼“å­˜ï¼‰")
        start_time = time.time()
        df2 = await data_service.get_kline_data(symbol, start_date=start_date, end_date=end_date)
        second_duration = time.time() - start_time
        print(f"   è€—æ—¶: {second_duration:.3f} ç§’")

        # æ€§èƒ½æ¯”è¾ƒ
        if first_duration > 0 and second_duration > 0:
            speedup = first_duration / second_duration
            print(f"   ç¼“å­˜åŠ é€Ÿæ¯”: {speedup:.1f}x")

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        print("\n4. æ¸…ç†è¿‡æœŸç¼“å­˜")
        cleaned_count = data_service.cleanup_cache()
        print(f"   æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")

        print("\n=== ç¼“å­˜æ¼”ç¤ºå®Œæˆ ===")

    except Exception as e:
        logger.error(f"ç¼“å­˜æ¼”ç¤ºå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("PulseTrader æ•°æ®æœåŠ¡æ¼”ç¤ºç¨‹åº")
    print("=" * 50)

    try:
        await demo_basic_usage()
        await demo_cache_usage()
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## âœ… å®Œæˆæ£€æŸ¥æ¸…å•

### æ•°æ®æºå®ç°
- [ ] Tushare æ•°æ®æºå·²å®ç°
- [ ] akshare æ•°æ®æºå·²é›†æˆ
- [ ] pytdx å®æ—¶è¡Œæƒ…æ¥å£å·²å®ç°
- [ ] æ•°æ®æºç®¡ç†å™¨å·²å®Œæˆ

### ç¼“å­˜ç³»ç»Ÿ
- [ ] æ–‡ä»¶ç¼“å­˜å·²å®ç°
- [ ] å†…å­˜ç¼“å­˜å·²å®ç°
- [ ] ç¼“å­˜ç®¡ç†å™¨å·²å®Œæˆ
- [ ] ç¼“å­˜ç­–ç•¥å·²é…ç½®

### æ•°æ®æœåŠ¡å±‚
- [ ] ç»Ÿä¸€æ•°æ®æ¥å£å·²å®ç°
- [ ] æ‰¹é‡æ•°æ®è·å–åŠŸèƒ½å·²å®Œæˆ
- [ ] é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶å·²å®ç°
- [ ] æ€§èƒ½ä¼˜åŒ–å·²å®Œæˆ

### æµ‹è¯•å’Œç¤ºä¾‹
- [ ] å•å…ƒæµ‹è¯•å·²ç¼–å†™
- [ ] é›†æˆæµ‹è¯•å·²é€šè¿‡
- [ ] ä½¿ç”¨ç¤ºä¾‹å·²åˆ›å»º
- [ ] æ€§èƒ½æµ‹è¯•å·²å®Œæˆ

---

## ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

### é—®é¢˜ 1: Tushare Token é…ç½®é”™è¯¯
**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥é…ç½®æ–‡ä»¶
cat config/.env

# ç¡®ä¿TOKENæ ¼å¼æ­£ç¡®
# TUSHARE_TOKEN=ä½ çš„å®é™…token
```

### é—®é¢˜ 2: æ•°æ®æºè¿æ¥è¶…æ—¶
**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨é…ç½®æ–‡ä»¶ä¸­å¢åŠ è¶…æ—¶æ—¶é—´
data_sources:
  tushare:
    timeout: 30
  akshare:
    timeout: 30
  pytdx:
    timeout: 10
    retry_count: 3
```

### é—®é¢˜ 3: ç¼“å­˜æƒé™é—®é¢˜
**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿ç¼“å­˜ç›®å½•æœ‰å†™æƒé™
mkdir -p data/cache
chmod 755 data/cache
```

---

## ğŸ“ å¼€å‘è®°å½•

**å®Œæˆæ—¶é—´**: ___________
**å®é™…è€—æ—¶**: ___________

### å®ç°çš„åŠŸèƒ½
1.
2.
3.

### é‡åˆ°çš„é—®é¢˜
1.
2.
3.

### æ€§èƒ½ä¼˜åŒ–
1.
2.
3.

### æ•°æ®è´¨é‡éªŒè¯ç»“æœ
1.
2.
3.

---

## ğŸ¯ ä¸‹ä¸€æ­¥

ç¬¬ä¸‰é˜¶æ®µå®Œæˆåï¼Œæ‚¨å¯ä»¥ï¼š

1. **æäº¤ä»£ç **:
   ```bash
   git add .
   git commit -m "feat: å®Œæˆç¬¬ä¸‰é˜¶æ®µæ•°æ®å±‚å¼€å‘

   - å®ç°Tushareã€akshareã€pytdxæ•°æ®æº
   - åˆ›å»ºç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨
   - å®ç°æ–‡ä»¶å’Œå†…å­˜ç¼“å­˜ç³»ç»Ÿ
   - æ„å»ºæ•°æ®æœåŠ¡å±‚
   - æ·»åŠ æ•°æ®è´¨é‡éªŒè¯
   - å®Œæˆæµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹"
   ```

2. **å¼€å§‹ç¬¬å››é˜¶æ®µ**:
   ```bash
   git checkout -b feature/fourth-phase
   ```

3. **é˜…è¯»ä¸‹ä¸€é˜¶æ®µæ–‡æ¡£**: [14-ç¬¬å››é˜¶æ®µ-ç­–ç•¥å›æµ‹.md](./14-ç¬¬å››é˜¶æ®µ-ç­–ç•¥å›æµ‹.md)

**æ­å–œå®Œæˆç¬¬ä¸‰é˜¶æ®µï¼** ğŸ‰ æ‚¨å·²ç»å»ºç«‹äº†å®Œæ•´çš„æ•°æ®å±‚æ¶æ„ï¼Œä¸ºç­–ç•¥å¼€å‘å’Œå›æµ‹æä¾›äº†å¼ºå¤§çš„æ•°æ®æ”¯æŒã€‚